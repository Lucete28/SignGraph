diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
index ffef81f..f4065cc 100644
Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
index 7ac0c3b..c89f220 100644
Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
diff --git a/configs/phoenix2014.yaml b/configs/phoenix2014.yaml
index 500b8eb..2d4417d 100644
--- a/configs/phoenix2014.yaml
+++ b/configs/phoenix2014.yaml
@@ -1,4 +1,4 @@
 dataset_root: /phoenix2014-release/phoenix-2014-multisigner 
 dict_path: ./preprocess/phoenix2014/gloss_dict.npy
-evaluation_dir: ./evaluation/slr_eval
+evaluation_dir: ./evaluation/slr_eval645
 evaluation_prefix: phoenix2014-groundtruth
diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
index 529dabc..b6213f4 100644
Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
index c126e7a..b01e052 100644
--- a/dataset/dataloader_video.py
+++ b/dataset/dataloader_video.py
@@ -9,6 +9,7 @@ import torch
 import random
 import pandas
 import warnings
+import time
 
 warnings.simplefilter(action='ignore', category=FutureWarning)
 
@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
 
             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
-
+            # print('phoenix 데이터를 사용함')
+            # print(img_folder)
 #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
 #            print(img_folder)
 #            img_list = sorted(glob.glob(img_folder))
@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
     def read_features(self, index):
         # load file info
         fi = self.inputs_list[index]
+        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+        time.sleep(10)
         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
         return data['features'], data['label']
 
@@ -220,7 +228,7 @@ if __name__ == "__main__":
         dataset=feeder,
         batch_size=1,
         shuffle=True,
-        drop_last=True,
+        drop_last=False,
         num_workers=0,
     )
     for data in dataloader:
diff --git a/main.py b/main.py
index 18ac59b..2349480 100644
--- a/main.py
+++ b/main.py
@@ -21,6 +21,7 @@ import utils
 from seq_scripts import seq_train, seq_eval
 from torch.cuda.amp import autocast as autocast
 from utils.misc import *
+from utils.decode import analyze_frame_lengths
 class Processor():
     def __init__(self, arg):
         self.arg = arg
@@ -105,13 +106,25 @@ class Processor():
                 print('Please appoint --weights.')
             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+
+            train_wer = seq_eval(
+                self.arg, self.data_loader["train_eval"], self.model, self.device,
+                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+            )
+            dev_wer = seq_eval(
+                self.arg, self.data_loader["dev"], self.model, self.device,
+                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+            )
+            test_wer = seq_eval(
+                self.arg, self.data_loader["test"], self.model, self.device,
+                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+            )
+
             self.recoder.print_log('Evaluation Done.\n')
+
+            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+            analyze_frame_lengths()
+
         elif self.arg.phase == "features":
             for mode in ["train", "dev", "test"]:
                 seq_feature_generation(
@@ -119,6 +132,8 @@ class Processor():
                     self.model, self.device, mode, self.arg.work_dir, self.recoder
                 )
 
+
+
     def save_arg(self):
         arg_dict = vars(self.arg)
         if not os.path.exists(self.arg.work_dir):
@@ -225,12 +240,14 @@ class Processor():
         print("Loading Dataprocessing")
         self.feeder = import_class(self.arg.feeder)
         shutil.copy2(inspect.getfile(self.feeder), self.arg.work_dir)
+
         if self.arg.dataset == 'CSL':
             dataset_list = zip(["train", "dev"], [True, False])
         elif 'phoenix' in self.arg.dataset:
             dataset_list = zip(["train", "train_eval", "dev", "test"], [True, False, False, False]) 
         elif self.arg.dataset == 'CSL-Daily':
             dataset_list = zip(["train", "train_eval", "dev", "test"], [True, False, False, False])
+
         for idx, (mode, train_flag) in enumerate(dataset_list):
             arg = self.arg.feeder_args
             arg["prefix"] = self.arg.dataset_info['dataset_root']
@@ -239,6 +256,7 @@ class Processor():
             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
         print("Loading Dataprocessing finished.")
+        # time.sleep(10)
     def init_fn(self, worker_id):
         np.random.seed(int(self.arg.random_seed)+worker_id)
 
@@ -247,7 +265,7 @@ class Processor():
         if len(self.device.gpu_list) > 1:
             if train_flag:
                 sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=train_flag)
-            else:
+            else: # test flag
                 sampler = torch.utils.data.SequentialSampler(dataset)
             batch_size = self.arg.batch_size if mode == "train" else self.arg.test_batch_size
             loader = torch.utils.data.DataLoader(
diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
index 71519fd..b9664e1 100644
Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
diff --git a/seq_scripts.py b/seq_scripts.py
index d8fcaf9..77cfc71 100644
--- a/seq_scripts.py
+++ b/seq_scripts.py
@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
         })
 
     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
-    
+    # 전체 샘플 WER 평균 계산
+    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+
+    print("\n📊 전체 평균 WER")
+    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+
     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
     for sample in top5:
         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
diff --git a/tmp.ipynb b/tmp.ipynb
index e69de29..0342039 100644
--- a/tmp.ipynb
+++ b/tmp.ipynb
@@ -0,0 +1,272 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "\n",
+    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Model is a state_dict.\n",
+      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+     ]
+    }
+   ],
+   "source": [
+    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+    "    print(\"Model is a state_dict.\")\n",
+    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "import torch.nn as nn  # <== 여기가 중요\n",
+    "import torch.nn.functional as F\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+     ]
+    }
+   ],
+   "source": [
+    "import torch\n",
+    "import torch.nn as nn\n",
+    "import torch.nn.functional as F\n",
+    "import torchvision.models as models\n",
+    "import numpy as np\n",
+    "import modules.resnet as resnet\n",
+    "from modules import BiLSTMLayer, TemporalConv\n",
+    "from modules.criterions import SeqKD\n",
+    "import utils\n",
+    "import modules.resnet as resnet\n",
+    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+    "class Identity(nn.Module):\n",
+    "    def __init__(self):\n",
+    "        super(Identity, self).__init__()\n",
+    "\n",
+    "    def forward(self, x):\n",
+    "        return x\n",
+    "\n",
+    "# L2 정규화 선형 레이어\n",
+    "class NormLinear(nn.Module):\n",
+    "    def __init__(self, in_dim, out_dim):\n",
+    "        super(NormLinear, self).__init__()\n",
+    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+    "\n",
+    "    def forward(self, x):\n",
+    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+    "        return outputs\n",
+    "\n",
+    "# SLRModel (수어 인식 모델)\n",
+    "class SLRModel(nn.Module):\n",
+    "    def __init__(\n",
+    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+    "            weight_norm=True, share_classifier=True\n",
+    "    ):\n",
+    "        super(SLRModel, self).__init__()\n",
+    "        self.decoder = None\n",
+    "        self.loss = dict()\n",
+    "        self.criterion_init()\n",
+    "        self.num_classes = num_classes\n",
+    "        self.loss_weights = loss_weights\n",
+    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+    "\n",
+    "        # 1D CNN을 활용한 Temporal Encoding\n",
+    "        self.conv1d = TemporalConv(input_size=512,\n",
+    "                                   hidden_size=hidden_size,\n",
+    "                                   conv_type=conv_type,\n",
+    "                                   use_bn=use_bn,\n",
+    "                                   num_classes=num_classes)\n",
+    "\n",
+    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+    "\n",
+    "        # BiLSTM 기반 Temporal Model\n",
+    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+    "                                          num_layers=2, bidirectional=True)\n",
+    "\n",
+    "        # Classifier (NormLinear 사용 여부 결정)\n",
+    "        if weight_norm:\n",
+    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+    "        else:\n",
+    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+    "\n",
+    "        # Classifier 공유 여부\n",
+    "        if share_classifier:\n",
+    "            self.conv1d.fc = self.classifier\n",
+    "\n",
+    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+    "        # CNN으로 Frame-wise Feature 추출\n",
+    "        if len(x.shape) == 5:\n",
+    "            batch, temp, channel, height, width = x.shape\n",
+    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+    "        else:\n",
+    "            framewise = x\n",
+    "\n",
+    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+    "        x = conv1d_outputs['visual_feat']\n",
+    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+    "\n",
+    "        # BiLSTM을 활용한 Temporal Modeling\n",
+    "        tm_outputs = self.temporal_model(x, lgt)\n",
+    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+    "\n",
+    "        # 최종 Classifier 적용\n",
+    "        outputs = self.classifier(features_before_classifier)\n",
+    "\n",
+    "        # Inference 모드에서 Decoding\n",
+    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+    "\n",
+    "        return {\n",
+    "            \"framewise_features\": framewise,\n",
+    "            \"visual_features\": x,\n",
+    "            \"temproal_features\": tm_outputs['predictions'],\n",
+    "            \"feat_len\": lgt,\n",
+    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+    "            \"sequence_logits\": outputs,\n",
+    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+    "            \"conv_sents\": conv_pred,\n",
+    "            \"recognized_sents\": pred,\n",
+    "        }\n",
+    "\n",
+    "    def criterion_init(self):\n",
+    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+    "        self.loss['distillation'] = SeqKD(T=8)\n",
+    "        return self.loss\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "KeyError",
+     "evalue": "'dataset_info'",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+     ]
+    }
+   ],
+   "source": [
+    "import os\n",
+    "import numpy as np\n",
+    "import yaml\n",
+    "\n",
+    "# 환경 변수 설정\n",
+    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+    "\n",
+    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+    "with open(config_path, \"r\") as f:\n",
+    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+    "\n",
+    "# ✅ gloss_dict 로드\n",
+    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+    "\n",
+    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "AttributeError",
+     "evalue": "'NoneType' object has no attribute 'items'",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+     ]
+    }
+   ],
+   "source": [
+    "import torch\n",
+    "\n",
+    "# 모델 불러오기\n",
+    "model = SLRModel(\n",
+    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+    ")\n",
+    "\n",
+    "# 저장된 가중치 로드\n",
+    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+    "\n",
+    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+    "if isinstance(state_dict, dict):\n",
+    "    model.load_state_dict(state_dict)\n",
+    "\n",
+    "# 모델을 평가 모드로 설정\n",
+    "model.eval()\n"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "3.9.13",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.13"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
index cb157af..297047c 100644
Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
diff --git a/utils/__pycache__/video_augmentation.cpython-39.pyc b/utils/__pycache__/video_augmentation.cpython-39.pyc
index d4fe444..0efce2c 100644
Binary files a/utils/__pycache__/video_augmentation.cpython-39.pyc and b/utils/__pycache__/video_augmentation.cpython-39.pyc differ
diff --git a/utils/decode.py b/utils/decode.py
index 3877729..ac8dab6 100644
--- a/utils/decode.py
+++ b/utils/decode.py
@@ -6,6 +6,38 @@ import ctcdecode
 import numpy as np
 from itertools import groupby
 import torch.nn.functional as F
+import matplotlib.pyplot as plt
+
+# ⬇ 프레임 길이 저장용 전역 리스트
+frame_lengths = []
+
+def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+    probs = torch.softmax(nn_output, dim=-1)
+    pred_ids = torch.argmax(probs, dim=-1)
+
+    length = int(vid_lgt[batch_idx].item())
+    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+    x = np.arange(length)
+
+    plt.figure(figsize=(15, 4))
+    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+
+    blank_indices = np.where(pred_seq == blank_id)[0]
+    if len(blank_indices) > 0:
+        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+
+    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+    if sample_id:
+        title_str += f"\nSample: {sample_id}"
+    plt.title(title_str)
+    plt.xlabel("Time Step")
+    plt.ylabel("Gloss ID")
+    plt.yticks(np.unique(pred_seq))
+    plt.grid(True)
+    plt.legend()
+    plt.tight_layout()
+    plt.show()
 
 
 class Decode(object):
@@ -16,35 +48,27 @@ class Decode(object):
         self.search_mode = search_mode
         self.blank_id = blank_id
         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
-                                                    num_processes=10)
+        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
 
-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
         if not batch_first:
             nn_output = nn_output.permute(1, 0, 2)
+
+        # ⬇ 프레임 길이 수집
+        global frame_lengths
+        for i in range(vid_lgt.size(0)):
+            frame_lengths.append(int(vid_lgt[i].item()))
+
+        # sample_id가 존재하면 시각화
+        sample_id = sample_ids[0] if sample_ids else None
+        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+
         if self.search_mode == "max":
             return self.MaxDecode(nn_output, vid_lgt)
         else:
             return self.BeamSearch(nn_output, vid_lgt, probs)
 
     def BeamSearch(self, nn_output, vid_lgt, probs=False):
-        '''
-        CTCBeamDecoder Shape:
-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
-                          timesteps (B, N_beams)
-                          out_lens (B, N_beams)
-        '''
-
-        index_list = torch.argmax(nn_output.cpu(), axis=2)
-        batchsize, lgt = index_list.shape
-        blank_rate =[]
-        for batch_idx in range(batchsize):
-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
-            blank_rate.append(group_result)
-
-
         if not probs:
             nn_output = nn_output.softmax(-1).cpu()
         vid_lgt = vid_lgt.cpu()
@@ -54,9 +78,7 @@ class Decode(object):
             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
             if len(first_result) != 0:
                 first_result = torch.stack([x[0] for x in groupby(first_result)])
-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
-                             enumerate(first_result)])
+            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
         return ret_list
 
     def MaxDecode(self, nn_output, vid_lgt):
@@ -65,12 +87,34 @@ class Decode(object):
         ret_list = []
         for batch_idx in range(batchsize):
             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+            filtered = [x for x in group_result if x != self.blank_id]
             if len(filtered) > 0:
                 max_result = torch.stack(filtered)
                 max_result = [x[0] for x in groupby(max_result)]
             else:
                 max_result = filtered
-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
-                             enumerate(max_result)])
+            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
         return ret_list
+
+
+# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
+def analyze_frame_lengths():
+    if not frame_lengths:
+        print("⚠ 분석할 frame_lengths가 없습니다.")
+        return
+
+    print("\n📊 Test Video Frame Length Analysis:")
+    print(f"- Total samples: {len(frame_lengths)}")
+    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
+    print(f"- Min length  : {np.min(frame_lengths)}")
+    print(f"- Max length  : {np.max(frame_lengths)}")
+
+    # 히스토그램 시각화
+    plt.figure(figsize=(10, 5))
+    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
+    plt.title("Test Video Frame Length Distribution")
+    plt.xlabel("Frame Length")
+    plt.ylabel("Number of Samples")
+    plt.grid(True)
+    plt.tight_layout()
+    plt.show()
diff --git a/utils/video_augmentation.py b/utils/video_augmentation.py
index c52ab2d..89dfafb 100644
--- a/utils/video_augmentation.py
+++ b/utils/video_augmentation.py
@@ -304,7 +304,7 @@ class RandomRotation(object):
             raise TypeError('Expected numpy.ndarray or PIL.Image' +
                             'but got list of {0}'.format(type(clip[0])))
         return rotated
-
+import time
 
 class TemporalRescale(object):
     def __init__(self, temp_scaling=0.2, frame_interval=1):
@@ -326,6 +326,8 @@ class TemporalRescale(object):
             index = sorted(random.sample(range(vid_len), new_len))
         else:
             index = sorted(random.choices(range(vid_len), k=new_len))
+        # print(f"[TemporalRescale] 입력 길이: {vid_len}, 출력 길이: {new_len}")
+        # time.sleep(2)
         return clip[index]
 
 
diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
index 7d0a2aa..cd66258 100644
Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
index c31483a..7ae9234 100644
--- a/work_dirt/config.yaml
+++ b/work_dirt/config.yaml
@@ -4,10 +4,10 @@ dataset: phoenix2014
 dataset_info:
   dataset_root: /phoenix2014-release/phoenix-2014-multisigner
   dict_path: ./preprocess/phoenix2014/gloss_dict.npy
-  evaluation_dir: ./evaluation/slr_eval
+  evaluation_dir: ./evaluation/slr_eval645
   evaluation_prefix: phoenix2014-groundtruth
 decode_mode: beam
-device: your_device
+device: cuda
 dist_url: env://
 eval_interval: 1
 evaluate_tool: python
diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
index c126e7a..b01e052 100644
--- a/work_dirt/dataloader_video.py
+++ b/work_dirt/dataloader_video.py
@@ -9,6 +9,7 @@ import torch
 import random
 import pandas
 import warnings
+import time
 
 warnings.simplefilter(action='ignore', category=FutureWarning)
 
@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
 
             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
-
+            # print('phoenix 데이터를 사용함')
+            # print(img_folder)
 #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
 #            print(img_folder)
 #            img_list = sorted(glob.glob(img_folder))
@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
     def read_features(self, index):
         # load file info
         fi = self.inputs_list[index]
+        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+        time.sleep(10)
         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
         return data['features'], data['label']
 
@@ -220,7 +228,7 @@ if __name__ == "__main__":
         dataset=feeder,
         batch_size=1,
         shuffle=True,
-        drop_last=True,
+        drop_last=False,
         num_workers=0,
     )
     for data in dataloader:
diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
index 00c1a0e..d2acbc6 100644
--- a/work_dirt/dev.txt
+++ b/work_dirt/dev.txt
@@ -8,3 +8,31 @@
 [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
 [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
 [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Thu Apr  3 13:53:43 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Thu Apr  3 13:53:43 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Thu Apr  3 16:25:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Thu Apr  3 16:25:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+[ Fri Apr  4 14:17:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+[ Fri Apr  4 14:17:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
index 8a22ece..f83ceac 100644
--- a/work_dirt/dirty.patch
+++ b/work_dirt/dirty.patch
@@ -1,284 +1,21298 @@
-diff --git a/README.md b/README.md
-index bdbc17f..8cb240b 100644
---- a/README.md
-+++ b/README.md
-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
- 
- 
- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
- 
- ### Training
- 
-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
-index bfc1da8..25ffa61 100644
---- a/configs/baseline.yaml
-+++ b/configs/baseline.yaml
-@@ -1,14 +1,14 @@
- feeder: dataset.dataloader_video.BaseFeeder
- phase: train
--dataset: phoenix2014-T
-+dataset: phoenix2014
- #CSL-Daily
- # dataset: phoenix14-si5
- 
- work_dir: ./work_dirt/
--batch_size: 4
-+batch_size: 1
- random_seed: 0 
--test_batch_size: 4
--num_worker: 20
-+test_batch_size: 1
-+num_worker: 3
- device: 0
- log_interval: 10000
- eval_interval: 1
+diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
+index ffef81f..f4065cc 100644
+Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
+diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+index 7ac0c3b..c89f220 100644
+Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+diff --git a/configs/phoenix2014.yaml b/configs/phoenix2014.yaml
+index 500b8eb..2d4417d 100644
+--- a/configs/phoenix2014.yaml
++++ b/configs/phoenix2014.yaml
+@@ -1,4 +1,4 @@
+ dataset_root: /phoenix2014-release/phoenix-2014-multisigner 
+ dict_path: ./preprocess/phoenix2014/gloss_dict.npy
+-evaluation_dir: ./evaluation/slr_eval
++evaluation_dir: ./evaluation/slr_eval645
+ evaluation_prefix: phoenix2014-groundtruth
+diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
+index 529dabc..b6213f4 100644
+Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
 diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
-index 555f4b8..c126e7a 100644
+index c126e7a..b01e052 100644
 --- a/dataset/dataloader_video.py
 +++ b/dataset/dataloader_video.py
-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
-         self.transform_mode = "train" if transform_mode else "test"
--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
-+
-         print(mode, len(self))
-         self.data_aug = self.transform()
-         print("")
-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
-             return input_data, label, self.inputs_list[idx]['original_info']
+@@ -9,6 +9,7 @@ import torch
+ import random
+ import pandas
+ import warnings
++import time
  
-     def read_video(self, index):
--        # load file info
-         fi = self.inputs_list[index]
-+    
-         if 'phoenix' in self.dataset:
--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
-+#            img_list = sorted(glob.glob(frame_pattern))
-+#            print(img_list)
-+
-+#            print("[LOG] Using phoenix")
-+
-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+ warnings.simplefilter(action='ignore', category=FutureWarning)
  
-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
-+#            print(img_folder)
-+#            img_list = sorted(glob.glob(img_folder))
-+#            print(f"[DEBUG] Found {len(img_list)} frames")
-+#            print(len(img_list))
-         elif self.dataset == 'CSL-Daily':
--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
-+    
-         img_list = sorted(glob.glob(img_folder))
-+    
-+        if len(img_list) == 0:
-+            print(f"[WARNING] No frames found in: {img_list}")
-+    
-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
-+    
-         label_list = []
--        if self.dataset=='phoenix2014':
-+        if self.dataset == 'phoenix2014':
-             fi['label'] = clean_phoenix_2014(fi['label'])
--        if self.dataset=='phoenix2014-T':
--            fi['label']=clean_phoenix_2014_trans(fi['label'])
-+        elif self.dataset == 'phoenix2014-T':
-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
-+    
-         for phase in fi['label'].split(" "):
--            if phase == '':
--                continue
--            if phase in self.dict.keys():
-+            if phase and phase in self.dict:
-                 label_list.append(self.dict[phase][0])
--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
-+    
-+        video = [
-+            cv2.cvtColor(
-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
-+                cv2.COLOR_BGR2RGB
-+            )   
-+            for img_path in img_list
-+        ]
-+    
-+        return video, label_list, fi
+@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
  
+             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+-
++            # print('phoenix 데이터를 사용함')
++            # print(img_folder)
+ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+ #            print(img_folder)
+ #            img_list = sorted(glob.glob(img_folder))
+@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
      def read_features(self, index):
          # load file info
+         fi = self.inputs_list[index]
++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++        time.sleep(10)
+         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+         return data['features'], data['label']
+ 
+@@ -220,7 +228,7 @@ if __name__ == "__main__":
+         dataset=feeder,
+         batch_size=1,
+         shuffle=True,
+-        drop_last=True,
++        drop_last=False,
+         num_workers=0,
+     )
+     for data in dataloader:
 diff --git a/main.py b/main.py
-index 9e68cee..18ac59b 100644
+index 18ac59b..4e8393c 100644
 --- a/main.py
 +++ b/main.py
-@@ -256,7 +256,7 @@ class Processor():
-                 batch_size=batch_size,
-                 collate_fn=self.feeder.collate_fn,
-                 num_workers=self.arg.num_worker,
--                pin_memory=True,
-+                pin_memory=False,
-                 worker_init_fn=self.init_fn,
-             )
-             return loader
-@@ -268,7 +268,7 @@ class Processor():
-                 drop_last=train_flag,
-                 num_workers=self.arg.num_worker,  # if train_flag else 0
-                 collate_fn=self.feeder.collate_fn,
--                pin_memory=True,
-+                pin_memory=False,
-                 worker_init_fn=self.init_fn,
-             )
+@@ -21,6 +21,7 @@ import utils
+ from seq_scripts import seq_train, seq_eval
+ from torch.cuda.amp import autocast as autocast
+ from utils.misc import *
++from utils.decode import analyze_frame_lengths
+ class Processor():
+     def __init__(self, arg):
+         self.arg = arg
+@@ -105,13 +106,25 @@ class Processor():
+                 print('Please appoint --weights.')
+             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++
++            train_wer = seq_eval(
++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++            )
++            dev_wer = seq_eval(
++                self.arg, self.data_loader["dev"], self.model, self.device,
++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++            )
++            test_wer = seq_eval(
++                self.arg, self.data_loader["test"], self.model, self.device,
++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++            )
++
+             self.recoder.print_log('Evaluation Done.\n')
++
++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++            decode.analyze_frame_lengths()
++
+         elif self.arg.phase == "features":
+             for mode in ["train", "dev", "test"]:
+                 seq_feature_generation(
+@@ -119,6 +132,8 @@ class Processor():
+                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+                 )
+ 
++
++
+     def save_arg(self):
+         arg_dict = vars(self.arg)
+         if not os.path.exists(self.arg.work_dir):
+@@ -225,12 +240,14 @@ class Processor():
+         print("Loading Dataprocessing")
+         self.feeder = import_class(self.arg.feeder)
+         shutil.copy2(inspect.getfile(self.feeder), self.arg.work_dir)
++
+         if self.arg.dataset == 'CSL':
+             dataset_list = zip(["train", "dev"], [True, False])
+         elif 'phoenix' in self.arg.dataset:
+             dataset_list = zip(["train", "train_eval", "dev", "test"], [True, False, False, False]) 
+         elif self.arg.dataset == 'CSL-Daily':
+             dataset_list = zip(["train", "train_eval", "dev", "test"], [True, False, False, False])
++
+         for idx, (mode, train_flag) in enumerate(dataset_list):
+             arg = self.arg.feeder_args
+             arg["prefix"] = self.arg.dataset_info['dataset_root']
+@@ -239,6 +256,7 @@ class Processor():
+             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
+             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
+         print("Loading Dataprocessing finished.")
++        # time.sleep(10)
+     def init_fn(self, worker_id):
+         np.random.seed(int(self.arg.random_seed)+worker_id)
  
+diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+index 71519fd..b9664e1 100644
+Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
 diff --git a/seq_scripts.py b/seq_scripts.py
-index 528856d..d8fcaf9 100644
+index d8fcaf9..77cfc71 100644
 --- a/seq_scripts.py
 +++ b/seq_scripts.py
-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
-     return
- 
- 
-+import csv 
-+from jiwer import wer as jiwer_wer
- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
-     model.eval()
--    results=defaultdict(dict)
-+    results = defaultdict(dict)
+@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+         })
  
-     for batch_idx, data in enumerate(tqdm(loader)):
-         recoder.record_timer("device")
-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
-                 results[inf]['conv_sents'] = conv_sents
-                 results[inf]['recognized_sents'] = recognized_sents
-                 results[inf]['gloss'] = gl
+     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+-    
++    # 전체 샘플 WER 평균 계산
++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
 +
-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
-     gls_ref = [results[n]['gloss'] for n in results]
-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++    print("\n📊 전체 평균 WER")
++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
 +
-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
--    if wer_results['wer'] < wer_results_con['wer']:
--        reg_per = wer_results
--    else:
--        reg_per = wer_results_con
+     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+     for sample in top5:
+         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+diff --git a/tmp.ipynb b/tmp.ipynb
+index e69de29..0342039 100644
+--- a/tmp.ipynb
++++ b/tmp.ipynb
+@@ -0,0 +1,272 @@
++{
++ "cells": [
++  {
++   "cell_type": "code",
++   "execution_count": 2,
++   "metadata": {},
++   "outputs": [],
++   "source": [
++    "import torch\n",
++    "\n",
++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++   ]
++  },
++  {
++   "cell_type": "code",
++   "execution_count": 3,
++   "metadata": {},
++   "outputs": [
++    {
++     "name": "stdout",
++     "output_type": "stream",
++     "text": [
++      "Model is a state_dict.\n",
++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++     ]
++    }
++   ],
++   "source": [
++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++    "    print(\"Model is a state_dict.\")\n",
++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++   ]
++  },
++  {
++   "cell_type": "code",
++   "execution_count": 7,
++   "metadata": {},
++   "outputs": [],
++   "source": [
++    "import torch\n",
++    "import torch.nn as nn  # <== 여기가 중요\n",
++    "import torch.nn.functional as F\n"
++   ]
++  },
++  {
++   "cell_type": "code",
++   "execution_count": 1,
++   "metadata": {},
++   "outputs": [
++    {
++     "name": "stderr",
++     "output_type": "stream",
++     "text": [
++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++     ]
++    }
++   ],
++   "source": [
++    "import torch\n",
++    "import torch.nn as nn\n",
++    "import torch.nn.functional as F\n",
++    "import torchvision.models as models\n",
++    "import numpy as np\n",
++    "import modules.resnet as resnet\n",
++    "from modules import BiLSTMLayer, TemporalConv\n",
++    "from modules.criterions import SeqKD\n",
++    "import utils\n",
++    "import modules.resnet as resnet\n",
++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++    "class Identity(nn.Module):\n",
++    "    def __init__(self):\n",
++    "        super(Identity, self).__init__()\n",
++    "\n",
++    "    def forward(self, x):\n",
++    "        return x\n",
++    "\n",
++    "# L2 정규화 선형 레이어\n",
++    "class NormLinear(nn.Module):\n",
++    "    def __init__(self, in_dim, out_dim):\n",
++    "        super(NormLinear, self).__init__()\n",
++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++    "\n",
++    "    def forward(self, x):\n",
++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++    "        return outputs\n",
++    "\n",
++    "# SLRModel (수어 인식 모델)\n",
++    "class SLRModel(nn.Module):\n",
++    "    def __init__(\n",
++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++    "            weight_norm=True, share_classifier=True\n",
++    "    ):\n",
++    "        super(SLRModel, self).__init__()\n",
++    "        self.decoder = None\n",
++    "        self.loss = dict()\n",
++    "        self.criterion_init()\n",
++    "        self.num_classes = num_classes\n",
++    "        self.loss_weights = loss_weights\n",
++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++    "\n",
++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++    "        self.conv1d = TemporalConv(input_size=512,\n",
++    "                                   hidden_size=hidden_size,\n",
++    "                                   conv_type=conv_type,\n",
++    "                                   use_bn=use_bn,\n",
++    "                                   num_classes=num_classes)\n",
++    "\n",
++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++    "\n",
++    "        # BiLSTM 기반 Temporal Model\n",
++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++    "                                          num_layers=2, bidirectional=True)\n",
++    "\n",
++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++    "        if weight_norm:\n",
++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++    "        else:\n",
++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++    "\n",
++    "        # Classifier 공유 여부\n",
++    "        if share_classifier:\n",
++    "            self.conv1d.fc = self.classifier\n",
++    "\n",
++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++    "        # CNN으로 Frame-wise Feature 추출\n",
++    "        if len(x.shape) == 5:\n",
++    "            batch, temp, channel, height, width = x.shape\n",
++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++    "        else:\n",
++    "            framewise = x\n",
++    "\n",
++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++    "        x = conv1d_outputs['visual_feat']\n",
++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++    "\n",
++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++    "\n",
++    "        # 최종 Classifier 적용\n",
++    "        outputs = self.classifier(features_before_classifier)\n",
++    "\n",
++    "        # Inference 모드에서 Decoding\n",
++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++    "\n",
++    "        return {\n",
++    "            \"framewise_features\": framewise,\n",
++    "            \"visual_features\": x,\n",
++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++    "            \"feat_len\": lgt,\n",
++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++    "            \"sequence_logits\": outputs,\n",
++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++    "            \"conv_sents\": conv_pred,\n",
++    "            \"recognized_sents\": pred,\n",
++    "        }\n",
++    "\n",
++    "    def criterion_init(self):\n",
++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++    "        return self.loss\n"
++   ]
++  },
++  {
++   "cell_type": "code",
++   "execution_count": 6,
++   "metadata": {},
++   "outputs": [
++    {
++     "ename": "KeyError",
++     "evalue": "'dataset_info'",
++     "output_type": "error",
++     "traceback": [
++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++     ]
++    }
++   ],
++   "source": [
++    "import os\n",
++    "import numpy as np\n",
++    "import yaml\n",
++    "\n",
++    "# 환경 변수 설정\n",
++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++    "\n",
++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++    "with open(config_path, \"r\") as f:\n",
++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++    "\n",
++    "# ✅ gloss_dict 로드\n",
++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++    "\n",
++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++   ]
++  },
++  {
++   "cell_type": "code",
++   "execution_count": 5,
++   "metadata": {},
++   "outputs": [
++    {
++     "ename": "AttributeError",
++     "evalue": "'NoneType' object has no attribute 'items'",
++     "output_type": "error",
++     "traceback": [
++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++     ]
++    }
++   ],
++   "source": [
++    "import torch\n",
++    "\n",
++    "# 모델 불러오기\n",
++    "model = SLRModel(\n",
++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++    ")\n",
++    "\n",
++    "# 저장된 가중치 로드\n",
++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++    "\n",
++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++    "if isinstance(state_dict, dict):\n",
++    "    model.load_state_dict(state_dict)\n",
++    "\n",
++    "# 모델을 평가 모드로 설정\n",
++    "model.eval()\n"
++   ]
++  }
++ ],
++ "metadata": {
++  "kernelspec": {
++   "display_name": "3.9.13",
++   "language": "python",
++   "name": "python3"
++  },
++  "language_info": {
++   "codemirror_mode": {
++    "name": "ipython",
++    "version": 3
++   },
++   "file_extension": ".py",
++   "mimetype": "text/x-python",
++   "name": "python",
++   "nbconvert_exporter": "python",
++   "pygments_lexer": "ipython3",
++   "version": "3.9.13"
++  }
++ },
++ "nbformat": 4,
++ "nbformat_minor": 2
++}
+diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+index cb157af..297047c 100644
+Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+diff --git a/utils/__pycache__/video_augmentation.cpython-39.pyc b/utils/__pycache__/video_augmentation.cpython-39.pyc
+index d4fe444..0efce2c 100644
+Binary files a/utils/__pycache__/video_augmentation.cpython-39.pyc and b/utils/__pycache__/video_augmentation.cpython-39.pyc differ
+diff --git a/utils/decode.py b/utils/decode.py
+index 3877729..ac8dab6 100644
+--- a/utils/decode.py
++++ b/utils/decode.py
+@@ -6,6 +6,38 @@ import ctcdecode
+ import numpy as np
+ from itertools import groupby
+ import torch.nn.functional as F
++import matplotlib.pyplot as plt
 +
-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++# ⬇ 프레임 길이 저장용 전역 리스트
++frame_lengths = []
 +
-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
-         f"{work_dir}/{mode}.txt")
++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++    probs = torch.softmax(nn_output, dim=-1)
++    pred_ids = torch.argmax(probs, dim=-1)
 +
-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
-+        f"{work_dir}/{mode}.txt")
++    length = int(vid_lgt[batch_idx].item())
++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++    x = np.arange(length)
 +
-+    # ✅ 전체 결과 CSV로 저장
-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
-+    os.makedirs(save_folder, exist_ok=True)
-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++    plt.figure(figsize=(15, 4))
++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
 +
-+    rows = []
-+    for file_id in results:
-+        gt = results[file_id]['gloss']
-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
-+        conv_wer = jiwer_wer(gt, conv_pred)
-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++    blank_indices = np.where(pred_seq == blank_id)[0]
++    if len(blank_indices) > 0:
++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
 +
-+        rows.append([
-+            file_id,
-+            gt,
-+            conv_pred,
-+            f"{conv_wer:.4f}",
-+            lstm_pred,
-+            f"{lstm_wer:.4f}"
-+        ])
-+
-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
-+        writer = csv.writer(f)
-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
-+        writer.writerows(rows)
++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++    if sample_id:
++        title_str += f"\nSample: {sample_id}"
++    plt.title(title_str)
++    plt.xlabel("Time Step")
++    plt.ylabel("Gloss ID")
++    plt.yticks(np.unique(pred_seq))
++    plt.grid(True)
++    plt.legend()
++    plt.tight_layout()
++    plt.show()
+ 
+ 
+ class Decode(object):
+@@ -16,35 +48,27 @@ class Decode(object):
+         self.search_mode = search_mode
+         self.blank_id = blank_id
+         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+-                                                    num_processes=10)
++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+ 
+-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+         if not batch_first:
+             nn_output = nn_output.permute(1, 0, 2)
 +
-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++        # ⬇ 프레임 길이 수집
++        global frame_lengths
++        for i in range(vid_lgt.size(0)):
++            frame_lengths.append(int(vid_lgt[i].item()))
 +
++        # sample_id가 존재하면 시각화
++        sample_id = sample_ids[0] if sample_ids else None
++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
 +
+         if self.search_mode == "max":
+             return self.MaxDecode(nn_output, vid_lgt)
+         else:
+             return self.BeamSearch(nn_output, vid_lgt, probs)
+ 
+     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+-        '''
+-        CTCBeamDecoder Shape:
+-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+-                          timesteps (B, N_beams)
+-                          out_lens (B, N_beams)
+-        '''
+-
+-        index_list = torch.argmax(nn_output.cpu(), axis=2)
+-        batchsize, lgt = index_list.shape
+-        blank_rate =[]
+-        for batch_idx in range(batchsize):
+-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+-            blank_rate.append(group_result)
+-
+-
+         if not probs:
+             nn_output = nn_output.softmax(-1).cpu()
+         vid_lgt = vid_lgt.cpu()
+@@ -54,9 +78,7 @@ class Decode(object):
+             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+             if len(first_result) != 0:
+                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+-                             enumerate(first_result)])
++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+         return ret_list
+ 
+     def MaxDecode(self, nn_output, vid_lgt):
+@@ -65,12 +87,34 @@ class Decode(object):
+         ret_list = []
+         for batch_idx in range(batchsize):
+             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++            filtered = [x for x in group_result if x != self.blank_id]
+             if len(filtered) > 0:
+                 max_result = torch.stack(filtered)
+                 max_result = [x[0] for x in groupby(max_result)]
+             else:
+                 max_result = filtered
+-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+-                             enumerate(max_result)])
++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+         return ret_list
 +
-+    # WER 기준 상위 5개 샘플 출력
-+    sample_wers = []
-+    for file_id in results:
-+        gt = results[file_id]['gloss']
-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
-+    
-+        conv_wer = jiwer_wer(gt, conv_pred)
-+        lstm_wer = jiwer_wer(gt, lstm_pred)
-+    
-+        sample_wers.append({
-+            'file_id': file_id,
-+            'gt': gt,
-+            'conv_pred': conv_pred,
-+            'conv_wer': conv_wer,
-+            'lstm_pred': lstm_pred,
-+            'lstm_wer': lstm_wer,
-+            'max_wer': max(conv_wer, lstm_wer)
-+        })
 +
-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
-+    
-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
-+    for sample in top5:
-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
-+        print(f"GT   : {sample['gt']}")
-+        print(f"Conv : {sample['conv_pred']}")
-+        print(f"LSTM : {sample['lstm_pred']}")
-+        print("-" * 60)
++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
++def analyze_frame_lengths():
++    if not frame_lengths:
++        print("⚠ 분석할 frame_lengths가 없습니다.")
++        return
 +
++    print("\n📊 Test Video Frame Length Analysis:")
++    print(f"- Total samples: {len(frame_lengths)}")
++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
++    print(f"- Min length  : {np.min(frame_lengths)}")
++    print(f"- Max length  : {np.max(frame_lengths)}")
 +
++    # 히스토그램 시각화
++    plt.figure(figsize=(10, 5))
++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
++    plt.title("Test Video Frame Length Distribution")
++    plt.xlabel("Frame Length")
++    plt.ylabel("Number of Samples")
++    plt.grid(True)
++    plt.tight_layout()
++    plt.show()
+diff --git a/utils/video_augmentation.py b/utils/video_augmentation.py
+index c52ab2d..89dfafb 100644
+--- a/utils/video_augmentation.py
++++ b/utils/video_augmentation.py
+@@ -304,7 +304,7 @@ class RandomRotation(object):
+             raise TypeError('Expected numpy.ndarray or PIL.Image' +
+                             'but got list of {0}'.format(type(clip[0])))
+         return rotated
+-
++import time
+ 
+ class TemporalRescale(object):
+     def __init__(self, temp_scaling=0.2, frame_interval=1):
+@@ -326,6 +326,8 @@ class TemporalRescale(object):
+             index = sorted(random.sample(range(vid_len), new_len))
+         else:
+             index = sorted(random.choices(range(vid_len), k=new_len))
++        # print(f"[TemporalRescale] 입력 길이: {vid_len}, 출력 길이: {new_len}")
++        # time.sleep(2)
+         return clip[index]
+ 
+ 
+diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+index 7d0a2aa..cd66258 100644
+Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+index c31483a..7ae9234 100644
+--- a/work_dirt/config.yaml
++++ b/work_dirt/config.yaml
+@@ -4,10 +4,10 @@ dataset: phoenix2014
+ dataset_info:
+   dataset_root: /phoenix2014-release/phoenix-2014-multisigner
+   dict_path: ./preprocess/phoenix2014/gloss_dict.npy
+-  evaluation_dir: ./evaluation/slr_eval
++  evaluation_dir: ./evaluation/slr_eval645
+   evaluation_prefix: phoenix2014-groundtruth
+ decode_mode: beam
+-device: your_device
++device: cuda
+ dist_url: env://
+ eval_interval: 1
+ evaluate_tool: python
+diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
+index c126e7a..b01e052 100644
+--- a/work_dirt/dataloader_video.py
++++ b/work_dirt/dataloader_video.py
+@@ -9,6 +9,7 @@ import torch
+ import random
+ import pandas
+ import warnings
++import time
+ 
+ warnings.simplefilter(action='ignore', category=FutureWarning)
+ 
+@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+ 
+             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+-
++            # print('phoenix 데이터를 사용함')
++            # print(img_folder)
+ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+ #            print(img_folder)
+ #            img_list = sorted(glob.glob(img_folder))
+@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+     def read_features(self, index):
+         # load file info
+         fi = self.inputs_list[index]
++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++        time.sleep(10)
+         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+         return data['features'], data['label']
+ 
+@@ -220,7 +228,7 @@ if __name__ == "__main__":
+         dataset=feeder,
+         batch_size=1,
+         shuffle=True,
+-        drop_last=True,
++        drop_last=False,
+         num_workers=0,
+     )
+     for data in dataloader:
+diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
+index 00c1a0e..57f6c0f 100644
+--- a/work_dirt/dev.txt
++++ b/work_dirt/dev.txt
+@@ -8,3 +8,29 @@
+ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
+ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Thu Apr  3 13:53:43 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Thu Apr  3 13:53:43 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++[ Thu Apr  3 16:25:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++[ Thu Apr  3 16:25:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+index 8a22ece..58008ea 100644
+--- a/work_dirt/dirty.patch
++++ b/work_dirt/dirty.patch
+@@ -1,284 +1,20119 @@
+-diff --git a/README.md b/README.md
+-index bdbc17f..8cb240b 100644
+---- a/README.md
+-+++ b/README.md
+-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+- 
+- 
+- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+- 
+- ### Training
+- 
+-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+-index bfc1da8..25ffa61 100644
+---- a/configs/baseline.yaml
+-+++ b/configs/baseline.yaml
+-@@ -1,14 +1,14 @@
+- feeder: dataset.dataloader_video.BaseFeeder
+- phase: train
+--dataset: phoenix2014-T
+-+dataset: phoenix2014
+- #CSL-Daily
+- # dataset: phoenix14-si5
+- 
+- work_dir: ./work_dirt/
+--batch_size: 4
+-+batch_size: 1
+- random_seed: 0 
+--test_batch_size: 4
+--num_worker: 20
+-+test_batch_size: 1
+-+num_worker: 3
+- device: 0
+- log_interval: 10000
+- eval_interval: 1
++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
++index ffef81f..f4065cc 100644
++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++index 7ac0c3b..c89f220 100644
++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
++index 529dabc..b6213f4 100644
++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
+ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+-index 555f4b8..c126e7a 100644
++index c126e7a..b01e052 100644
+ --- a/dataset/dataloader_video.py
+ +++ b/dataset/dataloader_video.py
+-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+-         self.transform_mode = "train" if transform_mode else "test"
+--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+-+
+-         print(mode, len(self))
+-         self.data_aug = self.transform()
+-         print("")
+-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+-             return input_data, label, self.inputs_list[idx]['original_info']
++@@ -9,6 +9,7 @@ import torch
++ import random
++ import pandas
++ import warnings
+++import time
+  
+-     def read_video(self, index):
+--        # load file info
+-         fi = self.inputs_list[index]
+-+    
+-         if 'phoenix' in self.dataset:
+--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+-+#            img_list = sorted(glob.glob(frame_pattern))
+-+#            print(img_list)
+-+
+-+#            print("[LOG] Using phoenix")
+-+
+-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++ warnings.simplefilter(action='ignore', category=FutureWarning)
+  
+-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+-+#            print(img_folder)
+-+#            img_list = sorted(glob.glob(img_folder))
+-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+-+#            print(len(img_list))
+-         elif self.dataset == 'CSL-Daily':
+--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+-+    
+-         img_list = sorted(glob.glob(img_folder))
+-+    
+-+        if len(img_list) == 0:
+-+            print(f"[WARNING] No frames found in: {img_list}")
+-+    
+-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+-+    
+-         label_list = []
+--        if self.dataset=='phoenix2014':
+-+        if self.dataset == 'phoenix2014':
+-             fi['label'] = clean_phoenix_2014(fi['label'])
+--        if self.dataset=='phoenix2014-T':
+--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+-+        elif self.dataset == 'phoenix2014-T':
+-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+-+    
+-         for phase in fi['label'].split(" "):
+--            if phase == '':
+--                continue
+--            if phase in self.dict.keys():
+-+            if phase and phase in self.dict:
+-                 label_list.append(self.dict[phase][0])
+--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+-+    
+-+        video = [
+-+            cv2.cvtColor(
+-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+-+                cv2.COLOR_BGR2RGB
+-+            )   
+-+            for img_path in img_list
+-+        ]
+-+    
+-+        return video, label_list, fi
++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+  
++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++-
+++            # print('phoenix 데이터를 사용함')
+++            # print(img_folder)
++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++ #            print(img_folder)
++ #            img_list = sorted(glob.glob(img_folder))
++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+      def read_features(self, index):
+          # load file info
++         fi = self.inputs_list[index]
+++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++        time.sleep(10)
++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++         return data['features'], data['label']
++ 
++@@ -220,7 +228,7 @@ if __name__ == "__main__":
++         dataset=feeder,
++         batch_size=1,
++         shuffle=True,
++-        drop_last=True,
+++        drop_last=False,
++         num_workers=0,
++     )
++     for data in dataloader:
+ diff --git a/main.py b/main.py
+-index 9e68cee..18ac59b 100644
++index 18ac59b..7f82626 100644
+ --- a/main.py
+ +++ b/main.py
+-@@ -256,7 +256,7 @@ class Processor():
+-                 batch_size=batch_size,
+-                 collate_fn=self.feeder.collate_fn,
+-                 num_workers=self.arg.num_worker,
+--                pin_memory=True,
+-+                pin_memory=False,
+-                 worker_init_fn=self.init_fn,
+-             )
+-             return loader
+-@@ -268,7 +268,7 @@ class Processor():
+-                 drop_last=train_flag,
+-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+-                 collate_fn=self.feeder.collate_fn,
+--                pin_memory=True,
+-+                pin_memory=False,
+-                 worker_init_fn=self.init_fn,
+-             )
++@@ -21,6 +21,7 @@ import utils
++ from seq_scripts import seq_train, seq_eval
++ from torch.cuda.amp import autocast as autocast
++ from utils.misc import *
+++from utils.decode import analyze_frame_lengths
++ class Processor():
++     def __init__(self, arg):
++         self.arg = arg
++@@ -105,13 +106,25 @@ class Processor():
++                 print('Please appoint --weights.')
++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++
+++            train_wer = seq_eval(
+++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++            )
+++            dev_wer = seq_eval(
+++                self.arg, self.data_loader["dev"], self.model, self.device,
+++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++            )
+++            test_wer = seq_eval(
+++                self.arg, self.data_loader["test"], self.model, self.device,
+++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++            )
+++
++             self.recoder.print_log('Evaluation Done.\n')
+++
+++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++            analyze_frame_lengths()
+++
++         elif self.arg.phase == "features":
++             for mode in ["train", "dev", "test"]:
++                 seq_feature_generation(
++@@ -119,6 +132,8 @@ class Processor():
++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++                 )
++ 
+++
+++
++     def save_arg(self):
++         arg_dict = vars(self.arg)
++         if not os.path.exists(self.arg.work_dir):
++@@ -239,6 +254,7 @@ class Processor():
++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
++         print("Loading Dataprocessing finished.")
+++        time.sleep(10)
++     def init_fn(self, worker_id):
++         np.random.seed(int(self.arg.random_seed)+worker_id)
+  
++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++index 71519fd..b9664e1 100644
++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+ diff --git a/seq_scripts.py b/seq_scripts.py
+-index 528856d..d8fcaf9 100644
++index d8fcaf9..77cfc71 100644
+ --- a/seq_scripts.py
+ +++ b/seq_scripts.py
+-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+-     return
+- 
++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++         })
+  
+-+import csv 
+-+from jiwer import wer as jiwer_wer
+- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+-     model.eval()
+--    results=defaultdict(dict)
+-+    results = defaultdict(dict)
+- 
+-     for batch_idx, data in enumerate(tqdm(loader)):
+-         recoder.record_timer("device")
+-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+-                 results[inf]['conv_sents'] = conv_sents
+-                 results[inf]['recognized_sents'] = recognized_sents
+-                 results[inf]['gloss'] = gl
+-+
+-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+-     gls_ref = [results[n]['gloss'] for n in results]
+-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+-+
+-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+--    if wer_results['wer'] < wer_results_con['wer']:
+--        reg_per = wer_results
+--    else:
+--        reg_per = wer_results_con
++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++-    
+++    # 전체 샘플 WER 평균 계산
+++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+ +
+-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++    print("\n📊 전체 평균 WER")
+++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+ +
+-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+-         f"{work_dir}/{mode}.txt")
++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++     for sample in top5:
++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++diff --git a/tmp.ipynb b/tmp.ipynb
++index e69de29..0342039 100644
++--- a/tmp.ipynb
+++++ b/tmp.ipynb
++@@ -0,0 +1,272 @@
+++{
+++ "cells": [
+++  {
+++   "cell_type": "code",
+++   "execution_count": 2,
+++   "metadata": {},
+++   "outputs": [],
+++   "source": [
+++    "import torch\n",
+++    "\n",
+++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++   ]
+++  },
+++  {
+++   "cell_type": "code",
+++   "execution_count": 3,
+++   "metadata": {},
+++   "outputs": [
+++    {
+++     "name": "stdout",
+++     "output_type": "stream",
+++     "text": [
+++      "Model is a state_dict.\n",
+++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++     ]
+++    }
+++   ],
+++   "source": [
+++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++    "    print(\"Model is a state_dict.\")\n",
+++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++   ]
+++  },
+++  {
+++   "cell_type": "code",
+++   "execution_count": 7,
+++   "metadata": {},
+++   "outputs": [],
+++   "source": [
+++    "import torch\n",
+++    "import torch.nn as nn  # <== 여기가 중요\n",
+++    "import torch.nn.functional as F\n"
+++   ]
+++  },
+++  {
+++   "cell_type": "code",
+++   "execution_count": 1,
+++   "metadata": {},
+++   "outputs": [
+++    {
+++     "name": "stderr",
+++     "output_type": "stream",
+++     "text": [
+++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++     ]
+++    }
+++   ],
+++   "source": [
+++    "import torch\n",
+++    "import torch.nn as nn\n",
+++    "import torch.nn.functional as F\n",
+++    "import torchvision.models as models\n",
+++    "import numpy as np\n",
+++    "import modules.resnet as resnet\n",
+++    "from modules import BiLSTMLayer, TemporalConv\n",
+++    "from modules.criterions import SeqKD\n",
+++    "import utils\n",
+++    "import modules.resnet as resnet\n",
+++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++    "class Identity(nn.Module):\n",
+++    "    def __init__(self):\n",
+++    "        super(Identity, self).__init__()\n",
+++    "\n",
+++    "    def forward(self, x):\n",
+++    "        return x\n",
+++    "\n",
+++    "# L2 정규화 선형 레이어\n",
+++    "class NormLinear(nn.Module):\n",
+++    "    def __init__(self, in_dim, out_dim):\n",
+++    "        super(NormLinear, self).__init__()\n",
+++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++    "\n",
+++    "    def forward(self, x):\n",
+++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++    "        return outputs\n",
+++    "\n",
+++    "# SLRModel (수어 인식 모델)\n",
+++    "class SLRModel(nn.Module):\n",
+++    "    def __init__(\n",
+++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++    "            weight_norm=True, share_classifier=True\n",
+++    "    ):\n",
+++    "        super(SLRModel, self).__init__()\n",
+++    "        self.decoder = None\n",
+++    "        self.loss = dict()\n",
+++    "        self.criterion_init()\n",
+++    "        self.num_classes = num_classes\n",
+++    "        self.loss_weights = loss_weights\n",
+++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++    "\n",
+++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++    "                                   hidden_size=hidden_size,\n",
+++    "                                   conv_type=conv_type,\n",
+++    "                                   use_bn=use_bn,\n",
+++    "                                   num_classes=num_classes)\n",
+++    "\n",
+++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++    "\n",
+++    "        # BiLSTM 기반 Temporal Model\n",
+++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++    "                                          num_layers=2, bidirectional=True)\n",
+++    "\n",
+++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++    "        if weight_norm:\n",
+++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++    "        else:\n",
+++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++    "\n",
+++    "        # Classifier 공유 여부\n",
+++    "        if share_classifier:\n",
+++    "            self.conv1d.fc = self.classifier\n",
+++    "\n",
+++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++    "        # CNN으로 Frame-wise Feature 추출\n",
+++    "        if len(x.shape) == 5:\n",
+++    "            batch, temp, channel, height, width = x.shape\n",
+++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++    "        else:\n",
+++    "            framewise = x\n",
+++    "\n",
+++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++    "        x = conv1d_outputs['visual_feat']\n",
+++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++    "\n",
+++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++    "\n",
+++    "        # 최종 Classifier 적용\n",
+++    "        outputs = self.classifier(features_before_classifier)\n",
+++    "\n",
+++    "        # Inference 모드에서 Decoding\n",
+++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++    "\n",
+++    "        return {\n",
+++    "            \"framewise_features\": framewise,\n",
+++    "            \"visual_features\": x,\n",
+++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++    "            \"feat_len\": lgt,\n",
+++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++    "            \"sequence_logits\": outputs,\n",
+++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++    "            \"conv_sents\": conv_pred,\n",
+++    "            \"recognized_sents\": pred,\n",
+++    "        }\n",
+++    "\n",
+++    "    def criterion_init(self):\n",
+++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++    "        return self.loss\n"
+++   ]
+++  },
+++  {
+++   "cell_type": "code",
+++   "execution_count": 6,
+++   "metadata": {},
+++   "outputs": [
+++    {
+++     "ename": "KeyError",
+++     "evalue": "'dataset_info'",
+++     "output_type": "error",
+++     "traceback": [
+++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++     ]
+++    }
+++   ],
+++   "source": [
+++    "import os\n",
+++    "import numpy as np\n",
+++    "import yaml\n",
+++    "\n",
+++    "# 환경 변수 설정\n",
+++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++    "\n",
+++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++    "with open(config_path, \"r\") as f:\n",
+++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++    "\n",
+++    "# ✅ gloss_dict 로드\n",
+++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++    "\n",
+++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++   ]
+++  },
+++  {
+++   "cell_type": "code",
+++   "execution_count": 5,
+++   "metadata": {},
+++   "outputs": [
+++    {
+++     "ename": "AttributeError",
+++     "evalue": "'NoneType' object has no attribute 'items'",
+++     "output_type": "error",
+++     "traceback": [
+++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++     ]
+++    }
+++   ],
+++   "source": [
+++    "import torch\n",
+++    "\n",
+++    "# 모델 불러오기\n",
+++    "model = SLRModel(\n",
+++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++    ")\n",
+++    "\n",
+++    "# 저장된 가중치 로드\n",
+++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++    "\n",
+++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++    "if isinstance(state_dict, dict):\n",
+++    "    model.load_state_dict(state_dict)\n",
+++    "\n",
+++    "# 모델을 평가 모드로 설정\n",
+++    "model.eval()\n"
+++   ]
+++  }
+++ ],
+++ "metadata": {
+++  "kernelspec": {
+++   "display_name": "3.9.13",
+++   "language": "python",
+++   "name": "python3"
+++  },
+++  "language_info": {
+++   "codemirror_mode": {
+++    "name": "ipython",
+++    "version": 3
+++   },
+++   "file_extension": ".py",
+++   "mimetype": "text/x-python",
+++   "name": "python",
+++   "nbconvert_exporter": "python",
+++   "pygments_lexer": "ipython3",
+++   "version": "3.9.13"
+++  }
+++ },
+++ "nbformat": 4,
+++ "nbformat_minor": 2
+++}
++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++index cb157af..c502b5d 100644
++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++diff --git a/utils/__pycache__/video_augmentation.cpython-39.pyc b/utils/__pycache__/video_augmentation.cpython-39.pyc
++index d4fe444..5b044df 100644
++Binary files a/utils/__pycache__/video_augmentation.cpython-39.pyc and b/utils/__pycache__/video_augmentation.cpython-39.pyc differ
++diff --git a/utils/decode.py b/utils/decode.py
++index 3877729..ac8dab6 100644
++--- a/utils/decode.py
+++++ b/utils/decode.py
++@@ -6,6 +6,38 @@ import ctcdecode
++ import numpy as np
++ from itertools import groupby
++ import torch.nn.functional as F
+++import matplotlib.pyplot as plt
+ +
+-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+-+        f"{work_dir}/{mode}.txt")
+++# ⬇ 프레임 길이 저장용 전역 리스트
+++frame_lengths = []
+ +
+-+    # ✅ 전체 결과 CSV로 저장
+-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+-+    os.makedirs(save_folder, exist_ok=True)
+-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++    probs = torch.softmax(nn_output, dim=-1)
+++    pred_ids = torch.argmax(probs, dim=-1)
+ +
+-+    rows = []
+-+    for file_id in results:
+-+        gt = results[file_id]['gloss']
+-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+-+        conv_wer = jiwer_wer(gt, conv_pred)
+-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++    length = int(vid_lgt[batch_idx].item())
+++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++    x = np.arange(length)
+ +
+-+        rows.append([
+-+            file_id,
+-+            gt,
+-+            conv_pred,
+-+            f"{conv_wer:.4f}",
+-+            lstm_pred,
+-+            f"{lstm_wer:.4f}"
+-+        ])
+-+
+-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+-+        writer = csv.writer(f)
+-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+-+        writer.writerows(rows)
+-+
+-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++    plt.figure(figsize=(15, 4))
+++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+ +
+++    blank_indices = np.where(pred_seq == blank_id)[0]
+++    if len(blank_indices) > 0:
+++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+ +
+++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++    if sample_id:
+++        title_str += f"\nSample: {sample_id}"
+++    plt.title(title_str)
+++    plt.xlabel("Time Step")
+++    plt.ylabel("Gloss ID")
+++    plt.yticks(np.unique(pred_seq))
+++    plt.grid(True)
+++    plt.legend()
+++    plt.tight_layout()
+++    plt.show()
++ 
++ 
++ class Decode(object):
++@@ -16,35 +48,27 @@ class Decode(object):
++         self.search_mode = search_mode
++         self.blank_id = blank_id
++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++-                                                    num_processes=10)
+++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++ 
++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++         if not batch_first:
++             nn_output = nn_output.permute(1, 0, 2)
+ +
+-+    # WER 기준 상위 5개 샘플 출력
+-+    sample_wers = []
+-+    for file_id in results:
+-+        gt = results[file_id]['gloss']
+-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+-+    
+-+        conv_wer = jiwer_wer(gt, conv_pred)
+-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+-+    
+-+        sample_wers.append({
+-+            'file_id': file_id,
+-+            'gt': gt,
+-+            'conv_pred': conv_pred,
+-+            'conv_wer': conv_wer,
+-+            'lstm_pred': lstm_pred,
+-+            'lstm_wer': lstm_wer,
+-+            'max_wer': max(conv_wer, lstm_wer)
+-+        })
+++        # ⬇ 프레임 길이 수집
+++        global frame_lengths
+++        for i in range(vid_lgt.size(0)):
+++            frame_lengths.append(int(vid_lgt[i].item()))
+ +
+-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+-+    
+-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+-+    for sample in top5:
+-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+-+        print(f"GT   : {sample['gt']}")
+-+        print(f"Conv : {sample['conv_pred']}")
+-+        print(f"LSTM : {sample['lstm_pred']}")
+-+        print("-" * 60)
+++        # sample_id가 존재하면 시각화
+++        sample_id = sample_ids[0] if sample_ids else None
+++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+ +
++         if self.search_mode == "max":
++             return self.MaxDecode(nn_output, vid_lgt)
++         else:
++             return self.BeamSearch(nn_output, vid_lgt, probs)
++ 
++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++-        '''
++-        CTCBeamDecoder Shape:
++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++-                          timesteps (B, N_beams)
++-                          out_lens (B, N_beams)
++-        '''
++-
++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
++-        batchsize, lgt = index_list.shape
++-        blank_rate =[]
++-        for batch_idx in range(batchsize):
++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++-            blank_rate.append(group_result)
++-
++-
++         if not probs:
++             nn_output = nn_output.softmax(-1).cpu()
++         vid_lgt = vid_lgt.cpu()
++@@ -54,9 +78,7 @@ class Decode(object):
++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++             if len(first_result) != 0:
++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++-                             enumerate(first_result)])
+++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++         return ret_list
++ 
++     def MaxDecode(self, nn_output, vid_lgt):
++@@ -65,12 +87,34 @@ class Decode(object):
++         ret_list = []
++         for batch_idx in range(batchsize):
++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++            filtered = [x for x in group_result if x != self.blank_id]
++             if len(filtered) > 0:
++                 max_result = torch.stack(filtered)
++                 max_result = [x[0] for x in groupby(max_result)]
++             else:
++                 max_result = filtered
++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++-                             enumerate(max_result)])
+++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
++         return ret_list
+ +
+ +
+++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
+++def analyze_frame_lengths():
+++    if not frame_lengths:
+++        print("⚠ 분석할 frame_lengths가 없습니다.")
+++        return
+ +
+++    print("\n📊 Test Video Frame Length Analysis:")
+++    print(f"- Total samples: {len(frame_lengths)}")
+++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
+++    print(f"- Min length  : {np.min(frame_lengths)}")
+++    print(f"- Max length  : {np.max(frame_lengths)}")
+ +
+++    # 히스토그램 시각화
+++    plt.figure(figsize=(10, 5))
+++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
+++    plt.title("Test Video Frame Length Distribution")
+++    plt.xlabel("Frame Length")
+++    plt.ylabel("Number of Samples")
+++    plt.grid(True)
+++    plt.tight_layout()
+++    plt.show()
++diff --git a/utils/video_augmentation.py b/utils/video_augmentation.py
++index c52ab2d..417a070 100644
++--- a/utils/video_augmentation.py
+++++ b/utils/video_augmentation.py
++@@ -304,7 +304,7 @@ class RandomRotation(object):
++             raise TypeError('Expected numpy.ndarray or PIL.Image' +
++                             'but got list of {0}'.format(type(clip[0])))
++         return rotated
++-
+++import time
++ 
++ class TemporalRescale(object):
++     def __init__(self, temp_scaling=0.2, frame_interval=1):
++@@ -326,6 +326,8 @@ class TemporalRescale(object):
++             index = sorted(random.sample(range(vid_len), new_len))
++         else:
++             index = sorted(random.choices(range(vid_len), k=new_len))
+++        print(f"[TemporalRescale] 입력 길이: {vid_len}, 출력 길이: {new_len}")
+++        time.sleep(2)
++         return clip[index]
++ 
++ 
++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++index 7d0a2aa..cd66258 100644
++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++index c31483a..239691c 100644
++--- a/work_dirt/config.yaml
+++++ b/work_dirt/config.yaml
++@@ -7,7 +7,7 @@ dataset_info:
++   evaluation_dir: ./evaluation/slr_eval
++   evaluation_prefix: phoenix2014-groundtruth
++ decode_mode: beam
++-device: your_device
+++device: cuda
++ dist_url: env://
++ eval_interval: 1
++ evaluate_tool: python
++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
++index c126e7a..b01e052 100644
++--- a/work_dirt/dataloader_video.py
+++++ b/work_dirt/dataloader_video.py
++@@ -9,6 +9,7 @@ import torch
++ import random
++ import pandas
++ import warnings
+++import time
++ 
++ warnings.simplefilter(action='ignore', category=FutureWarning)
++ 
++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++ 
++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++-
+++            # print('phoenix 데이터를 사용함')
+++            # print(img_folder)
++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++ #            print(img_folder)
++ #            img_list = sorted(glob.glob(img_folder))
++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++     def read_features(self, index):
++         # load file info
++         fi = self.inputs_list[index]
+++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++        time.sleep(10)
++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++         return data['features'], data['label']
++ 
++@@ -220,7 +228,7 @@ if __name__ == "__main__":
++         dataset=feeder,
++         batch_size=1,
++         shuffle=True,
++-        drop_last=True,
+++        drop_last=False,
++         num_workers=0,
++     )
++     for data in dataloader:
++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
++index 00c1a0e..5d86cb6 100644
++--- a/work_dirt/dev.txt
+++++ b/work_dirt/dev.txt
++@@ -8,3 +8,27 @@
++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++[ Thu Apr  3 13:53:43 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++[ Thu Apr  3 13:53:43 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++index 8a22ece..335d5aa 100644
++--- a/work_dirt/dirty.patch
+++++ b/work_dirt/dirty.patch
++@@ -1,284 +1,18994 @@
++-diff --git a/README.md b/README.md
++-index bdbc17f..8cb240b 100644
++---- a/README.md
++-+++ b/README.md
++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++- 
++- 
++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++- 
++- ### Training
++- 
++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++-index bfc1da8..25ffa61 100644
++---- a/configs/baseline.yaml
++-+++ b/configs/baseline.yaml
++-@@ -1,14 +1,14 @@
++- feeder: dataset.dataloader_video.BaseFeeder
++- phase: train
++--dataset: phoenix2014-T
++-+dataset: phoenix2014
++- #CSL-Daily
++- # dataset: phoenix14-si5
++- 
++- work_dir: ./work_dirt/
++--batch_size: 4
++-+batch_size: 1
++- random_seed: 0 
++--test_batch_size: 4
++--num_worker: 20
++-+test_batch_size: 1
++-+num_worker: 3
++- device: 0
++- log_interval: 10000
++- eval_interval: 1
+++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
+++index ffef81f..f4065cc 100644
+++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
+++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++index 7ac0c3b..c89f220 100644
+++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
+++index 529dabc..b6213f4 100644
+++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++-index 555f4b8..c126e7a 100644
+++index c126e7a..b01e052 100644
++ --- a/dataset/dataloader_video.py
++ +++ b/dataset/dataloader_video.py
++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++-         self.transform_mode = "train" if transform_mode else "test"
++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++-+
++-         print(mode, len(self))
++-         self.data_aug = self.transform()
++-         print("")
++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++-             return input_data, label, self.inputs_list[idx]['original_info']
+++@@ -9,6 +9,7 @@ import torch
+++ import random
+++ import pandas
+++ import warnings
++++import time
++  
++-     def read_video(self, index):
++--        # load file info
++-         fi = self.inputs_list[index]
++-+    
++-         if 'phoenix' in self.dataset:
++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++-+#            img_list = sorted(glob.glob(frame_pattern))
++-+#            print(img_list)
++-+
++-+#            print("[LOG] Using phoenix")
++-+
++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++ warnings.simplefilter(action='ignore', category=FutureWarning)
++  
++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++-+#            print(img_folder)
++-+#            img_list = sorted(glob.glob(img_folder))
++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++-+#            print(len(img_list))
++-         elif self.dataset == 'CSL-Daily':
++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++-+    
++-         img_list = sorted(glob.glob(img_folder))
++-+    
++-+        if len(img_list) == 0:
++-+            print(f"[WARNING] No frames found in: {img_list}")
++-+    
++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++-+    
++-         label_list = []
++--        if self.dataset=='phoenix2014':
++-+        if self.dataset == 'phoenix2014':
++-             fi['label'] = clean_phoenix_2014(fi['label'])
++--        if self.dataset=='phoenix2014-T':
++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++-+        elif self.dataset == 'phoenix2014-T':
++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++-+    
++-         for phase in fi['label'].split(" "):
++--            if phase == '':
++--                continue
++--            if phase in self.dict.keys():
++-+            if phase and phase in self.dict:
++-                 label_list.append(self.dict[phase][0])
++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++-+    
++-+        video = [
++-+            cv2.cvtColor(
++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++-+                cv2.COLOR_BGR2RGB
++-+            )   
++-+            for img_path in img_list
++-+        ]
++-+    
++-+        return video, label_list, fi
+++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++  
+++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++-
++++            # print('phoenix 데이터를 사용함')
++++            # print(img_folder)
+++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++ #            print(img_folder)
+++ #            img_list = sorted(glob.glob(img_folder))
+++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++      def read_features(self, index):
++          # load file info
+++         fi = self.inputs_list[index]
++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++        time.sleep(10)
+++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++         return data['features'], data['label']
+++ 
+++@@ -220,7 +228,7 @@ if __name__ == "__main__":
+++         dataset=feeder,
+++         batch_size=1,
+++         shuffle=True,
+++-        drop_last=True,
++++        drop_last=False,
+++         num_workers=0,
+++     )
+++     for data in dataloader:
++ diff --git a/main.py b/main.py
++-index 9e68cee..18ac59b 100644
+++index 18ac59b..7f82626 100644
++ --- a/main.py
++ +++ b/main.py
++-@@ -256,7 +256,7 @@ class Processor():
++-                 batch_size=batch_size,
++-                 collate_fn=self.feeder.collate_fn,
++-                 num_workers=self.arg.num_worker,
++--                pin_memory=True,
++-+                pin_memory=False,
++-                 worker_init_fn=self.init_fn,
++-             )
++-             return loader
++-@@ -268,7 +268,7 @@ class Processor():
++-                 drop_last=train_flag,
++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++-                 collate_fn=self.feeder.collate_fn,
++--                pin_memory=True,
++-+                pin_memory=False,
++-                 worker_init_fn=self.init_fn,
++-             )
+++@@ -21,6 +21,7 @@ import utils
+++ from seq_scripts import seq_train, seq_eval
+++ from torch.cuda.amp import autocast as autocast
+++ from utils.misc import *
++++from utils.decode import analyze_frame_lengths
+++ class Processor():
+++     def __init__(self, arg):
+++         self.arg = arg
+++@@ -105,13 +106,25 @@ class Processor():
+++                 print('Please appoint --weights.')
+++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++
++++            train_wer = seq_eval(
++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++            )
++++            dev_wer = seq_eval(
++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++            )
++++            test_wer = seq_eval(
++++                self.arg, self.data_loader["test"], self.model, self.device,
++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++            )
++++
+++             self.recoder.print_log('Evaluation Done.\n')
++++
++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++            analyze_frame_lengths()
++++
+++         elif self.arg.phase == "features":
+++             for mode in ["train", "dev", "test"]:
+++                 seq_feature_generation(
+++@@ -119,6 +132,8 @@ class Processor():
+++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++                 )
++  
++++
++++
+++     def save_arg(self):
+++         arg_dict = vars(self.arg)
+++         if not os.path.exists(self.arg.work_dir):
+++@@ -239,6 +254,7 @@ class Processor():
+++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
+++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
+++         print("Loading Dataprocessing finished.")
++++        time.sleep(10)
+++     def init_fn(self, worker_id):
+++         np.random.seed(int(self.arg.random_seed)+worker_id)
+++ 
+++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++index 71519fd..b9664e1 100644
+++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++ diff --git a/seq_scripts.py b/seq_scripts.py
++-index 528856d..d8fcaf9 100644
+++index d8fcaf9..77cfc71 100644
++ --- a/seq_scripts.py
++ +++ b/seq_scripts.py
++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++-     return
++- 
++- 
++-+import csv 
++-+from jiwer import wer as jiwer_wer
++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++-     model.eval()
++--    results=defaultdict(dict)
++-+    results = defaultdict(dict)
+++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++         })
++  
++-     for batch_idx, data in enumerate(tqdm(loader)):
++-         recoder.record_timer("device")
++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++-                 results[inf]['conv_sents'] = conv_sents
++-                 results[inf]['recognized_sents'] = recognized_sents
++-                 results[inf]['gloss'] = gl
+++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++-    
++++    # 전체 샘플 WER 평균 계산
++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
++ +
++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++-     gls_ref = [results[n]['gloss'] for n in results]
++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++    print("\n📊 전체 평균 WER")
++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
++ +
++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++--    if wer_results['wer'] < wer_results_con['wer']:
++--        reg_per = wer_results
++--    else:
++--        reg_per = wer_results_con
+++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++     for sample in top5:
+++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++diff --git a/tmp.ipynb b/tmp.ipynb
+++index e69de29..0342039 100644
+++--- a/tmp.ipynb
++++++ b/tmp.ipynb
+++@@ -0,0 +1,272 @@
++++{
++++ "cells": [
++++  {
++++   "cell_type": "code",
++++   "execution_count": 2,
++++   "metadata": {},
++++   "outputs": [],
++++   "source": [
++++    "import torch\n",
++++    "\n",
++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++   ]
++++  },
++++  {
++++   "cell_type": "code",
++++   "execution_count": 3,
++++   "metadata": {},
++++   "outputs": [
++++    {
++++     "name": "stdout",
++++     "output_type": "stream",
++++     "text": [
++++      "Model is a state_dict.\n",
++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++     ]
++++    }
++++   ],
++++   "source": [
++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++    "    print(\"Model is a state_dict.\")\n",
++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++   ]
++++  },
++++  {
++++   "cell_type": "code",
++++   "execution_count": 7,
++++   "metadata": {},
++++   "outputs": [],
++++   "source": [
++++    "import torch\n",
++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++    "import torch.nn.functional as F\n"
++++   ]
++++  },
++++  {
++++   "cell_type": "code",
++++   "execution_count": 1,
++++   "metadata": {},
++++   "outputs": [
++++    {
++++     "name": "stderr",
++++     "output_type": "stream",
++++     "text": [
++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++     ]
++++    }
++++   ],
++++   "source": [
++++    "import torch\n",
++++    "import torch.nn as nn\n",
++++    "import torch.nn.functional as F\n",
++++    "import torchvision.models as models\n",
++++    "import numpy as np\n",
++++    "import modules.resnet as resnet\n",
++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++    "from modules.criterions import SeqKD\n",
++++    "import utils\n",
++++    "import modules.resnet as resnet\n",
++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++    "class Identity(nn.Module):\n",
++++    "    def __init__(self):\n",
++++    "        super(Identity, self).__init__()\n",
++++    "\n",
++++    "    def forward(self, x):\n",
++++    "        return x\n",
++++    "\n",
++++    "# L2 정규화 선형 레이어\n",
++++    "class NormLinear(nn.Module):\n",
++++    "    def __init__(self, in_dim, out_dim):\n",
++++    "        super(NormLinear, self).__init__()\n",
++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++    "\n",
++++    "    def forward(self, x):\n",
++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++    "        return outputs\n",
++++    "\n",
++++    "# SLRModel (수어 인식 모델)\n",
++++    "class SLRModel(nn.Module):\n",
++++    "    def __init__(\n",
++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++    "            weight_norm=True, share_classifier=True\n",
++++    "    ):\n",
++++    "        super(SLRModel, self).__init__()\n",
++++    "        self.decoder = None\n",
++++    "        self.loss = dict()\n",
++++    "        self.criterion_init()\n",
++++    "        self.num_classes = num_classes\n",
++++    "        self.loss_weights = loss_weights\n",
++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++    "\n",
++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++    "                                   hidden_size=hidden_size,\n",
++++    "                                   conv_type=conv_type,\n",
++++    "                                   use_bn=use_bn,\n",
++++    "                                   num_classes=num_classes)\n",
++++    "\n",
++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++    "\n",
++++    "        # BiLSTM 기반 Temporal Model\n",
++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++    "                                          num_layers=2, bidirectional=True)\n",
++++    "\n",
++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++    "        if weight_norm:\n",
++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++    "        else:\n",
++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++    "\n",
++++    "        # Classifier 공유 여부\n",
++++    "        if share_classifier:\n",
++++    "            self.conv1d.fc = self.classifier\n",
++++    "\n",
++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++    "        if len(x.shape) == 5:\n",
++++    "            batch, temp, channel, height, width = x.shape\n",
++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++    "        else:\n",
++++    "            framewise = x\n",
++++    "\n",
++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++    "        x = conv1d_outputs['visual_feat']\n",
++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++    "\n",
++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++    "\n",
++++    "        # 최종 Classifier 적용\n",
++++    "        outputs = self.classifier(features_before_classifier)\n",
++++    "\n",
++++    "        # Inference 모드에서 Decoding\n",
++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++    "\n",
++++    "        return {\n",
++++    "            \"framewise_features\": framewise,\n",
++++    "            \"visual_features\": x,\n",
++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++    "            \"feat_len\": lgt,\n",
++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++    "            \"sequence_logits\": outputs,\n",
++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++    "            \"conv_sents\": conv_pred,\n",
++++    "            \"recognized_sents\": pred,\n",
++++    "        }\n",
++++    "\n",
++++    "    def criterion_init(self):\n",
++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++    "        return self.loss\n"
++++   ]
++++  },
++++  {
++++   "cell_type": "code",
++++   "execution_count": 6,
++++   "metadata": {},
++++   "outputs": [
++++    {
++++     "ename": "KeyError",
++++     "evalue": "'dataset_info'",
++++     "output_type": "error",
++++     "traceback": [
++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++     ]
++++    }
++++   ],
++++   "source": [
++++    "import os\n",
++++    "import numpy as np\n",
++++    "import yaml\n",
++++    "\n",
++++    "# 환경 변수 설정\n",
++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++    "\n",
++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++    "with open(config_path, \"r\") as f:\n",
++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++    "\n",
++++    "# ✅ gloss_dict 로드\n",
++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++    "\n",
++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++   ]
++++  },
++++  {
++++   "cell_type": "code",
++++   "execution_count": 5,
++++   "metadata": {},
++++   "outputs": [
++++    {
++++     "ename": "AttributeError",
++++     "evalue": "'NoneType' object has no attribute 'items'",
++++     "output_type": "error",
++++     "traceback": [
++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++     ]
++++    }
++++   ],
++++   "source": [
++++    "import torch\n",
++++    "\n",
++++    "# 모델 불러오기\n",
++++    "model = SLRModel(\n",
++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++    ")\n",
++++    "\n",
++++    "# 저장된 가중치 로드\n",
++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++    "\n",
++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++    "if isinstance(state_dict, dict):\n",
++++    "    model.load_state_dict(state_dict)\n",
++++    "\n",
++++    "# 모델을 평가 모드로 설정\n",
++++    "model.eval()\n"
++++   ]
++++  }
++++ ],
++++ "metadata": {
++++  "kernelspec": {
++++   "display_name": "3.9.13",
++++   "language": "python",
++++   "name": "python3"
++++  },
++++  "language_info": {
++++   "codemirror_mode": {
++++    "name": "ipython",
++++    "version": 3
++++   },
++++   "file_extension": ".py",
++++   "mimetype": "text/x-python",
++++   "name": "python",
++++   "nbconvert_exporter": "python",
++++   "pygments_lexer": "ipython3",
++++   "version": "3.9.13"
++++  }
++++ },
++++ "nbformat": 4,
++++ "nbformat_minor": 2
++++}
+++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++index cb157af..c502b5d 100644
+++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++diff --git a/utils/__pycache__/video_augmentation.cpython-39.pyc b/utils/__pycache__/video_augmentation.cpython-39.pyc
+++index d4fe444..47b57b8 100644
+++Binary files a/utils/__pycache__/video_augmentation.cpython-39.pyc and b/utils/__pycache__/video_augmentation.cpython-39.pyc differ
+++diff --git a/utils/decode.py b/utils/decode.py
+++index 3877729..ac8dab6 100644
+++--- a/utils/decode.py
++++++ b/utils/decode.py
+++@@ -6,6 +6,38 @@ import ctcdecode
+++ import numpy as np
+++ from itertools import groupby
+++ import torch.nn.functional as F
++++import matplotlib.pyplot as plt
++ +
++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++# ⬇ 프레임 길이 저장용 전역 리스트
++++frame_lengths = []
++ +
++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++-         f"{work_dir}/{mode}.txt")
++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++    probs = torch.softmax(nn_output, dim=-1)
++++    pred_ids = torch.argmax(probs, dim=-1)
++ +
++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++-+        f"{work_dir}/{mode}.txt")
++++    length = int(vid_lgt[batch_idx].item())
++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++    x = np.arange(length)
++ +
++-+    # ✅ 전체 결과 CSV로 저장
++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++-+    os.makedirs(save_folder, exist_ok=True)
++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++-+
++-+    rows = []
++-+    for file_id in results:
++-+        gt = results[file_id]['gloss']
++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++-+        conv_wer = jiwer_wer(gt, conv_pred)
++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++-+
++-+        rows.append([
++-+            file_id,
++-+            gt,
++-+            conv_pred,
++-+            f"{conv_wer:.4f}",
++-+            lstm_pred,
++-+            f"{lstm_wer:.4f}"
++-+        ])
++-+
++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++-+        writer = csv.writer(f)
++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++-+        writer.writerows(rows)
++-+
++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++++    plt.figure(figsize=(15, 4))
++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++ +
++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++    if len(blank_indices) > 0:
++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++ +
++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++    if sample_id:
++++        title_str += f"\nSample: {sample_id}"
++++    plt.title(title_str)
++++    plt.xlabel("Time Step")
++++    plt.ylabel("Gloss ID")
++++    plt.yticks(np.unique(pred_seq))
++++    plt.grid(True)
++++    plt.legend()
++++    plt.tight_layout()
++++    plt.show()
+++ 
+++ 
+++ class Decode(object):
+++@@ -16,35 +48,27 @@ class Decode(object):
+++         self.search_mode = search_mode
+++         self.blank_id = blank_id
+++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+++-                                                    num_processes=10)
++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+++ 
+++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+++         if not batch_first:
+++             nn_output = nn_output.permute(1, 0, 2)
++ +
++-+    # WER 기준 상위 5개 샘플 출력
++-+    sample_wers = []
++-+    for file_id in results:
++-+        gt = results[file_id]['gloss']
++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++-+    
++-+        conv_wer = jiwer_wer(gt, conv_pred)
++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++-+    
++-+        sample_wers.append({
++-+            'file_id': file_id,
++-+            'gt': gt,
++-+            'conv_pred': conv_pred,
++-+            'conv_wer': conv_wer,
++-+            'lstm_pred': lstm_pred,
++-+            'lstm_wer': lstm_wer,
++-+            'max_wer': max(conv_wer, lstm_wer)
++-+        })
++++        # ⬇ 프레임 길이 수집
++++        global frame_lengths
++++        for i in range(vid_lgt.size(0)):
++++            frame_lengths.append(int(vid_lgt[i].item()))
++ +
++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++-+    
++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++-+    for sample in top5:
++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++-+        print(f"GT   : {sample['gt']}")
++-+        print(f"Conv : {sample['conv_pred']}")
++-+        print(f"LSTM : {sample['lstm_pred']}")
++-+        print("-" * 60)
++++        # sample_id가 존재하면 시각화
++++        sample_id = sample_ids[0] if sample_ids else None
++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
++ +
+++         if self.search_mode == "max":
+++             return self.MaxDecode(nn_output, vid_lgt)
+++         else:
+++             return self.BeamSearch(nn_output, vid_lgt, probs)
+++ 
+++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+++-        '''
+++-        CTCBeamDecoder Shape:
+++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+++-                          timesteps (B, N_beams)
+++-                          out_lens (B, N_beams)
+++-        '''
+++-
+++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
+++-        batchsize, lgt = index_list.shape
+++-        blank_rate =[]
+++-        for batch_idx in range(batchsize):
+++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+++-            blank_rate.append(group_result)
+++-
+++-
+++         if not probs:
+++             nn_output = nn_output.softmax(-1).cpu()
+++         vid_lgt = vid_lgt.cpu()
+++@@ -54,9 +78,7 @@ class Decode(object):
+++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+++             if len(first_result) != 0:
+++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+++-                             enumerate(first_result)])
++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+++         return ret_list
+++ 
+++     def MaxDecode(self, nn_output, vid_lgt):
+++@@ -65,12 +87,34 @@ class Decode(object):
+++         ret_list = []
+++         for batch_idx in range(batchsize):
+++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++++            filtered = [x for x in group_result if x != self.blank_id]
+++             if len(filtered) > 0:
+++                 max_result = torch.stack(filtered)
+++                 max_result = [x[0] for x in groupby(max_result)]
+++             else:
+++                 max_result = filtered
+++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+++-                             enumerate(max_result)])
++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++         return ret_list
++ +
++ +
++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
++++def analyze_frame_lengths():
++++    if not frame_lengths:
++++        print("⚠ 분석할 frame_lengths가 없습니다.")
++++        return
++ +
++++    print("\n📊 Test Video Frame Length Analysis:")
++++    print(f"- Total samples: {len(frame_lengths)}")
++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
++++    print(f"- Min length  : {np.min(frame_lengths)}")
++++    print(f"- Max length  : {np.max(frame_lengths)}")
++ +
++++    # 히스토그램 시각화
++++    plt.figure(figsize=(10, 5))
++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
++++    plt.title("Test Video Frame Length Distribution")
++++    plt.xlabel("Frame Length")
++++    plt.ylabel("Number of Samples")
++++    plt.grid(True)
++++    plt.tight_layout()
++++    plt.show()
+++diff --git a/utils/video_augmentation.py b/utils/video_augmentation.py
+++index c52ab2d..02445fc 100644
+++--- a/utils/video_augmentation.py
++++++ b/utils/video_augmentation.py
+++@@ -326,6 +326,7 @@ class TemporalRescale(object):
+++             index = sorted(random.sample(range(vid_len), new_len))
+++         else:
+++             index = sorted(random.choices(range(vid_len), k=new_len))
++++        print(f"[TemporalRescale] 입력 길이: {vid_len}, 출력 길이: {new_len}")
+++         return clip[index]
+++ 
+++ 
+++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++index 7d0a2aa..cd66258 100644
+++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++index c31483a..239691c 100644
+++--- a/work_dirt/config.yaml
++++++ b/work_dirt/config.yaml
+++@@ -7,7 +7,7 @@ dataset_info:
+++   evaluation_dir: ./evaluation/slr_eval
+++   evaluation_prefix: phoenix2014-groundtruth
+++ decode_mode: beam
+++-device: your_device
++++device: cuda
+++ dist_url: env://
+++ eval_interval: 1
+++ evaluate_tool: python
+++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
+++index c126e7a..b01e052 100644
+++--- a/work_dirt/dataloader_video.py
++++++ b/work_dirt/dataloader_video.py
+++@@ -9,6 +9,7 @@ import torch
+++ import random
+++ import pandas
+++ import warnings
++++import time
+++ 
+++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++ 
+++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++ 
+++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++-
++++            # print('phoenix 데이터를 사용함')
++++            # print(img_folder)
+++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++ #            print(img_folder)
+++ #            img_list = sorted(glob.glob(img_folder))
+++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++     def read_features(self, index):
+++         # load file info
+++         fi = self.inputs_list[index]
++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++        time.sleep(10)
+++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++         return data['features'], data['label']
+++ 
+++@@ -220,7 +228,7 @@ if __name__ == "__main__":
+++         dataset=feeder,
+++         batch_size=1,
+++         shuffle=True,
+++-        drop_last=True,
++++        drop_last=False,
+++         num_workers=0,
+++     )
+++     for data in dataloader:
+++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
+++index 00c1a0e..5d86cb6 100644
+++--- a/work_dirt/dev.txt
++++++ b/work_dirt/dev.txt
+++@@ -8,3 +8,27 @@
+++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
+++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++[ Thu Apr  3 13:53:43 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++[ Thu Apr  3 13:53:43 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++index 8a22ece..2e115e7 100644
+++--- a/work_dirt/dirty.patch
++++++ b/work_dirt/dirty.patch
+++@@ -1,284 +1,17880 @@
+++-diff --git a/README.md b/README.md
+++-index bdbc17f..8cb240b 100644
+++---- a/README.md
+++-+++ b/README.md
+++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++- 
+++- 
+++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++- 
+++- ### Training
+++- 
+++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++-index bfc1da8..25ffa61 100644
+++---- a/configs/baseline.yaml
+++-+++ b/configs/baseline.yaml
+++-@@ -1,14 +1,14 @@
+++- feeder: dataset.dataloader_video.BaseFeeder
+++- phase: train
+++--dataset: phoenix2014-T
+++-+dataset: phoenix2014
+++- #CSL-Daily
+++- # dataset: phoenix14-si5
+++- 
+++- work_dir: ./work_dirt/
+++--batch_size: 4
+++-+batch_size: 1
+++- random_seed: 0 
+++--test_batch_size: 4
+++--num_worker: 20
+++-+test_batch_size: 1
+++-+num_worker: 3
+++- device: 0
+++- log_interval: 10000
+++- eval_interval: 1
++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
++++index ffef81f..f4065cc 100644
++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++index 7ac0c3b..c89f220 100644
++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
++++index 529dabc..b6213f4 100644
++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
+++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++-index 555f4b8..c126e7a 100644
++++index c126e7a..b01e052 100644
+++ --- a/dataset/dataloader_video.py
+++ +++ b/dataset/dataloader_video.py
+++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++-         self.transform_mode = "train" if transform_mode else "test"
+++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++-+
+++-         print(mode, len(self))
+++-         self.data_aug = self.transform()
+++-         print("")
+++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++-             return input_data, label, self.inputs_list[idx]['original_info']
++++@@ -9,6 +9,7 @@ import torch
++++ import random
++++ import pandas
++++ import warnings
+++++import time
+++  
+++-     def read_video(self, index):
+++--        # load file info
+++-         fi = self.inputs_list[index]
+++-+    
+++-         if 'phoenix' in self.dataset:
+++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++-+#            img_list = sorted(glob.glob(frame_pattern))
+++-+#            print(img_list)
+++-+
+++-+#            print("[LOG] Using phoenix")
+++-+
+++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++  
+++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++-+#            print(img_folder)
+++-+#            img_list = sorted(glob.glob(img_folder))
+++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++-+#            print(len(img_list))
+++-         elif self.dataset == 'CSL-Daily':
+++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++-+    
+++-         img_list = sorted(glob.glob(img_folder))
+++-+    
+++-+        if len(img_list) == 0:
+++-+            print(f"[WARNING] No frames found in: {img_list}")
+++-+    
+++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++-+    
+++-         label_list = []
+++--        if self.dataset=='phoenix2014':
+++-+        if self.dataset == 'phoenix2014':
+++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++--        if self.dataset=='phoenix2014-T':
+++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++-+        elif self.dataset == 'phoenix2014-T':
+++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++-+    
+++-         for phase in fi['label'].split(" "):
+++--            if phase == '':
+++--                continue
+++--            if phase in self.dict.keys():
+++-+            if phase and phase in self.dict:
+++-                 label_list.append(self.dict[phase][0])
+++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++-+    
+++-+        video = [
+++-+            cv2.cvtColor(
+++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++-+                cv2.COLOR_BGR2RGB
+++-+            )   
+++-+            for img_path in img_list
+++-+        ]
+++-+    
+++-+        return video, label_list, fi
++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++  
++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++-
+++++            # print('phoenix 데이터를 사용함')
+++++            # print(img_folder)
++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++ #            print(img_folder)
++++ #            img_list = sorted(glob.glob(img_folder))
++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++      def read_features(self, index):
+++          # load file info
++++         fi = self.inputs_list[index]
+++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++        time.sleep(10)
++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++         return data['features'], data['label']
++++ 
++++@@ -220,7 +228,7 @@ if __name__ == "__main__":
++++         dataset=feeder,
++++         batch_size=1,
++++         shuffle=True,
++++-        drop_last=True,
+++++        drop_last=False,
++++         num_workers=0,
++++     )
++++     for data in dataloader:
+++ diff --git a/main.py b/main.py
+++-index 9e68cee..18ac59b 100644
++++index 18ac59b..7f82626 100644
+++ --- a/main.py
+++ +++ b/main.py
+++-@@ -256,7 +256,7 @@ class Processor():
+++-                 batch_size=batch_size,
+++-                 collate_fn=self.feeder.collate_fn,
+++-                 num_workers=self.arg.num_worker,
+++--                pin_memory=True,
+++-+                pin_memory=False,
+++-                 worker_init_fn=self.init_fn,
+++-             )
+++-             return loader
+++-@@ -268,7 +268,7 @@ class Processor():
+++-                 drop_last=train_flag,
+++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++-                 collate_fn=self.feeder.collate_fn,
+++--                pin_memory=True,
+++-+                pin_memory=False,
+++-                 worker_init_fn=self.init_fn,
+++-             )
+++- 
+++-diff --git a/seq_scripts.py b/seq_scripts.py
+++-index 528856d..d8fcaf9 100644
+++---- a/seq_scripts.py
+++-+++ b/seq_scripts.py
+++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++-     return
+++- 
+++- 
+++-+import csv 
+++-+from jiwer import wer as jiwer_wer
+++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++-     model.eval()
+++--    results=defaultdict(dict)
+++-+    results = defaultdict(dict)
+++- 
+++-     for batch_idx, data in enumerate(tqdm(loader)):
+++-         recoder.record_timer("device")
+++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++-                 results[inf]['conv_sents'] = conv_sents
+++-                 results[inf]['recognized_sents'] = recognized_sents
+++-                 results[inf]['gloss'] = gl
++++@@ -21,6 +21,7 @@ import utils
++++ from seq_scripts import seq_train, seq_eval
++++ from torch.cuda.amp import autocast as autocast
++++ from utils.misc import *
+++++from utils.decode import analyze_frame_lengths
++++ class Processor():
++++     def __init__(self, arg):
++++         self.arg = arg
++++@@ -105,13 +106,25 @@ class Processor():
++++                 print('Please appoint --weights.')
++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++
+++++            train_wer = seq_eval(
+++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++            )
+++++            dev_wer = seq_eval(
+++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++            )
+++++            test_wer = seq_eval(
+++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++            )
+++ +
+++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++-     gls_ref = [results[n]['gloss'] for n in results]
+++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++             self.recoder.print_log('Evaluation Done.\n')
+++ +
+++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++--    if wer_results['wer'] < wer_results_con['wer']:
+++--        reg_per = wer_results
+++--    else:
+++--        reg_per = wer_results_con
+++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++            analyze_frame_lengths()
+++ +
+++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++         elif self.arg.phase == "features":
++++             for mode in ["train", "dev", "test"]:
++++                 seq_feature_generation(
++++@@ -119,6 +132,8 @@ class Processor():
++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++                 )
++++ 
+++ +
+++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++-         f"{work_dir}/{mode}.txt")
+++ +
+++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++-+        f"{work_dir}/{mode}.txt")
++++     def save_arg(self):
++++         arg_dict = vars(self.arg)
++++         if not os.path.exists(self.arg.work_dir):
++++@@ -239,6 +254,7 @@ class Processor():
++++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
++++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
++++         print("Loading Dataprocessing finished.")
+++++        time.sleep(10)
++++     def init_fn(self, worker_id):
++++         np.random.seed(int(self.arg.random_seed)+worker_id)
++++ 
++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++index 71519fd..b9664e1 100644
++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++diff --git a/seq_scripts.py b/seq_scripts.py
++++index d8fcaf9..77cfc71 100644
++++--- a/seq_scripts.py
+++++++ b/seq_scripts.py
++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++         })
++++ 
++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++-    
+++++    # 전체 샘플 WER 평균 계산
+++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+++ +
+++-+    # ✅ 전체 결과 CSV로 저장
+++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++-+    os.makedirs(save_folder, exist_ok=True)
+++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++    print("\n📊 전체 평균 WER")
+++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+++ +
+++-+    rows = []
+++-+    for file_id in results:
+++-+        gt = results[file_id]['gloss']
+++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++     for sample in top5:
++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++diff --git a/tmp.ipynb b/tmp.ipynb
++++index e69de29..0342039 100644
++++--- a/tmp.ipynb
+++++++ b/tmp.ipynb
++++@@ -0,0 +1,272 @@
+++++{
+++++ "cells": [
+++++  {
+++++   "cell_type": "code",
+++++   "execution_count": 2,
+++++   "metadata": {},
+++++   "outputs": [],
+++++   "source": [
+++++    "import torch\n",
+++++    "\n",
+++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++   ]
+++++  },
+++++  {
+++++   "cell_type": "code",
+++++   "execution_count": 3,
+++++   "metadata": {},
+++++   "outputs": [
+++++    {
+++++     "name": "stdout",
+++++     "output_type": "stream",
+++++     "text": [
+++++      "Model is a state_dict.\n",
+++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++     ]
+++++    }
+++++   ],
+++++   "source": [
+++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++    "    print(\"Model is a state_dict.\")\n",
+++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++   ]
+++++  },
+++++  {
+++++   "cell_type": "code",
+++++   "execution_count": 7,
+++++   "metadata": {},
+++++   "outputs": [],
+++++   "source": [
+++++    "import torch\n",
+++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++    "import torch.nn.functional as F\n"
+++++   ]
+++++  },
+++++  {
+++++   "cell_type": "code",
+++++   "execution_count": 1,
+++++   "metadata": {},
+++++   "outputs": [
+++++    {
+++++     "name": "stderr",
+++++     "output_type": "stream",
+++++     "text": [
+++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++     ]
+++++    }
+++++   ],
+++++   "source": [
+++++    "import torch\n",
+++++    "import torch.nn as nn\n",
+++++    "import torch.nn.functional as F\n",
+++++    "import torchvision.models as models\n",
+++++    "import numpy as np\n",
+++++    "import modules.resnet as resnet\n",
+++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++    "from modules.criterions import SeqKD\n",
+++++    "import utils\n",
+++++    "import modules.resnet as resnet\n",
+++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++    "class Identity(nn.Module):\n",
+++++    "    def __init__(self):\n",
+++++    "        super(Identity, self).__init__()\n",
+++++    "\n",
+++++    "    def forward(self, x):\n",
+++++    "        return x\n",
+++++    "\n",
+++++    "# L2 정규화 선형 레이어\n",
+++++    "class NormLinear(nn.Module):\n",
+++++    "    def __init__(self, in_dim, out_dim):\n",
+++++    "        super(NormLinear, self).__init__()\n",
+++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++    "\n",
+++++    "    def forward(self, x):\n",
+++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++    "        return outputs\n",
+++++    "\n",
+++++    "# SLRModel (수어 인식 모델)\n",
+++++    "class SLRModel(nn.Module):\n",
+++++    "    def __init__(\n",
+++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++    "            weight_norm=True, share_classifier=True\n",
+++++    "    ):\n",
+++++    "        super(SLRModel, self).__init__()\n",
+++++    "        self.decoder = None\n",
+++++    "        self.loss = dict()\n",
+++++    "        self.criterion_init()\n",
+++++    "        self.num_classes = num_classes\n",
+++++    "        self.loss_weights = loss_weights\n",
+++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++    "\n",
+++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++    "                                   hidden_size=hidden_size,\n",
+++++    "                                   conv_type=conv_type,\n",
+++++    "                                   use_bn=use_bn,\n",
+++++    "                                   num_classes=num_classes)\n",
+++++    "\n",
+++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++    "\n",
+++++    "        # BiLSTM 기반 Temporal Model\n",
+++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++    "                                          num_layers=2, bidirectional=True)\n",
+++++    "\n",
+++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++    "        if weight_norm:\n",
+++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++    "        else:\n",
+++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++    "\n",
+++++    "        # Classifier 공유 여부\n",
+++++    "        if share_classifier:\n",
+++++    "            self.conv1d.fc = self.classifier\n",
+++++    "\n",
+++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++    "        if len(x.shape) == 5:\n",
+++++    "            batch, temp, channel, height, width = x.shape\n",
+++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++    "        else:\n",
+++++    "            framewise = x\n",
+++++    "\n",
+++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++    "        x = conv1d_outputs['visual_feat']\n",
+++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++    "\n",
+++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++    "\n",
+++++    "        # 최종 Classifier 적용\n",
+++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++    "\n",
+++++    "        # Inference 모드에서 Decoding\n",
+++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++    "\n",
+++++    "        return {\n",
+++++    "            \"framewise_features\": framewise,\n",
+++++    "            \"visual_features\": x,\n",
+++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++    "            \"feat_len\": lgt,\n",
+++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++    "            \"sequence_logits\": outputs,\n",
+++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++    "            \"conv_sents\": conv_pred,\n",
+++++    "            \"recognized_sents\": pred,\n",
+++++    "        }\n",
+++++    "\n",
+++++    "    def criterion_init(self):\n",
+++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++    "        return self.loss\n"
+++++   ]
+++++  },
+++++  {
+++++   "cell_type": "code",
+++++   "execution_count": 6,
+++++   "metadata": {},
+++++   "outputs": [
+++++    {
+++++     "ename": "KeyError",
+++++     "evalue": "'dataset_info'",
+++++     "output_type": "error",
+++++     "traceback": [
+++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++     ]
+++++    }
+++++   ],
+++++   "source": [
+++++    "import os\n",
+++++    "import numpy as np\n",
+++++    "import yaml\n",
+++++    "\n",
+++++    "# 환경 변수 설정\n",
+++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++    "\n",
+++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++    "with open(config_path, \"r\") as f:\n",
+++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++    "\n",
+++++    "# ✅ gloss_dict 로드\n",
+++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++    "\n",
+++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++   ]
+++++  },
+++++  {
+++++   "cell_type": "code",
+++++   "execution_count": 5,
+++++   "metadata": {},
+++++   "outputs": [
+++++    {
+++++     "ename": "AttributeError",
+++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++     "output_type": "error",
+++++     "traceback": [
+++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++     ]
+++++    }
+++++   ],
+++++   "source": [
+++++    "import torch\n",
+++++    "\n",
+++++    "# 모델 불러오기\n",
+++++    "model = SLRModel(\n",
+++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++    ")\n",
+++++    "\n",
+++++    "# 저장된 가중치 로드\n",
+++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++    "\n",
+++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++    "if isinstance(state_dict, dict):\n",
+++++    "    model.load_state_dict(state_dict)\n",
+++++    "\n",
+++++    "# 모델을 평가 모드로 설정\n",
+++++    "model.eval()\n"
+++++   ]
+++++  }
+++++ ],
+++++ "metadata": {
+++++  "kernelspec": {
+++++   "display_name": "3.9.13",
+++++   "language": "python",
+++++   "name": "python3"
+++++  },
+++++  "language_info": {
+++++   "codemirror_mode": {
+++++    "name": "ipython",
+++++    "version": 3
+++++   },
+++++   "file_extension": ".py",
+++++   "mimetype": "text/x-python",
+++++   "name": "python",
+++++   "nbconvert_exporter": "python",
+++++   "pygments_lexer": "ipython3",
+++++   "version": "3.9.13"
+++++  }
+++++ },
+++++ "nbformat": 4,
+++++ "nbformat_minor": 2
+++++}
++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++index cb157af..c502b5d 100644
++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++diff --git a/utils/decode.py b/utils/decode.py
++++index 3877729..ac8dab6 100644
++++--- a/utils/decode.py
+++++++ b/utils/decode.py
++++@@ -6,6 +6,38 @@ import ctcdecode
++++ import numpy as np
++++ from itertools import groupby
++++ import torch.nn.functional as F
+++++import matplotlib.pyplot as plt
+++ +
+++-+        rows.append([
+++-+            file_id,
+++-+            gt,
+++-+            conv_pred,
+++-+            f"{conv_wer:.4f}",
+++-+            lstm_pred,
+++-+            f"{lstm_wer:.4f}"
+++-+        ])
+++++# ⬇ 프레임 길이 저장용 전역 리스트
+++++frame_lengths = []
+++ +
+++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++-+        writer = csv.writer(f)
+++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++-+        writer.writerows(rows)
+++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++    probs = torch.softmax(nn_output, dim=-1)
+++++    pred_ids = torch.argmax(probs, dim=-1)
+++ +
+++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++    length = int(vid_lgt[batch_idx].item())
+++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++    x = np.arange(length)
+++ +
+++++    plt.figure(figsize=(15, 4))
+++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++ +
+++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++    if len(blank_indices) > 0:
+++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++ +
+++-+    # WER 기준 상위 5개 샘플 출력
+++-+    sample_wers = []
+++-+    for file_id in results:
+++-+        gt = results[file_id]['gloss']
+++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++-+    
+++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++-+    
+++-+        sample_wers.append({
+++-+            'file_id': file_id,
+++-+            'gt': gt,
+++-+            'conv_pred': conv_pred,
+++-+            'conv_wer': conv_wer,
+++-+            'lstm_pred': lstm_pred,
+++-+            'lstm_wer': lstm_wer,
+++-+            'max_wer': max(conv_wer, lstm_wer)
+++-+        })
+++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++++    if sample_id:
+++++        title_str += f"\nSample: {sample_id}"
+++++    plt.title(title_str)
+++++    plt.xlabel("Time Step")
+++++    plt.ylabel("Gloss ID")
+++++    plt.yticks(np.unique(pred_seq))
+++++    plt.grid(True)
+++++    plt.legend()
+++++    plt.tight_layout()
+++++    plt.show()
++++ 
++++ 
++++ class Decode(object):
++++@@ -16,35 +48,27 @@ class Decode(object):
++++         self.search_mode = search_mode
++++         self.blank_id = blank_id
++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++++-                                                    num_processes=10)
+++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++++ 
++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++++         if not batch_first:
++++             nn_output = nn_output.permute(1, 0, 2)
+++ +
+++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++-+    
+++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++-+    for sample in top5:
+++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++-+        print(f"GT   : {sample['gt']}")
+++-+        print(f"Conv : {sample['conv_pred']}")
+++-+        print(f"LSTM : {sample['lstm_pred']}")
+++-+        print("-" * 60)
+++++        # ⬇ 프레임 길이 수집
+++++        global frame_lengths
+++++        for i in range(vid_lgt.size(0)):
+++++            frame_lengths.append(int(vid_lgt[i].item()))
+++ +
+++++        # sample_id가 존재하면 시각화
+++++        sample_id = sample_ids[0] if sample_ids else None
+++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+++ +
++++         if self.search_mode == "max":
++++             return self.MaxDecode(nn_output, vid_lgt)
++++         else:
++++             return self.BeamSearch(nn_output, vid_lgt, probs)
++++ 
++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++++-        '''
++++-        CTCBeamDecoder Shape:
++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++++-                          timesteps (B, N_beams)
++++-                          out_lens (B, N_beams)
++++-        '''
++++-
++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
++++-        batchsize, lgt = index_list.shape
++++-        blank_rate =[]
++++-        for batch_idx in range(batchsize):
++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++++-            blank_rate.append(group_result)
++++-
++++-
++++         if not probs:
++++             nn_output = nn_output.softmax(-1).cpu()
++++         vid_lgt = vid_lgt.cpu()
++++@@ -54,9 +78,7 @@ class Decode(object):
++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++++             if len(first_result) != 0:
++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++++-                             enumerate(first_result)])
+++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++++         return ret_list
++++ 
++++     def MaxDecode(self, nn_output, vid_lgt):
++++@@ -65,12 +87,34 @@ class Decode(object):
++++         ret_list = []
++++         for batch_idx in range(batchsize):
++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++++            filtered = [x for x in group_result if x != self.blank_id]
++++             if len(filtered) > 0:
++++                 max_result = torch.stack(filtered)
++++                 max_result = [x[0] for x in groupby(max_result)]
++++             else:
++++                 max_result = filtered
++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++++-                             enumerate(max_result)])
+++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
++++         return ret_list
+++ +
+++ +
+++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
+++++def analyze_frame_lengths():
+++++    if not frame_lengths:
+++++        print("⚠ 분석할 frame_lengths가 없습니다.")
+++++        return
+++ +
+++++    print("\n📊 Test Video Frame Length Analysis:")
+++++    print(f"- Total samples: {len(frame_lengths)}")
+++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
+++++    print(f"- Min length  : {np.min(frame_lengths)}")
+++++    print(f"- Max length  : {np.max(frame_lengths)}")
+++ +
+++++    # 히스토그램 시각화
+++++    plt.figure(figsize=(10, 5))
+++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
+++++    plt.title("Test Video Frame Length Distribution")
+++++    plt.xlabel("Frame Length")
+++++    plt.ylabel("Number of Samples")
+++++    plt.grid(True)
+++++    plt.tight_layout()
+++++    plt.show()
++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++index 7d0a2aa..cd66258 100644
++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++index c31483a..239691c 100644
++++--- a/work_dirt/config.yaml
+++++++ b/work_dirt/config.yaml
++++@@ -7,7 +7,7 @@ dataset_info:
++++   evaluation_dir: ./evaluation/slr_eval
++++   evaluation_prefix: phoenix2014-groundtruth
++++ decode_mode: beam
++++-device: your_device
+++++device: cuda
++++ dist_url: env://
++++ eval_interval: 1
++++ evaluate_tool: python
++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
++++index c126e7a..b01e052 100644
++++--- a/work_dirt/dataloader_video.py
+++++++ b/work_dirt/dataloader_video.py
++++@@ -9,6 +9,7 @@ import torch
++++ import random
++++ import pandas
++++ import warnings
+++++import time
++++ 
++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++ 
++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++++ 
++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++-
+++++            # print('phoenix 데이터를 사용함')
+++++            # print(img_folder)
++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++ #            print(img_folder)
++++ #            img_list = sorted(glob.glob(img_folder))
++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++++     def read_features(self, index):
++++         # load file info
++++         fi = self.inputs_list[index]
+++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++        time.sleep(10)
++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++         return data['features'], data['label']
++++ 
++++@@ -220,7 +228,7 @@ if __name__ == "__main__":
++++         dataset=feeder,
++++         batch_size=1,
++++         shuffle=True,
++++-        drop_last=True,
+++++        drop_last=False,
++++         num_workers=0,
++++     )
++++     for data in dataloader:
++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
++++index 00c1a0e..92ee861 100644
++++--- a/work_dirt/dev.txt
+++++++ b/work_dirt/dev.txt
++++@@ -8,3 +8,25 @@
++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++[ Thu Apr  3 13:49:35 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++index 8a22ece..1009537 100644
++++--- a/work_dirt/dirty.patch
+++++++ b/work_dirt/dirty.patch
++++@@ -1,284 +1,16791 @@
++++-diff --git a/README.md b/README.md
++++-index bdbc17f..8cb240b 100644
++++---- a/README.md
++++-+++ b/README.md
++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++- 
++++- 
++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++- 
++++- ### Training
++++- 
++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++-index bfc1da8..25ffa61 100644
++++---- a/configs/baseline.yaml
++++-+++ b/configs/baseline.yaml
++++-@@ -1,14 +1,14 @@
++++- feeder: dataset.dataloader_video.BaseFeeder
++++- phase: train
++++--dataset: phoenix2014-T
++++-+dataset: phoenix2014
++++- #CSL-Daily
++++- # dataset: phoenix14-si5
++++- 
++++- work_dir: ./work_dirt/
++++--batch_size: 4
++++-+batch_size: 1
++++- random_seed: 0 
++++--test_batch_size: 4
++++--num_worker: 20
++++-+test_batch_size: 1
++++-+num_worker: 3
++++- device: 0
++++- log_interval: 10000
++++- eval_interval: 1
+++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
+++++index ffef81f..f4065cc 100644
+++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
+++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++index 7ac0c3b..c89f220 100644
+++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
+++++index 529dabc..b6213f4 100644
+++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++-index 555f4b8..c126e7a 100644
+++++index c126e7a..b01e052 100644
++++ --- a/dataset/dataloader_video.py
++++ +++ b/dataset/dataloader_video.py
++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++-         self.transform_mode = "train" if transform_mode else "test"
++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++-+
++++-         print(mode, len(self))
++++-         self.data_aug = self.transform()
++++-         print("")
++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++-             return input_data, label, self.inputs_list[idx]['original_info']
+++++@@ -9,6 +9,7 @@ import torch
+++++ import random
+++++ import pandas
+++++ import warnings
++++++import time
++++  
++++-     def read_video(self, index):
++++--        # load file info
++++-         fi = self.inputs_list[index]
++++-+    
++++-         if 'phoenix' in self.dataset:
++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++-+#            print(img_list)
++++-+
++++-+#            print("[LOG] Using phoenix")
++++-+
++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++  
++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++-+#            print(img_folder)
++++-+#            img_list = sorted(glob.glob(img_folder))
++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++-+#            print(len(img_list))
++++-         elif self.dataset == 'CSL-Daily':
++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++-+    
++++-         img_list = sorted(glob.glob(img_folder))
++++-+    
++++-+        if len(img_list) == 0:
++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++-+    
++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++-+    
++++-         label_list = []
++++--        if self.dataset=='phoenix2014':
++++-+        if self.dataset == 'phoenix2014':
++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++--        if self.dataset=='phoenix2014-T':
++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++-+        elif self.dataset == 'phoenix2014-T':
++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++-+    
++++-         for phase in fi['label'].split(" "):
++++--            if phase == '':
++++--                continue
++++--            if phase in self.dict.keys():
++++-+            if phase and phase in self.dict:
++++-                 label_list.append(self.dict[phase][0])
++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++-+    
++++-+        video = [
++++-+            cv2.cvtColor(
++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++-+                cv2.COLOR_BGR2RGB
++++-+            )   
++++-+            for img_path in img_list
++++-+        ]
++++-+    
++++-+        return video, label_list, fi
+++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++++  
+++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++-
++++++            # print('phoenix 데이터를 사용함')
++++++            # print(img_folder)
+++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++ #            print(img_folder)
+++++ #            img_list = sorted(glob.glob(img_folder))
+++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++++      def read_features(self, index):
++++          # load file info
+++++         fi = self.inputs_list[index]
++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++        time.sleep(10)
+++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++         return data['features'], data['label']
+++++ 
+++++@@ -220,7 +228,7 @@ if __name__ == "__main__":
+++++         dataset=feeder,
+++++         batch_size=1,
+++++         shuffle=True,
+++++-        drop_last=True,
++++++        drop_last=False,
+++++         num_workers=0,
+++++     )
+++++     for data in dataloader:
++++ diff --git a/main.py b/main.py
++++-index 9e68cee..18ac59b 100644
+++++index 18ac59b..7f82626 100644
++++ --- a/main.py
++++ +++ b/main.py
++++-@@ -256,7 +256,7 @@ class Processor():
++++-                 batch_size=batch_size,
++++-                 collate_fn=self.feeder.collate_fn,
++++-                 num_workers=self.arg.num_worker,
++++--                pin_memory=True,
++++-+                pin_memory=False,
++++-                 worker_init_fn=self.init_fn,
++++-             )
++++-             return loader
++++-@@ -268,7 +268,7 @@ class Processor():
++++-                 drop_last=train_flag,
++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++-                 collate_fn=self.feeder.collate_fn,
++++--                pin_memory=True,
++++-+                pin_memory=False,
++++-                 worker_init_fn=self.init_fn,
++++-             )
++++- 
++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++-index 528856d..d8fcaf9 100644
++++---- a/seq_scripts.py
++++-+++ b/seq_scripts.py
++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++-     return
++++- 
++++- 
++++-+import csv 
++++-+from jiwer import wer as jiwer_wer
++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++-     model.eval()
++++--    results=defaultdict(dict)
++++-+    results = defaultdict(dict)
++++- 
++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++-         recoder.record_timer("device")
++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++-                 results[inf]['conv_sents'] = conv_sents
++++-                 results[inf]['recognized_sents'] = recognized_sents
++++-                 results[inf]['gloss'] = gl
+++++@@ -21,6 +21,7 @@ import utils
+++++ from seq_scripts import seq_train, seq_eval
+++++ from torch.cuda.amp import autocast as autocast
+++++ from utils.misc import *
++++++from utils.decode import analyze_frame_lengths
+++++ class Processor():
+++++     def __init__(self, arg):
+++++         self.arg = arg
+++++@@ -105,13 +106,25 @@ class Processor():
+++++                 print('Please appoint --weights.')
+++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++ +
++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++-     gls_ref = [results[n]['gloss'] for n in results]
++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++            train_wer = seq_eval(
++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++            )
++++++            dev_wer = seq_eval(
++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++            )
++++++            test_wer = seq_eval(
++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++            )
++++ +
++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++--    if wer_results['wer'] < wer_results_con['wer']:
++++--        reg_per = wer_results
++++--    else:
++++--        reg_per = wer_results_con
+++++             self.recoder.print_log('Evaluation Done.\n')
++++ +
++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++            analyze_frame_lengths()
++++ +
++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++-         f"{work_dir}/{mode}.txt")
+++++         elif self.arg.phase == "features":
+++++             for mode in ["train", "dev", "test"]:
+++++                 seq_feature_generation(
+++++@@ -119,6 +132,8 @@ class Processor():
+++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++                 )
+++++ 
++++ +
++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++-+        f"{work_dir}/{mode}.txt")
++++ +
++++-+    # ✅ 전체 결과 CSV로 저장
++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++-+    os.makedirs(save_folder, exist_ok=True)
++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++     def save_arg(self):
+++++         arg_dict = vars(self.arg)
+++++         if not os.path.exists(self.arg.work_dir):
+++++@@ -239,6 +254,7 @@ class Processor():
+++++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
+++++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
+++++         print("Loading Dataprocessing finished.")
++++++        time.sleep(10)
+++++     def init_fn(self, worker_id):
+++++         np.random.seed(int(self.arg.random_seed)+worker_id)
+++++ 
+++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++index 71519fd..b9664e1 100644
+++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++diff --git a/seq_scripts.py b/seq_scripts.py
+++++index d8fcaf9..77cfc71 100644
+++++--- a/seq_scripts.py
++++++++ b/seq_scripts.py
+++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++         })
+++++ 
+++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++-    
++++++    # 전체 샘플 WER 평균 계산
++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
++++ +
++++-+    rows = []
++++-+    for file_id in results:
++++-+        gt = results[file_id]['gloss']
++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++    print("\n📊 전체 평균 WER")
++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
++++ +
++++-+        rows.append([
++++-+            file_id,
++++-+            gt,
++++-+            conv_pred,
++++-+            f"{conv_wer:.4f}",
++++-+            lstm_pred,
++++-+            f"{lstm_wer:.4f}"
++++-+        ])
+++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++     for sample in top5:
+++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++index e69de29..0342039 100644
+++++--- a/tmp.ipynb
++++++++ b/tmp.ipynb
+++++@@ -0,0 +1,272 @@
++++++{
++++++ "cells": [
++++++  {
++++++   "cell_type": "code",
++++++   "execution_count": 2,
++++++   "metadata": {},
++++++   "outputs": [],
++++++   "source": [
++++++    "import torch\n",
++++++    "\n",
++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++   ]
++++++  },
++++++  {
++++++   "cell_type": "code",
++++++   "execution_count": 3,
++++++   "metadata": {},
++++++   "outputs": [
++++++    {
++++++     "name": "stdout",
++++++     "output_type": "stream",
++++++     "text": [
++++++      "Model is a state_dict.\n",
++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++     ]
++++++    }
++++++   ],
++++++   "source": [
++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++    "    print(\"Model is a state_dict.\")\n",
++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++   ]
++++++  },
++++++  {
++++++   "cell_type": "code",
++++++   "execution_count": 7,
++++++   "metadata": {},
++++++   "outputs": [],
++++++   "source": [
++++++    "import torch\n",
++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++    "import torch.nn.functional as F\n"
++++++   ]
++++++  },
++++++  {
++++++   "cell_type": "code",
++++++   "execution_count": 1,
++++++   "metadata": {},
++++++   "outputs": [
++++++    {
++++++     "name": "stderr",
++++++     "output_type": "stream",
++++++     "text": [
++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++     ]
++++++    }
++++++   ],
++++++   "source": [
++++++    "import torch\n",
++++++    "import torch.nn as nn\n",
++++++    "import torch.nn.functional as F\n",
++++++    "import torchvision.models as models\n",
++++++    "import numpy as np\n",
++++++    "import modules.resnet as resnet\n",
++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++    "from modules.criterions import SeqKD\n",
++++++    "import utils\n",
++++++    "import modules.resnet as resnet\n",
++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++    "class Identity(nn.Module):\n",
++++++    "    def __init__(self):\n",
++++++    "        super(Identity, self).__init__()\n",
++++++    "\n",
++++++    "    def forward(self, x):\n",
++++++    "        return x\n",
++++++    "\n",
++++++    "# L2 정규화 선형 레이어\n",
++++++    "class NormLinear(nn.Module):\n",
++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++    "        super(NormLinear, self).__init__()\n",
++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++    "\n",
++++++    "    def forward(self, x):\n",
++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++    "        return outputs\n",
++++++    "\n",
++++++    "# SLRModel (수어 인식 모델)\n",
++++++    "class SLRModel(nn.Module):\n",
++++++    "    def __init__(\n",
++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++    "            weight_norm=True, share_classifier=True\n",
++++++    "    ):\n",
++++++    "        super(SLRModel, self).__init__()\n",
++++++    "        self.decoder = None\n",
++++++    "        self.loss = dict()\n",
++++++    "        self.criterion_init()\n",
++++++    "        self.num_classes = num_classes\n",
++++++    "        self.loss_weights = loss_weights\n",
++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++    "\n",
++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++    "                                   hidden_size=hidden_size,\n",
++++++    "                                   conv_type=conv_type,\n",
++++++    "                                   use_bn=use_bn,\n",
++++++    "                                   num_classes=num_classes)\n",
++++++    "\n",
++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++    "\n",
++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++    "\n",
++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++    "        if weight_norm:\n",
++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++    "        else:\n",
++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++    "\n",
++++++    "        # Classifier 공유 여부\n",
++++++    "        if share_classifier:\n",
++++++    "            self.conv1d.fc = self.classifier\n",
++++++    "\n",
++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++    "        if len(x.shape) == 5:\n",
++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++    "        else:\n",
++++++    "            framewise = x\n",
++++++    "\n",
++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++    "\n",
++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++    "\n",
++++++    "        # 최종 Classifier 적용\n",
++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++    "\n",
++++++    "        # Inference 모드에서 Decoding\n",
++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++    "\n",
++++++    "        return {\n",
++++++    "            \"framewise_features\": framewise,\n",
++++++    "            \"visual_features\": x,\n",
++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++    "            \"feat_len\": lgt,\n",
++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++    "            \"sequence_logits\": outputs,\n",
++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++    "            \"conv_sents\": conv_pred,\n",
++++++    "            \"recognized_sents\": pred,\n",
++++++    "        }\n",
++++++    "\n",
++++++    "    def criterion_init(self):\n",
++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++    "        return self.loss\n"
++++++   ]
++++++  },
++++++  {
++++++   "cell_type": "code",
++++++   "execution_count": 6,
++++++   "metadata": {},
++++++   "outputs": [
++++++    {
++++++     "ename": "KeyError",
++++++     "evalue": "'dataset_info'",
++++++     "output_type": "error",
++++++     "traceback": [
++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++     ]
++++++    }
++++++   ],
++++++   "source": [
++++++    "import os\n",
++++++    "import numpy as np\n",
++++++    "import yaml\n",
++++++    "\n",
++++++    "# 환경 변수 설정\n",
++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++    "\n",
++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++    "with open(config_path, \"r\") as f:\n",
++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++    "\n",
++++++    "# ✅ gloss_dict 로드\n",
++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++    "\n",
++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++   ]
++++++  },
++++++  {
++++++   "cell_type": "code",
++++++   "execution_count": 5,
++++++   "metadata": {},
++++++   "outputs": [
++++++    {
++++++     "ename": "AttributeError",
++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++     "output_type": "error",
++++++     "traceback": [
++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++     ]
++++++    }
++++++   ],
++++++   "source": [
++++++    "import torch\n",
++++++    "\n",
++++++    "# 모델 불러오기\n",
++++++    "model = SLRModel(\n",
++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++    ")\n",
++++++    "\n",
++++++    "# 저장된 가중치 로드\n",
++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++    "\n",
++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++    "if isinstance(state_dict, dict):\n",
++++++    "    model.load_state_dict(state_dict)\n",
++++++    "\n",
++++++    "# 모델을 평가 모드로 설정\n",
++++++    "model.eval()\n"
++++++   ]
++++++  }
++++++ ],
++++++ "metadata": {
++++++  "kernelspec": {
++++++   "display_name": "3.9.13",
++++++   "language": "python",
++++++   "name": "python3"
++++++  },
++++++  "language_info": {
++++++   "codemirror_mode": {
++++++    "name": "ipython",
++++++    "version": 3
++++++   },
++++++   "file_extension": ".py",
++++++   "mimetype": "text/x-python",
++++++   "name": "python",
++++++   "nbconvert_exporter": "python",
++++++   "pygments_lexer": "ipython3",
++++++   "version": "3.9.13"
++++++  }
++++++ },
++++++ "nbformat": 4,
++++++ "nbformat_minor": 2
++++++}
+++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++index cb157af..c502b5d 100644
+++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++diff --git a/utils/decode.py b/utils/decode.py
+++++index 3877729..ac8dab6 100644
+++++--- a/utils/decode.py
++++++++ b/utils/decode.py
+++++@@ -6,6 +6,38 @@ import ctcdecode
+++++ import numpy as np
+++++ from itertools import groupby
+++++ import torch.nn.functional as F
++++++import matplotlib.pyplot as plt
++++ +
++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++-+        writer = csv.writer(f)
++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++-+        writer.writerows(rows)
++++++# ⬇ 프레임 길이 저장용 전역 리스트
++++++frame_lengths = []
++++ +
++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++    probs = torch.softmax(nn_output, dim=-1)
++++++    pred_ids = torch.argmax(probs, dim=-1)
++++ +
++++++    length = int(vid_lgt[batch_idx].item())
++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++    x = np.arange(length)
++++ +
++++++    plt.figure(figsize=(15, 4))
++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++ +
++++-+    # WER 기준 상위 5개 샘플 출력
++++-+    sample_wers = []
++++-+    for file_id in results:
++++-+        gt = results[file_id]['gloss']
++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++-+    
++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++-+    
++++-+        sample_wers.append({
++++-+            'file_id': file_id,
++++-+            'gt': gt,
++++-+            'conv_pred': conv_pred,
++++-+            'conv_wer': conv_wer,
++++-+            'lstm_pred': lstm_pred,
++++-+            'lstm_wer': lstm_wer,
++++-+            'max_wer': max(conv_wer, lstm_wer)
++++-+        })
++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++    if len(blank_indices) > 0:
++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++ +
++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++-+    
++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++-+    for sample in top5:
++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++-+        print(f"GT   : {sample['gt']}")
++++-+        print(f"Conv : {sample['conv_pred']}")
++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++-+        print("-" * 60)
++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++++    if sample_id:
++++++        title_str += f"\nSample: {sample_id}"
++++++    plt.title(title_str)
++++++    plt.xlabel("Time Step")
++++++    plt.ylabel("Gloss ID")
++++++    plt.yticks(np.unique(pred_seq))
++++++    plt.grid(True)
++++++    plt.legend()
++++++    plt.tight_layout()
++++++    plt.show()
+++++ 
+++++ 
+++++ class Decode(object):
+++++@@ -16,35 +48,27 @@ class Decode(object):
+++++         self.search_mode = search_mode
+++++         self.blank_id = blank_id
+++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+++++-                                                    num_processes=10)
++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+++++ 
+++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+++++         if not batch_first:
+++++             nn_output = nn_output.permute(1, 0, 2)
++++ +
++++++        # ⬇ 프레임 길이 수집
++++++        global frame_lengths
++++++        for i in range(vid_lgt.size(0)):
++++++            frame_lengths.append(int(vid_lgt[i].item()))
++++ +
++++++        # sample_id가 존재하면 시각화
++++++        sample_id = sample_ids[0] if sample_ids else None
++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
++++ +
+++++         if self.search_mode == "max":
+++++             return self.MaxDecode(nn_output, vid_lgt)
+++++         else:
+++++             return self.BeamSearch(nn_output, vid_lgt, probs)
+++++ 
+++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+++++-        '''
+++++-        CTCBeamDecoder Shape:
+++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+++++-                          timesteps (B, N_beams)
+++++-                          out_lens (B, N_beams)
+++++-        '''
+++++-
+++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
+++++-        batchsize, lgt = index_list.shape
+++++-        blank_rate =[]
+++++-        for batch_idx in range(batchsize):
+++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+++++-            blank_rate.append(group_result)
+++++-
+++++-
+++++         if not probs:
+++++             nn_output = nn_output.softmax(-1).cpu()
+++++         vid_lgt = vid_lgt.cpu()
+++++@@ -54,9 +78,7 @@ class Decode(object):
+++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+++++             if len(first_result) != 0:
+++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+++++-                             enumerate(first_result)])
++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+++++         return ret_list
+++++ 
+++++     def MaxDecode(self, nn_output, vid_lgt):
+++++@@ -65,12 +87,34 @@ class Decode(object):
+++++         ret_list = []
+++++         for batch_idx in range(batchsize):
+++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++++++            filtered = [x for x in group_result if x != self.blank_id]
+++++             if len(filtered) > 0:
+++++                 max_result = torch.stack(filtered)
+++++                 max_result = [x[0] for x in groupby(max_result)]
+++++             else:
+++++                 max_result = filtered
+++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+++++-                             enumerate(max_result)])
++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++++         return ret_list
++++ +
++++ +
++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
++++++def analyze_frame_lengths():
++++++    if not frame_lengths:
++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
++++++        return
++++ +
++++++    print("\n📊 Test Video Frame Length Analysis:")
++++++    print(f"- Total samples: {len(frame_lengths)}")
++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
++++++    print(f"- Min length  : {np.min(frame_lengths)}")
++++++    print(f"- Max length  : {np.max(frame_lengths)}")
++++ +
++++++    # 히스토그램 시각화
++++++    plt.figure(figsize=(10, 5))
++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
++++++    plt.title("Test Video Frame Length Distribution")
++++++    plt.xlabel("Frame Length")
++++++    plt.ylabel("Number of Samples")
++++++    plt.grid(True)
++++++    plt.tight_layout()
++++++    plt.show()
+++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++index 7d0a2aa..cd66258 100644
+++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++++index c31483a..239691c 100644
+++++--- a/work_dirt/config.yaml
++++++++ b/work_dirt/config.yaml
+++++@@ -7,7 +7,7 @@ dataset_info:
+++++   evaluation_dir: ./evaluation/slr_eval
+++++   evaluation_prefix: phoenix2014-groundtruth
+++++ decode_mode: beam
+++++-device: your_device
++++++device: cuda
+++++ dist_url: env://
+++++ eval_interval: 1
+++++ evaluate_tool: python
+++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
+++++index c126e7a..b01e052 100644
+++++--- a/work_dirt/dataloader_video.py
++++++++ b/work_dirt/dataloader_video.py
+++++@@ -9,6 +9,7 @@ import torch
+++++ import random
+++++ import pandas
+++++ import warnings
++++++import time
+++++ 
+++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++++ 
+++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++++ 
+++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++-
++++++            # print('phoenix 데이터를 사용함')
++++++            # print(img_folder)
+++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++ #            print(img_folder)
+++++ #            img_list = sorted(glob.glob(img_folder))
+++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++++     def read_features(self, index):
+++++         # load file info
+++++         fi = self.inputs_list[index]
++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++        time.sleep(10)
+++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++         return data['features'], data['label']
+++++ 
+++++@@ -220,7 +228,7 @@ if __name__ == "__main__":
+++++         dataset=feeder,
+++++         batch_size=1,
+++++         shuffle=True,
+++++-        drop_last=True,
++++++        drop_last=False,
+++++         num_workers=0,
+++++     )
+++++     for data in dataloader:
+++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
+++++index 00c1a0e..cb85b6f 100644
+++++--- a/work_dirt/dev.txt
++++++++ b/work_dirt/dev.txt
+++++@@ -8,3 +8,23 @@
+++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
+++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++[ Thu Apr  3 13:46:16 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++index 8a22ece..4c62b91 100644
+++++--- a/work_dirt/dirty.patch
++++++++ b/work_dirt/dirty.patch
+++++@@ -1,284 +1,15711 @@
+++++-diff --git a/README.md b/README.md
+++++-index bdbc17f..8cb240b 100644
+++++---- a/README.md
+++++-+++ b/README.md
+++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++- 
+++++- 
+++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++- 
+++++- ### Training
+++++- 
+++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++-index bfc1da8..25ffa61 100644
+++++---- a/configs/baseline.yaml
+++++-+++ b/configs/baseline.yaml
+++++-@@ -1,14 +1,14 @@
+++++- feeder: dataset.dataloader_video.BaseFeeder
+++++- phase: train
+++++--dataset: phoenix2014-T
+++++-+dataset: phoenix2014
+++++- #CSL-Daily
+++++- # dataset: phoenix14-si5
+++++- 
+++++- work_dir: ./work_dirt/
+++++--batch_size: 4
+++++-+batch_size: 1
+++++- random_seed: 0 
+++++--test_batch_size: 4
+++++--num_worker: 20
+++++-+test_batch_size: 1
+++++-+num_worker: 3
+++++- device: 0
+++++- log_interval: 10000
+++++- eval_interval: 1
++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
++++++index ffef81f..f4065cc 100644
++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++index 7ac0c3b..c89f220 100644
++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
++++++index 529dabc..b6213f4 100644
++++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
+++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++-index 555f4b8..c126e7a 100644
++++++index c126e7a..b01e052 100644
+++++ --- a/dataset/dataloader_video.py
+++++ +++ b/dataset/dataloader_video.py
+++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++-         self.transform_mode = "train" if transform_mode else "test"
+++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++-+
+++++-         print(mode, len(self))
+++++-         self.data_aug = self.transform()
+++++-         print("")
+++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++@@ -9,6 +9,7 @@ import torch
++++++ import random
++++++ import pandas
++++++ import warnings
+++++++import time
+++++  
+++++-     def read_video(self, index):
+++++--        # load file info
+++++-         fi = self.inputs_list[index]
+++++-+    
+++++-         if 'phoenix' in self.dataset:
+++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++-+#            print(img_list)
+++++-+
+++++-+#            print("[LOG] Using phoenix")
+++++-+
+++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++++  
+++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++-+#            print(img_folder)
+++++-+#            img_list = sorted(glob.glob(img_folder))
+++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++-+#            print(len(img_list))
+++++-         elif self.dataset == 'CSL-Daily':
+++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++-+    
+++++-         img_list = sorted(glob.glob(img_folder))
+++++-+    
+++++-+        if len(img_list) == 0:
+++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++-+    
+++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++-+    
+++++-         label_list = []
+++++--        if self.dataset=='phoenix2014':
+++++-+        if self.dataset == 'phoenix2014':
+++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++--        if self.dataset=='phoenix2014-T':
+++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++-+        elif self.dataset == 'phoenix2014-T':
+++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++-+    
+++++-         for phase in fi['label'].split(" "):
+++++--            if phase == '':
+++++--                continue
+++++--            if phase in self.dict.keys():
+++++-+            if phase and phase in self.dict:
+++++-                 label_list.append(self.dict[phase][0])
+++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++-+    
+++++-+        video = [
+++++-+            cv2.cvtColor(
+++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++-+                cv2.COLOR_BGR2RGB
+++++-+            )   
+++++-+            for img_path in img_list
+++++-+        ]
+++++-+    
+++++-+        return video, label_list, fi
++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++++  
++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++-
+++++++            # print('phoenix 데이터를 사용함')
+++++++            # print(img_folder)
++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++ #            print(img_folder)
++++++ #            img_list = sorted(glob.glob(img_folder))
++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++++      def read_features(self, index):
+++++          # load file info
++++++         fi = self.inputs_list[index]
+++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++        time.sleep(10)
++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++++         return data['features'], data['label']
++++++ 
++++++@@ -220,7 +228,7 @@ if __name__ == "__main__":
++++++         dataset=feeder,
++++++         batch_size=1,
++++++         shuffle=True,
++++++-        drop_last=True,
+++++++        drop_last=False,
++++++         num_workers=0,
++++++     )
++++++     for data in dataloader:
+++++ diff --git a/main.py b/main.py
+++++-index 9e68cee..18ac59b 100644
++++++index 18ac59b..7f82626 100644
+++++ --- a/main.py
+++++ +++ b/main.py
+++++-@@ -256,7 +256,7 @@ class Processor():
+++++-                 batch_size=batch_size,
+++++-                 collate_fn=self.feeder.collate_fn,
+++++-                 num_workers=self.arg.num_worker,
+++++--                pin_memory=True,
+++++-+                pin_memory=False,
+++++-                 worker_init_fn=self.init_fn,
+++++-             )
+++++-             return loader
+++++-@@ -268,7 +268,7 @@ class Processor():
+++++-                 drop_last=train_flag,
+++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++-                 collate_fn=self.feeder.collate_fn,
+++++--                pin_memory=True,
+++++-+                pin_memory=False,
+++++-                 worker_init_fn=self.init_fn,
+++++-             )
+++++- 
+++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++-index 528856d..d8fcaf9 100644
+++++---- a/seq_scripts.py
+++++-+++ b/seq_scripts.py
+++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++-     return
+++++- 
+++++- 
+++++-+import csv 
+++++-+from jiwer import wer as jiwer_wer
+++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++-     model.eval()
+++++--    results=defaultdict(dict)
+++++-+    results = defaultdict(dict)
+++++- 
+++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++-         recoder.record_timer("device")
+++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++-                 results[inf]['conv_sents'] = conv_sents
+++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++-                 results[inf]['gloss'] = gl
+++++-+
+++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++@@ -21,6 +21,7 @@ import utils
++++++ from seq_scripts import seq_train, seq_eval
++++++ from torch.cuda.amp import autocast as autocast
++++++ from utils.misc import *
+++++++from utils.decode import analyze_frame_lengths
++++++ class Processor():
++++++     def __init__(self, arg):
++++++         self.arg = arg
++++++@@ -105,13 +106,25 @@ class Processor():
++++++                 print('Please appoint --weights.')
++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++ +
+++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++--        reg_per = wer_results
+++++--    else:
+++++--        reg_per = wer_results_con
+++++++            train_wer = seq_eval(
+++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++            )
+++++++            dev_wer = seq_eval(
+++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++            )
+++++++            test_wer = seq_eval(
+++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++            )
+++++ +
+++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++             self.recoder.print_log('Evaluation Done.\n')
+++++ +
+++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++-         f"{work_dir}/{mode}.txt")
+++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++            analyze_frame_lengths()
+++++ +
+++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++-+        f"{work_dir}/{mode}.txt")
++++++         elif self.arg.phase == "features":
++++++             for mode in ["train", "dev", "test"]:
++++++                 seq_feature_generation(
++++++@@ -119,6 +132,8 @@ class Processor():
++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++                 )
++++++ 
+++++ +
+++++-+    # ✅ 전체 결과 CSV로 저장
+++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++-+    os.makedirs(save_folder, exist_ok=True)
+++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++ +
+++++-+    rows = []
+++++-+    for file_id in results:
+++++-+        gt = results[file_id]['gloss']
+++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++     def save_arg(self):
++++++         arg_dict = vars(self.arg)
++++++         if not os.path.exists(self.arg.work_dir):
++++++@@ -239,6 +254,7 @@ class Processor():
++++++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
++++++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
++++++         print("Loading Dataprocessing finished.")
+++++++        time.sleep(10)
++++++     def init_fn(self, worker_id):
++++++         np.random.seed(int(self.arg.random_seed)+worker_id)
++++++ 
++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++index 71519fd..b9664e1 100644
++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++diff --git a/seq_scripts.py b/seq_scripts.py
++++++index d8fcaf9..77cfc71 100644
++++++--- a/seq_scripts.py
+++++++++ b/seq_scripts.py
++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++         })
++++++ 
++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++-    
+++++++    # 전체 샘플 WER 평균 계산
+++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+++++ +
+++++-+        rows.append([
+++++-+            file_id,
+++++-+            gt,
+++++-+            conv_pred,
+++++-+            f"{conv_wer:.4f}",
+++++-+            lstm_pred,
+++++-+            f"{lstm_wer:.4f}"
+++++-+        ])
+++++++    print("\n📊 전체 평균 WER")
+++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+++++ +
+++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++-+        writer = csv.writer(f)
+++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++-+        writer.writerows(rows)
++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++     for sample in top5:
++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++index e69de29..0342039 100644
++++++--- a/tmp.ipynb
+++++++++ b/tmp.ipynb
++++++@@ -0,0 +1,272 @@
+++++++{
+++++++ "cells": [
+++++++  {
+++++++   "cell_type": "code",
+++++++   "execution_count": 2,
+++++++   "metadata": {},
+++++++   "outputs": [],
+++++++   "source": [
+++++++    "import torch\n",
+++++++    "\n",
+++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++   ]
+++++++  },
+++++++  {
+++++++   "cell_type": "code",
+++++++   "execution_count": 3,
+++++++   "metadata": {},
+++++++   "outputs": [
+++++++    {
+++++++     "name": "stdout",
+++++++     "output_type": "stream",
+++++++     "text": [
+++++++      "Model is a state_dict.\n",
+++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++     ]
+++++++    }
+++++++   ],
+++++++   "source": [
+++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++    "    print(\"Model is a state_dict.\")\n",
+++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++   ]
+++++++  },
+++++++  {
+++++++   "cell_type": "code",
+++++++   "execution_count": 7,
+++++++   "metadata": {},
+++++++   "outputs": [],
+++++++   "source": [
+++++++    "import torch\n",
+++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++    "import torch.nn.functional as F\n"
+++++++   ]
+++++++  },
+++++++  {
+++++++   "cell_type": "code",
+++++++   "execution_count": 1,
+++++++   "metadata": {},
+++++++   "outputs": [
+++++++    {
+++++++     "name": "stderr",
+++++++     "output_type": "stream",
+++++++     "text": [
+++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++     ]
+++++++    }
+++++++   ],
+++++++   "source": [
+++++++    "import torch\n",
+++++++    "import torch.nn as nn\n",
+++++++    "import torch.nn.functional as F\n",
+++++++    "import torchvision.models as models\n",
+++++++    "import numpy as np\n",
+++++++    "import modules.resnet as resnet\n",
+++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++    "from modules.criterions import SeqKD\n",
+++++++    "import utils\n",
+++++++    "import modules.resnet as resnet\n",
+++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++    "class Identity(nn.Module):\n",
+++++++    "    def __init__(self):\n",
+++++++    "        super(Identity, self).__init__()\n",
+++++++    "\n",
+++++++    "    def forward(self, x):\n",
+++++++    "        return x\n",
+++++++    "\n",
+++++++    "# L2 정규화 선형 레이어\n",
+++++++    "class NormLinear(nn.Module):\n",
+++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++    "        super(NormLinear, self).__init__()\n",
+++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++    "\n",
+++++++    "    def forward(self, x):\n",
+++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++    "        return outputs\n",
+++++++    "\n",
+++++++    "# SLRModel (수어 인식 모델)\n",
+++++++    "class SLRModel(nn.Module):\n",
+++++++    "    def __init__(\n",
+++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++    "            weight_norm=True, share_classifier=True\n",
+++++++    "    ):\n",
+++++++    "        super(SLRModel, self).__init__()\n",
+++++++    "        self.decoder = None\n",
+++++++    "        self.loss = dict()\n",
+++++++    "        self.criterion_init()\n",
+++++++    "        self.num_classes = num_classes\n",
+++++++    "        self.loss_weights = loss_weights\n",
+++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++    "\n",
+++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++    "                                   hidden_size=hidden_size,\n",
+++++++    "                                   conv_type=conv_type,\n",
+++++++    "                                   use_bn=use_bn,\n",
+++++++    "                                   num_classes=num_classes)\n",
+++++++    "\n",
+++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++    "\n",
+++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++    "\n",
+++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++    "        if weight_norm:\n",
+++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++    "        else:\n",
+++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++    "\n",
+++++++    "        # Classifier 공유 여부\n",
+++++++    "        if share_classifier:\n",
+++++++    "            self.conv1d.fc = self.classifier\n",
+++++++    "\n",
+++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++    "        if len(x.shape) == 5:\n",
+++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++    "        else:\n",
+++++++    "            framewise = x\n",
+++++++    "\n",
+++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++    "\n",
+++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++    "\n",
+++++++    "        # 최종 Classifier 적용\n",
+++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++    "\n",
+++++++    "        # Inference 모드에서 Decoding\n",
+++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++    "\n",
+++++++    "        return {\n",
+++++++    "            \"framewise_features\": framewise,\n",
+++++++    "            \"visual_features\": x,\n",
+++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++    "            \"feat_len\": lgt,\n",
+++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++    "            \"sequence_logits\": outputs,\n",
+++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++    "            \"conv_sents\": conv_pred,\n",
+++++++    "            \"recognized_sents\": pred,\n",
+++++++    "        }\n",
+++++++    "\n",
+++++++    "    def criterion_init(self):\n",
+++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++    "        return self.loss\n"
+++++++   ]
+++++++  },
+++++++  {
+++++++   "cell_type": "code",
+++++++   "execution_count": 6,
+++++++   "metadata": {},
+++++++   "outputs": [
+++++++    {
+++++++     "ename": "KeyError",
+++++++     "evalue": "'dataset_info'",
+++++++     "output_type": "error",
+++++++     "traceback": [
+++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++     ]
+++++++    }
+++++++   ],
+++++++   "source": [
+++++++    "import os\n",
+++++++    "import numpy as np\n",
+++++++    "import yaml\n",
+++++++    "\n",
+++++++    "# 환경 변수 설정\n",
+++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++    "\n",
+++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++    "with open(config_path, \"r\") as f:\n",
+++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++    "\n",
+++++++    "# ✅ gloss_dict 로드\n",
+++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++    "\n",
+++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++   ]
+++++++  },
+++++++  {
+++++++   "cell_type": "code",
+++++++   "execution_count": 5,
+++++++   "metadata": {},
+++++++   "outputs": [
+++++++    {
+++++++     "ename": "AttributeError",
+++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++     "output_type": "error",
+++++++     "traceback": [
+++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++     ]
+++++++    }
+++++++   ],
+++++++   "source": [
+++++++    "import torch\n",
+++++++    "\n",
+++++++    "# 모델 불러오기\n",
+++++++    "model = SLRModel(\n",
+++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++    ")\n",
+++++++    "\n",
+++++++    "# 저장된 가중치 로드\n",
+++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++    "\n",
+++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++    "if isinstance(state_dict, dict):\n",
+++++++    "    model.load_state_dict(state_dict)\n",
+++++++    "\n",
+++++++    "# 모델을 평가 모드로 설정\n",
+++++++    "model.eval()\n"
+++++++   ]
+++++++  }
+++++++ ],
+++++++ "metadata": {
+++++++  "kernelspec": {
+++++++   "display_name": "3.9.13",
+++++++   "language": "python",
+++++++   "name": "python3"
+++++++  },
+++++++  "language_info": {
+++++++   "codemirror_mode": {
+++++++    "name": "ipython",
+++++++    "version": 3
+++++++   },
+++++++   "file_extension": ".py",
+++++++   "mimetype": "text/x-python",
+++++++   "name": "python",
+++++++   "nbconvert_exporter": "python",
+++++++   "pygments_lexer": "ipython3",
+++++++   "version": "3.9.13"
+++++++  }
+++++++ },
+++++++ "nbformat": 4,
+++++++ "nbformat_minor": 2
+++++++}
++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++index cb157af..c502b5d 100644
++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++diff --git a/utils/decode.py b/utils/decode.py
++++++index 3877729..ac8dab6 100644
++++++--- a/utils/decode.py
+++++++++ b/utils/decode.py
++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++ import numpy as np
++++++ from itertools import groupby
++++++ import torch.nn.functional as F
+++++++import matplotlib.pyplot as plt
+++++ +
+++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++# ⬇ 프레임 길이 저장용 전역 리스트
+++++++frame_lengths = []
+++++ +
+++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++    pred_ids = torch.argmax(probs, dim=-1)
+++++ +
+++++++    length = int(vid_lgt[batch_idx].item())
+++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++    x = np.arange(length)
+++++ +
+++++-+    # WER 기준 상위 5개 샘플 출력
+++++-+    sample_wers = []
+++++-+    for file_id in results:
+++++-+        gt = results[file_id]['gloss']
+++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++-+    
+++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++-+    
+++++-+        sample_wers.append({
+++++-+            'file_id': file_id,
+++++-+            'gt': gt,
+++++-+            'conv_pred': conv_pred,
+++++-+            'conv_wer': conv_wer,
+++++-+            'lstm_pred': lstm_pred,
+++++-+            'lstm_wer': lstm_wer,
+++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++-+        })
+++++++    plt.figure(figsize=(15, 4))
+++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++ +
+++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++-+    
+++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++-+    for sample in top5:
+++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++-+        print(f"GT   : {sample['gt']}")
+++++-+        print(f"Conv : {sample['conv_pred']}")
+++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++-+        print("-" * 60)
+++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++    if len(blank_indices) > 0:
+++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++ +
+++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++++++    if sample_id:
+++++++        title_str += f"\nSample: {sample_id}"
+++++++    plt.title(title_str)
+++++++    plt.xlabel("Time Step")
+++++++    plt.ylabel("Gloss ID")
+++++++    plt.yticks(np.unique(pred_seq))
+++++++    plt.grid(True)
+++++++    plt.legend()
+++++++    plt.tight_layout()
+++++++    plt.show()
++++++ 
++++++ 
++++++ class Decode(object):
++++++@@ -16,35 +48,27 @@ class Decode(object):
++++++         self.search_mode = search_mode
++++++         self.blank_id = blank_id
++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++++++-                                                    num_processes=10)
+++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++++++ 
++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++++++         if not batch_first:
++++++             nn_output = nn_output.permute(1, 0, 2)
+++++ +
+++++++        # ⬇ 프레임 길이 수집
+++++++        global frame_lengths
+++++++        for i in range(vid_lgt.size(0)):
+++++++            frame_lengths.append(int(vid_lgt[i].item()))
+++++ +
+++++++        # sample_id가 존재하면 시각화
+++++++        sample_id = sample_ids[0] if sample_ids else None
+++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+++++ +
++++++         if self.search_mode == "max":
++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++         else:
++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
++++++ 
++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++++++-        '''
++++++-        CTCBeamDecoder Shape:
++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++++++-                          timesteps (B, N_beams)
++++++-                          out_lens (B, N_beams)
++++++-        '''
++++++-
++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
++++++-        batchsize, lgt = index_list.shape
++++++-        blank_rate =[]
++++++-        for batch_idx in range(batchsize):
++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++++++-            blank_rate.append(group_result)
++++++-
++++++-
++++++         if not probs:
++++++             nn_output = nn_output.softmax(-1).cpu()
++++++         vid_lgt = vid_lgt.cpu()
++++++@@ -54,9 +78,7 @@ class Decode(object):
++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++++++             if len(first_result) != 0:
++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++++++-                             enumerate(first_result)])
+++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++++++         return ret_list
++++++ 
++++++     def MaxDecode(self, nn_output, vid_lgt):
++++++@@ -65,12 +87,34 @@ class Decode(object):
++++++         ret_list = []
++++++         for batch_idx in range(batchsize):
++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++++++            filtered = [x for x in group_result if x != self.blank_id]
++++++             if len(filtered) > 0:
++++++                 max_result = torch.stack(filtered)
++++++                 max_result = [x[0] for x in groupby(max_result)]
++++++             else:
++++++                 max_result = filtered
++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++++++-                             enumerate(max_result)])
+++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
++++++         return ret_list
+++++ +
+++++ +
+++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
+++++++def analyze_frame_lengths():
+++++++    if not frame_lengths:
+++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
+++++++        return
+++++ +
+++++++    print("\n📊 Test Video Frame Length Analysis:")
+++++++    print(f"- Total samples: {len(frame_lengths)}")
+++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
+++++++    print(f"- Min length  : {np.min(frame_lengths)}")
+++++++    print(f"- Max length  : {np.max(frame_lengths)}")
+++++ +
+++++++    # 히스토그램 시각화
+++++++    plt.figure(figsize=(10, 5))
+++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
+++++++    plt.title("Test Video Frame Length Distribution")
+++++++    plt.xlabel("Frame Length")
+++++++    plt.ylabel("Number of Samples")
+++++++    plt.grid(True)
+++++++    plt.tight_layout()
+++++++    plt.show()
++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++index 7d0a2aa..cd66258 100644
++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++++index c31483a..239691c 100644
++++++--- a/work_dirt/config.yaml
+++++++++ b/work_dirt/config.yaml
++++++@@ -7,7 +7,7 @@ dataset_info:
++++++   evaluation_dir: ./evaluation/slr_eval
++++++   evaluation_prefix: phoenix2014-groundtruth
++++++ decode_mode: beam
++++++-device: your_device
+++++++device: cuda
++++++ dist_url: env://
++++++ eval_interval: 1
++++++ evaluate_tool: python
++++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
++++++index c126e7a..b01e052 100644
++++++--- a/work_dirt/dataloader_video.py
+++++++++ b/work_dirt/dataloader_video.py
++++++@@ -9,6 +9,7 @@ import torch
++++++ import random
++++++ import pandas
++++++ import warnings
+++++++import time
++++++ 
++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++++ 
++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++++++ 
++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++-
+++++++            # print('phoenix 데이터를 사용함')
+++++++            # print(img_folder)
++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++ #            print(img_folder)
++++++ #            img_list = sorted(glob.glob(img_folder))
++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++++++     def read_features(self, index):
++++++         # load file info
++++++         fi = self.inputs_list[index]
+++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++        time.sleep(10)
++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++++         return data['features'], data['label']
++++++ 
++++++@@ -220,7 +228,7 @@ if __name__ == "__main__":
++++++         dataset=feeder,
++++++         batch_size=1,
++++++         shuffle=True,
++++++-        drop_last=True,
+++++++        drop_last=False,
++++++         num_workers=0,
++++++     )
++++++     for data in dataloader:
++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
++++++index 00c1a0e..6119677 100644
++++++--- a/work_dirt/dev.txt
+++++++++ b/work_dirt/dev.txt
++++++@@ -8,3 +8,21 @@
++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++[ Thu Apr  3 13:20:34 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++index 8a22ece..40356bb 100644
++++++--- a/work_dirt/dirty.patch
+++++++++ b/work_dirt/dirty.patch
++++++@@ -1,284 +1,14640 @@
++++++-diff --git a/README.md b/README.md
++++++-index bdbc17f..8cb240b 100644
++++++---- a/README.md
++++++-+++ b/README.md
++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++- 
++++++- 
++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++- 
++++++- ### Training
++++++- 
++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++-index bfc1da8..25ffa61 100644
++++++---- a/configs/baseline.yaml
++++++-+++ b/configs/baseline.yaml
++++++-@@ -1,14 +1,14 @@
++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++- phase: train
++++++--dataset: phoenix2014-T
++++++-+dataset: phoenix2014
++++++- #CSL-Daily
++++++- # dataset: phoenix14-si5
++++++- 
++++++- work_dir: ./work_dirt/
++++++--batch_size: 4
++++++-+batch_size: 1
++++++- random_seed: 0 
++++++--test_batch_size: 4
++++++--num_worker: 20
++++++-+test_batch_size: 1
++++++-+num_worker: 3
++++++- device: 0
++++++- log_interval: 10000
++++++- eval_interval: 1
+++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
+++++++index ffef81f..f4065cc 100644
+++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
+++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++index 7ac0c3b..c89f220 100644
+++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
+++++++index 529dabc..1c62326 100644
+++++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
++++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++-index 555f4b8..c126e7a 100644
+++++++index c126e7a..e5adb9a 100644
++++++ --- a/dataset/dataloader_video.py
++++++ +++ b/dataset/dataloader_video.py
++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++-+
++++++-         print(mode, len(self))
++++++-         self.data_aug = self.transform()
++++++-         print("")
++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++-             return input_data, label, self.inputs_list[idx]['original_info']
+++++++@@ -9,6 +9,7 @@ import torch
+++++++ import random
+++++++ import pandas
+++++++ import warnings
++++++++import time
++++++  
++++++-     def read_video(self, index):
++++++--        # load file info
++++++-         fi = self.inputs_list[index]
++++++-+    
++++++-         if 'phoenix' in self.dataset:
++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++-+#            print(img_list)
++++++-+
++++++-+#            print("[LOG] Using phoenix")
++++++-+
++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++++  
++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++-+#            print(img_folder)
++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++-+#            print(len(img_list))
++++++-         elif self.dataset == 'CSL-Daily':
++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++-+    
++++++-         img_list = sorted(glob.glob(img_folder))
++++++-+    
++++++-+        if len(img_list) == 0:
++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++-+    
++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++++-+    
++++++-         label_list = []
++++++--        if self.dataset=='phoenix2014':
++++++-+        if self.dataset == 'phoenix2014':
++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++--        if self.dataset=='phoenix2014-T':
++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++-+        elif self.dataset == 'phoenix2014-T':
++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++-+    
++++++-         for phase in fi['label'].split(" "):
++++++--            if phase == '':
++++++--                continue
++++++--            if phase in self.dict.keys():
++++++-+            if phase and phase in self.dict:
++++++-                 label_list.append(self.dict[phase][0])
++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++-+    
++++++-+        video = [
++++++-+            cv2.cvtColor(
++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++-+                cv2.COLOR_BGR2RGB
++++++-+            )   
++++++-+            for img_path in img_list
++++++-+        ]
++++++-+    
++++++-+        return video, label_list, fi
+++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++++++  
+++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++-
++++++++            # print('phoenix 데이터를 사용함')
++++++++            # print(img_folder)
+++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++ #            print(img_folder)
+++++++ #            img_list = sorted(glob.glob(img_folder))
+++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++++++      def read_features(self, index):
++++++          # load file info
+++++++         fi = self.inputs_list[index]
++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++        time.sleep(10)
+++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++++         return data['features'], data['label']
+++++++ 
++++++ diff --git a/main.py b/main.py
++++++-index 9e68cee..18ac59b 100644
+++++++index 18ac59b..7f82626 100644
++++++ --- a/main.py
++++++ +++ b/main.py
++++++-@@ -256,7 +256,7 @@ class Processor():
++++++-                 batch_size=batch_size,
++++++-                 collate_fn=self.feeder.collate_fn,
++++++-                 num_workers=self.arg.num_worker,
++++++--                pin_memory=True,
++++++-+                pin_memory=False,
++++++-                 worker_init_fn=self.init_fn,
++++++-             )
++++++-             return loader
++++++-@@ -268,7 +268,7 @@ class Processor():
++++++-                 drop_last=train_flag,
++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++-                 collate_fn=self.feeder.collate_fn,
++++++--                pin_memory=True,
++++++-+                pin_memory=False,
++++++-                 worker_init_fn=self.init_fn,
++++++-             )
++++++- 
++++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++++-index 528856d..d8fcaf9 100644
++++++---- a/seq_scripts.py
++++++-+++ b/seq_scripts.py
++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++-     return
++++++- 
++++++- 
++++++-+import csv 
++++++-+from jiwer import wer as jiwer_wer
++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++-     model.eval()
++++++--    results=defaultdict(dict)
++++++-+    results = defaultdict(dict)
++++++- 
++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++-         recoder.record_timer("device")
++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++-                 results[inf]['conv_sents'] = conv_sents
++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++-                 results[inf]['gloss'] = gl
++++++-+
++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++-+
++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++--        reg_per = wer_results
++++++--    else:
++++++--        reg_per = wer_results_con
+++++++@@ -21,6 +21,7 @@ import utils
+++++++ from seq_scripts import seq_train, seq_eval
+++++++ from torch.cuda.amp import autocast as autocast
+++++++ from utils.misc import *
++++++++from utils.decode import analyze_frame_lengths
+++++++ class Processor():
+++++++     def __init__(self, arg):
+++++++         self.arg = arg
+++++++@@ -105,13 +106,25 @@ class Processor():
+++++++                 print('Please appoint --weights.')
+++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++ +
++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++            train_wer = seq_eval(
++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++            )
++++++++            dev_wer = seq_eval(
++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++            )
++++++++            test_wer = seq_eval(
++++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++            )
++++++ +
++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++-         f"{work_dir}/{mode}.txt")
+++++++             self.recoder.print_log('Evaluation Done.\n')
++++++ +
++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++-+        f"{work_dir}/{mode}.txt")
++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++++            analyze_frame_lengths()
++++++ +
++++++-+    # ✅ 전체 결과 CSV로 저장
++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++         elif self.arg.phase == "features":
+++++++             for mode in ["train", "dev", "test"]:
+++++++                 seq_feature_generation(
+++++++@@ -119,6 +132,8 @@ class Processor():
+++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++++                 )
+++++++ 
++++++ +
++++++-+    rows = []
++++++-+    for file_id in results:
++++++-+        gt = results[file_id]['gloss']
++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++ +
++++++-+        rows.append([
++++++-+            file_id,
++++++-+            gt,
++++++-+            conv_pred,
++++++-+            f"{conv_wer:.4f}",
++++++-+            lstm_pred,
++++++-+            f"{lstm_wer:.4f}"
++++++-+        ])
+++++++     def save_arg(self):
+++++++         arg_dict = vars(self.arg)
+++++++         if not os.path.exists(self.arg.work_dir):
+++++++@@ -239,6 +254,7 @@ class Processor():
+++++++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
+++++++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
+++++++         print("Loading Dataprocessing finished.")
++++++++        time.sleep(10)
+++++++     def init_fn(self, worker_id):
+++++++         np.random.seed(int(self.arg.random_seed)+worker_id)
+++++++ 
+++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++index 71519fd..b9664e1 100644
+++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++diff --git a/seq_scripts.py b/seq_scripts.py
+++++++index d8fcaf9..77cfc71 100644
+++++++--- a/seq_scripts.py
++++++++++ b/seq_scripts.py
+++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++         })
+++++++ 
+++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++-    
++++++++    # 전체 샘플 WER 평균 계산
++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
++++++ +
++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++-+        writer = csv.writer(f)
++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++-+        writer.writerows(rows)
++++++++    print("\n📊 전체 평균 WER")
++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
++++++ +
++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++     for sample in top5:
+++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++index e69de29..0342039 100644
+++++++--- a/tmp.ipynb
++++++++++ b/tmp.ipynb
+++++++@@ -0,0 +1,272 @@
++++++++{
++++++++ "cells": [
++++++++  {
++++++++   "cell_type": "code",
++++++++   "execution_count": 2,
++++++++   "metadata": {},
++++++++   "outputs": [],
++++++++   "source": [
++++++++    "import torch\n",
++++++++    "\n",
++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++   ]
++++++++  },
++++++++  {
++++++++   "cell_type": "code",
++++++++   "execution_count": 3,
++++++++   "metadata": {},
++++++++   "outputs": [
++++++++    {
++++++++     "name": "stdout",
++++++++     "output_type": "stream",
++++++++     "text": [
++++++++      "Model is a state_dict.\n",
++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++     ]
++++++++    }
++++++++   ],
++++++++   "source": [
++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++   ]
++++++++  },
++++++++  {
++++++++   "cell_type": "code",
++++++++   "execution_count": 7,
++++++++   "metadata": {},
++++++++   "outputs": [],
++++++++   "source": [
++++++++    "import torch\n",
++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++    "import torch.nn.functional as F\n"
++++++++   ]
++++++++  },
++++++++  {
++++++++   "cell_type": "code",
++++++++   "execution_count": 1,
++++++++   "metadata": {},
++++++++   "outputs": [
++++++++    {
++++++++     "name": "stderr",
++++++++     "output_type": "stream",
++++++++     "text": [
++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++     ]
++++++++    }
++++++++   ],
++++++++   "source": [
++++++++    "import torch\n",
++++++++    "import torch.nn as nn\n",
++++++++    "import torch.nn.functional as F\n",
++++++++    "import torchvision.models as models\n",
++++++++    "import numpy as np\n",
++++++++    "import modules.resnet as resnet\n",
++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++    "from modules.criterions import SeqKD\n",
++++++++    "import utils\n",
++++++++    "import modules.resnet as resnet\n",
++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++    "class Identity(nn.Module):\n",
++++++++    "    def __init__(self):\n",
++++++++    "        super(Identity, self).__init__()\n",
++++++++    "\n",
++++++++    "    def forward(self, x):\n",
++++++++    "        return x\n",
++++++++    "\n",
++++++++    "# L2 정규화 선형 레이어\n",
++++++++    "class NormLinear(nn.Module):\n",
++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++    "        super(NormLinear, self).__init__()\n",
++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++    "\n",
++++++++    "    def forward(self, x):\n",
++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++    "        return outputs\n",
++++++++    "\n",
++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++    "class SLRModel(nn.Module):\n",
++++++++    "    def __init__(\n",
++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++    "    ):\n",
++++++++    "        super(SLRModel, self).__init__()\n",
++++++++    "        self.decoder = None\n",
++++++++    "        self.loss = dict()\n",
++++++++    "        self.criterion_init()\n",
++++++++    "        self.num_classes = num_classes\n",
++++++++    "        self.loss_weights = loss_weights\n",
++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++    "\n",
++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++    "                                   hidden_size=hidden_size,\n",
++++++++    "                                   conv_type=conv_type,\n",
++++++++    "                                   use_bn=use_bn,\n",
++++++++    "                                   num_classes=num_classes)\n",
++++++++    "\n",
++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++    "\n",
++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++    "\n",
++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++    "        if weight_norm:\n",
++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++    "        else:\n",
++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++    "\n",
++++++++    "        # Classifier 공유 여부\n",
++++++++    "        if share_classifier:\n",
++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++    "\n",
++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++    "        if len(x.shape) == 5:\n",
++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++    "        else:\n",
++++++++    "            framewise = x\n",
++++++++    "\n",
++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++    "\n",
++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++    "\n",
++++++++    "        # 최종 Classifier 적용\n",
++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++    "\n",
++++++++    "        # Inference 모드에서 Decoding\n",
++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++    "\n",
++++++++    "        return {\n",
++++++++    "            \"framewise_features\": framewise,\n",
++++++++    "            \"visual_features\": x,\n",
++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++    "            \"feat_len\": lgt,\n",
++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++    "            \"sequence_logits\": outputs,\n",
++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++    "            \"recognized_sents\": pred,\n",
++++++++    "        }\n",
++++++++    "\n",
++++++++    "    def criterion_init(self):\n",
++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++    "        return self.loss\n"
++++++++   ]
++++++++  },
++++++++  {
++++++++   "cell_type": "code",
++++++++   "execution_count": 6,
++++++++   "metadata": {},
++++++++   "outputs": [
++++++++    {
++++++++     "ename": "KeyError",
++++++++     "evalue": "'dataset_info'",
++++++++     "output_type": "error",
++++++++     "traceback": [
++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++     ]
++++++++    }
++++++++   ],
++++++++   "source": [
++++++++    "import os\n",
++++++++    "import numpy as np\n",
++++++++    "import yaml\n",
++++++++    "\n",
++++++++    "# 환경 변수 설정\n",
++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++    "\n",
++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++    "with open(config_path, \"r\") as f:\n",
++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++    "\n",
++++++++    "# ✅ gloss_dict 로드\n",
++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++    "\n",
++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++   ]
++++++++  },
++++++++  {
++++++++   "cell_type": "code",
++++++++   "execution_count": 5,
++++++++   "metadata": {},
++++++++   "outputs": [
++++++++    {
++++++++     "ename": "AttributeError",
++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++     "output_type": "error",
++++++++     "traceback": [
++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++     ]
++++++++    }
++++++++   ],
++++++++   "source": [
++++++++    "import torch\n",
++++++++    "\n",
++++++++    "# 모델 불러오기\n",
++++++++    "model = SLRModel(\n",
++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++    ")\n",
++++++++    "\n",
++++++++    "# 저장된 가중치 로드\n",
++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++    "\n",
++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++    "if isinstance(state_dict, dict):\n",
++++++++    "    model.load_state_dict(state_dict)\n",
++++++++    "\n",
++++++++    "# 모델을 평가 모드로 설정\n",
++++++++    "model.eval()\n"
++++++++   ]
++++++++  }
++++++++ ],
++++++++ "metadata": {
++++++++  "kernelspec": {
++++++++   "display_name": "3.9.13",
++++++++   "language": "python",
++++++++   "name": "python3"
++++++++  },
++++++++  "language_info": {
++++++++   "codemirror_mode": {
++++++++    "name": "ipython",
++++++++    "version": 3
++++++++   },
++++++++   "file_extension": ".py",
++++++++   "mimetype": "text/x-python",
++++++++   "name": "python",
++++++++   "nbconvert_exporter": "python",
++++++++   "pygments_lexer": "ipython3",
++++++++   "version": "3.9.13"
++++++++  }
++++++++ },
++++++++ "nbformat": 4,
++++++++ "nbformat_minor": 2
++++++++}
+++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++index cb157af..c502b5d 100644
+++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++index 3877729..ac8dab6 100644
+++++++--- a/utils/decode.py
++++++++++ b/utils/decode.py
+++++++@@ -6,6 +6,38 @@ import ctcdecode
+++++++ import numpy as np
+++++++ from itertools import groupby
+++++++ import torch.nn.functional as F
++++++++import matplotlib.pyplot as plt
++++++ +
++++++++# ⬇ 프레임 길이 저장용 전역 리스트
++++++++frame_lengths = []
++++++ +
++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++    pred_ids = torch.argmax(probs, dim=-1)
++++++ +
++++++-+    # WER 기준 상위 5개 샘플 출력
++++++-+    sample_wers = []
++++++-+    for file_id in results:
++++++-+        gt = results[file_id]['gloss']
++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++-+    
++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++-+    
++++++-+        sample_wers.append({
++++++-+            'file_id': file_id,
++++++-+            'gt': gt,
++++++-+            'conv_pred': conv_pred,
++++++-+            'conv_wer': conv_wer,
++++++-+            'lstm_pred': lstm_pred,
++++++-+            'lstm_wer': lstm_wer,
++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++-+        })
++++++++    length = int(vid_lgt[batch_idx].item())
++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++    x = np.arange(length)
++++++ +
++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++-+    
++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++-+    for sample in top5:
++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++-+        print(f"GT   : {sample['gt']}")
++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++-+        print("-" * 60)
++++++++    plt.figure(figsize=(15, 4))
++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++++ +
++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++++    if len(blank_indices) > 0:
++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++++ +
++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++++++    if sample_id:
++++++++        title_str += f"\nSample: {sample_id}"
++++++++    plt.title(title_str)
++++++++    plt.xlabel("Time Step")
++++++++    plt.ylabel("Gloss ID")
++++++++    plt.yticks(np.unique(pred_seq))
++++++++    plt.grid(True)
++++++++    plt.legend()
++++++++    plt.tight_layout()
++++++++    plt.show()
+++++++ 
+++++++ 
+++++++ class Decode(object):
+++++++@@ -16,35 +48,27 @@ class Decode(object):
+++++++         self.search_mode = search_mode
+++++++         self.blank_id = blank_id
+++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+++++++-                                                    num_processes=10)
++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+++++++ 
+++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+++++++         if not batch_first:
+++++++             nn_output = nn_output.permute(1, 0, 2)
++++++ +
++++++++        # ⬇ 프레임 길이 수집
++++++++        global frame_lengths
++++++++        for i in range(vid_lgt.size(0)):
++++++++            frame_lengths.append(int(vid_lgt[i].item()))
++++++ +
++++++++        # sample_id가 존재하면 시각화
++++++++        sample_id = sample_ids[0] if sample_ids else None
++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
++++++ +
+++++++         if self.search_mode == "max":
+++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++         else:
+++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
+++++++ 
+++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+++++++-        '''
+++++++-        CTCBeamDecoder Shape:
+++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+++++++-                          timesteps (B, N_beams)
+++++++-                          out_lens (B, N_beams)
+++++++-        '''
+++++++-
+++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
+++++++-        batchsize, lgt = index_list.shape
+++++++-        blank_rate =[]
+++++++-        for batch_idx in range(batchsize):
+++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+++++++-            blank_rate.append(group_result)
+++++++-
+++++++-
+++++++         if not probs:
+++++++             nn_output = nn_output.softmax(-1).cpu()
+++++++         vid_lgt = vid_lgt.cpu()
+++++++@@ -54,9 +78,7 @@ class Decode(object):
+++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+++++++             if len(first_result) != 0:
+++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+++++++-                             enumerate(first_result)])
++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+++++++         return ret_list
+++++++ 
+++++++     def MaxDecode(self, nn_output, vid_lgt):
+++++++@@ -65,12 +87,34 @@ class Decode(object):
+++++++         ret_list = []
+++++++         for batch_idx in range(batchsize):
+++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++++++++            filtered = [x for x in group_result if x != self.blank_id]
+++++++             if len(filtered) > 0:
+++++++                 max_result = torch.stack(filtered)
+++++++                 max_result = [x[0] for x in groupby(max_result)]
+++++++             else:
+++++++                 max_result = filtered
+++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+++++++-                             enumerate(max_result)])
++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++++++         return ret_list
++++++ +
++++++ +
++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
++++++++def analyze_frame_lengths():
++++++++    if not frame_lengths:
++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
++++++++        return
++++++ +
++++++++    print("\n📊 Test Video Frame Length Analysis:")
++++++++    print(f"- Total samples: {len(frame_lengths)}")
++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
++++++ +
++++++++    # 히스토그램 시각화
++++++++    plt.figure(figsize=(10, 5))
++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
++++++++    plt.title("Test Video Frame Length Distribution")
++++++++    plt.xlabel("Frame Length")
++++++++    plt.ylabel("Number of Samples")
++++++++    plt.grid(True)
++++++++    plt.tight_layout()
++++++++    plt.show()
+++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++index 7d0a2aa..cd66258 100644
+++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++++++index c31483a..239691c 100644
+++++++--- a/work_dirt/config.yaml
++++++++++ b/work_dirt/config.yaml
+++++++@@ -7,7 +7,7 @@ dataset_info:
+++++++   evaluation_dir: ./evaluation/slr_eval
+++++++   evaluation_prefix: phoenix2014-groundtruth
+++++++ decode_mode: beam
+++++++-device: your_device
++++++++device: cuda
+++++++ dist_url: env://
+++++++ eval_interval: 1
+++++++ evaluate_tool: python
+++++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
+++++++index c126e7a..e5adb9a 100644
+++++++--- a/work_dirt/dataloader_video.py
++++++++++ b/work_dirt/dataloader_video.py
+++++++@@ -9,6 +9,7 @@ import torch
+++++++ import random
+++++++ import pandas
+++++++ import warnings
++++++++import time
+++++++ 
+++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++++++ 
+++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++++++ 
+++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++-
++++++++            # print('phoenix 데이터를 사용함')
++++++++            # print(img_folder)
+++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++ #            print(img_folder)
+++++++ #            img_list = sorted(glob.glob(img_folder))
+++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++++++     def read_features(self, index):
+++++++         # load file info
+++++++         fi = self.inputs_list[index]
++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++        time.sleep(10)
+++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++++         return data['features'], data['label']
+++++++ 
+++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
+++++++index 00c1a0e..e4c3469 100644
+++++++--- a/work_dirt/dev.txt
++++++++++ b/work_dirt/dev.txt
+++++++@@ -8,3 +8,19 @@
+++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
+++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++[ Thu Apr  3 11:59:17 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++index 8a22ece..7defa11 100644
+++++++--- a/work_dirt/dirty.patch
++++++++++ b/work_dirt/dirty.patch
+++++++@@ -1,284 +1,13595 @@
+++++++-diff --git a/README.md b/README.md
+++++++-index bdbc17f..8cb240b 100644
+++++++---- a/README.md
+++++++-+++ b/README.md
+++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++++- 
+++++++- 
+++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++- 
+++++++- ### Training
+++++++- 
+++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++-index bfc1da8..25ffa61 100644
+++++++---- a/configs/baseline.yaml
+++++++-+++ b/configs/baseline.yaml
+++++++-@@ -1,14 +1,14 @@
+++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++- phase: train
+++++++--dataset: phoenix2014-T
+++++++-+dataset: phoenix2014
+++++++- #CSL-Daily
+++++++- # dataset: phoenix14-si5
+++++++- 
+++++++- work_dir: ./work_dirt/
+++++++--batch_size: 4
+++++++-+batch_size: 1
+++++++- random_seed: 0 
+++++++--test_batch_size: 4
+++++++--num_worker: 20
+++++++-+test_batch_size: 1
+++++++-+num_worker: 3
+++++++- device: 0
+++++++- log_interval: 10000
+++++++- eval_interval: 1
++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
++++++++index ffef81f..f4065cc 100644
++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++index 7ac0c3b..c89f220 100644
++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
++++++++index 529dabc..1c62326 100644
++++++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
+++++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++-index 555f4b8..c126e7a 100644
++++++++index c126e7a..e5adb9a 100644
+++++++ --- a/dataset/dataloader_video.py
+++++++ +++ b/dataset/dataloader_video.py
+++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++++-+
+++++++-         print(mode, len(self))
+++++++-         self.data_aug = self.transform()
+++++++-         print("")
+++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++@@ -9,6 +9,7 @@ import torch
++++++++ import random
++++++++ import pandas
++++++++ import warnings
+++++++++import time
+++++++  
+++++++-     def read_video(self, index):
+++++++--        # load file info
+++++++-         fi = self.inputs_list[index]
+++++++-+    
+++++++-         if 'phoenix' in self.dataset:
+++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++-+#            print(img_list)
+++++++-+
+++++++-+#            print("[LOG] Using phoenix")
+++++++-+
+++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++++++  
+++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++-+#            print(img_folder)
+++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++-+#            print(len(img_list))
+++++++-         elif self.dataset == 'CSL-Daily':
+++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++-+    
+++++++-         img_list = sorted(glob.glob(img_folder))
+++++++-+    
+++++++-+        if len(img_list) == 0:
+++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++-+    
+++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++-+    
+++++++-         label_list = []
+++++++--        if self.dataset=='phoenix2014':
+++++++-+        if self.dataset == 'phoenix2014':
+++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++--        if self.dataset=='phoenix2014-T':
+++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++-+    
+++++++-         for phase in fi['label'].split(" "):
+++++++--            if phase == '':
+++++++--                continue
+++++++--            if phase in self.dict.keys():
+++++++-+            if phase and phase in self.dict:
+++++++-                 label_list.append(self.dict[phase][0])
+++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++-+    
+++++++-+        video = [
+++++++-+            cv2.cvtColor(
+++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++-+                cv2.COLOR_BGR2RGB
+++++++-+            )   
+++++++-+            for img_path in img_list
+++++++-+        ]
+++++++-+    
+++++++-+        return video, label_list, fi
++++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++++++  
++++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++-
+++++++++            # print('phoenix 데이터를 사용함')
+++++++++            # print(img_folder)
++++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++ #            print(img_folder)
++++++++ #            img_list = sorted(glob.glob(img_folder))
++++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++++++      def read_features(self, index):
+++++++          # load file info
++++++++         fi = self.inputs_list[index]
+++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++        time.sleep(10)
++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++++++         return data['features'], data['label']
++++++++ 
+++++++ diff --git a/main.py b/main.py
+++++++-index 9e68cee..18ac59b 100644
++++++++index 18ac59b..8f75cf5 100644
+++++++ --- a/main.py
+++++++ +++ b/main.py
+++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++-                 batch_size=batch_size,
+++++++-                 collate_fn=self.feeder.collate_fn,
+++++++-                 num_workers=self.arg.num_worker,
+++++++--                pin_memory=True,
+++++++-+                pin_memory=False,
+++++++-                 worker_init_fn=self.init_fn,
+++++++-             )
+++++++-             return loader
+++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++-                 drop_last=train_flag,
+++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++-                 collate_fn=self.feeder.collate_fn,
+++++++--                pin_memory=True,
+++++++-+                pin_memory=False,
+++++++-                 worker_init_fn=self.init_fn,
+++++++-             )
+++++++- 
+++++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++++-index 528856d..d8fcaf9 100644
+++++++---- a/seq_scripts.py
+++++++-+++ b/seq_scripts.py
+++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++-     return
+++++++- 
+++++++- 
+++++++-+import csv 
+++++++-+from jiwer import wer as jiwer_wer
+++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++-     model.eval()
+++++++--    results=defaultdict(dict)
+++++++-+    results = defaultdict(dict)
+++++++- 
+++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++-         recoder.record_timer("device")
+++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++-                 results[inf]['gloss'] = gl
+++++++-+
+++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++-+
+++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++--        reg_per = wer_results
+++++++--    else:
+++++++--        reg_per = wer_results_con
+++++++-+
+++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++@@ -21,6 +21,7 @@ import utils
++++++++ from seq_scripts import seq_train, seq_eval
++++++++ from torch.cuda.amp import autocast as autocast
++++++++ from utils.misc import *
+++++++++from utils.decode import analyze_frame_lengths
++++++++ class Processor():
++++++++     def __init__(self, arg):
++++++++         self.arg = arg
++++++++@@ -105,13 +106,25 @@ class Processor():
++++++++                 print('Please appoint --weights.')
++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++ +
+++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++-         f"{work_dir}/{mode}.txt")
+++++++++            train_wer = seq_eval(
+++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++            )
+++++++++            dev_wer = seq_eval(
+++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++            )
+++++++++            test_wer = seq_eval(
+++++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++            )
+++++++ +
+++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++-+        f"{work_dir}/{mode}.txt")
++++++++             self.recoder.print_log('Evaluation Done.\n')
+++++++ +
+++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++++            analyze_frame_lengths()
+++++++ +
+++++++-+    rows = []
+++++++-+    for file_id in results:
+++++++-+        gt = results[file_id]['gloss']
+++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++         elif self.arg.phase == "features":
++++++++             for mode in ["train", "dev", "test"]:
++++++++                 seq_feature_generation(
++++++++@@ -119,6 +132,8 @@ class Processor():
++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++++                 )
++++++++ 
+++++++ +
+++++++-+        rows.append([
+++++++-+            file_id,
+++++++-+            gt,
+++++++-+            conv_pred,
+++++++-+            f"{conv_wer:.4f}",
+++++++-+            lstm_pred,
+++++++-+            f"{lstm_wer:.4f}"
+++++++-+        ])
+++++++ +
+++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++-+        writer = csv.writer(f)
+++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++-+        writer.writerows(rows)
++++++++     def save_arg(self):
++++++++         arg_dict = vars(self.arg)
++++++++         if not os.path.exists(self.arg.work_dir):
++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++index 71519fd..b9664e1 100644
++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++diff --git a/seq_scripts.py b/seq_scripts.py
++++++++index d8fcaf9..77cfc71 100644
++++++++--- a/seq_scripts.py
+++++++++++ b/seq_scripts.py
++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++         })
++++++++ 
++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++-    
+++++++++    # 전체 샘플 WER 평균 계산
+++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+++++++ +
+++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++    print("\n📊 전체 평균 WER")
+++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+++++++ +
++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++     for sample in top5:
++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++index e69de29..0342039 100644
++++++++--- a/tmp.ipynb
+++++++++++ b/tmp.ipynb
++++++++@@ -0,0 +1,272 @@
+++++++++{
+++++++++ "cells": [
+++++++++  {
+++++++++   "cell_type": "code",
+++++++++   "execution_count": 2,
+++++++++   "metadata": {},
+++++++++   "outputs": [],
+++++++++   "source": [
+++++++++    "import torch\n",
+++++++++    "\n",
+++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++   ]
+++++++++  },
+++++++++  {
+++++++++   "cell_type": "code",
+++++++++   "execution_count": 3,
+++++++++   "metadata": {},
+++++++++   "outputs": [
+++++++++    {
+++++++++     "name": "stdout",
+++++++++     "output_type": "stream",
+++++++++     "text": [
+++++++++      "Model is a state_dict.\n",
+++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++     ]
+++++++++    }
+++++++++   ],
+++++++++   "source": [
+++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++   ]
+++++++++  },
+++++++++  {
+++++++++   "cell_type": "code",
+++++++++   "execution_count": 7,
+++++++++   "metadata": {},
+++++++++   "outputs": [],
+++++++++   "source": [
+++++++++    "import torch\n",
+++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++    "import torch.nn.functional as F\n"
+++++++++   ]
+++++++++  },
+++++++++  {
+++++++++   "cell_type": "code",
+++++++++   "execution_count": 1,
+++++++++   "metadata": {},
+++++++++   "outputs": [
+++++++++    {
+++++++++     "name": "stderr",
+++++++++     "output_type": "stream",
+++++++++     "text": [
+++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++     ]
+++++++++    }
+++++++++   ],
+++++++++   "source": [
+++++++++    "import torch\n",
+++++++++    "import torch.nn as nn\n",
+++++++++    "import torch.nn.functional as F\n",
+++++++++    "import torchvision.models as models\n",
+++++++++    "import numpy as np\n",
+++++++++    "import modules.resnet as resnet\n",
+++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++    "from modules.criterions import SeqKD\n",
+++++++++    "import utils\n",
+++++++++    "import modules.resnet as resnet\n",
+++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++    "class Identity(nn.Module):\n",
+++++++++    "    def __init__(self):\n",
+++++++++    "        super(Identity, self).__init__()\n",
+++++++++    "\n",
+++++++++    "    def forward(self, x):\n",
+++++++++    "        return x\n",
+++++++++    "\n",
+++++++++    "# L2 정규화 선형 레이어\n",
+++++++++    "class NormLinear(nn.Module):\n",
+++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++    "\n",
+++++++++    "    def forward(self, x):\n",
+++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++    "        return outputs\n",
+++++++++    "\n",
+++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++    "class SLRModel(nn.Module):\n",
+++++++++    "    def __init__(\n",
+++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++    "    ):\n",
+++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++    "        self.decoder = None\n",
+++++++++    "        self.loss = dict()\n",
+++++++++    "        self.criterion_init()\n",
+++++++++    "        self.num_classes = num_classes\n",
+++++++++    "        self.loss_weights = loss_weights\n",
+++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++    "\n",
+++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++    "                                   conv_type=conv_type,\n",
+++++++++    "                                   use_bn=use_bn,\n",
+++++++++    "                                   num_classes=num_classes)\n",
+++++++++    "\n",
+++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++    "\n",
+++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++    "\n",
+++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++    "        if weight_norm:\n",
+++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++    "        else:\n",
+++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++    "\n",
+++++++++    "        # Classifier 공유 여부\n",
+++++++++    "        if share_classifier:\n",
+++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++    "\n",
+++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++    "        if len(x.shape) == 5:\n",
+++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++    "        else:\n",
+++++++++    "            framewise = x\n",
+++++++++    "\n",
+++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++    "\n",
+++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++    "\n",
+++++++++    "        # 최종 Classifier 적용\n",
+++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++    "\n",
+++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++    "\n",
+++++++++    "        return {\n",
+++++++++    "            \"framewise_features\": framewise,\n",
+++++++++    "            \"visual_features\": x,\n",
+++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++    "            \"feat_len\": lgt,\n",
+++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++    "            \"recognized_sents\": pred,\n",
+++++++++    "        }\n",
+++++++++    "\n",
+++++++++    "    def criterion_init(self):\n",
+++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++    "        return self.loss\n"
+++++++++   ]
+++++++++  },
+++++++++  {
+++++++++   "cell_type": "code",
+++++++++   "execution_count": 6,
+++++++++   "metadata": {},
+++++++++   "outputs": [
+++++++++    {
+++++++++     "ename": "KeyError",
+++++++++     "evalue": "'dataset_info'",
+++++++++     "output_type": "error",
+++++++++     "traceback": [
+++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++     ]
+++++++++    }
+++++++++   ],
+++++++++   "source": [
+++++++++    "import os\n",
+++++++++    "import numpy as np\n",
+++++++++    "import yaml\n",
+++++++++    "\n",
+++++++++    "# 환경 변수 설정\n",
+++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++    "\n",
+++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++    "\n",
+++++++++    "# ✅ gloss_dict 로드\n",
+++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++    "\n",
+++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++   ]
+++++++++  },
+++++++++  {
+++++++++   "cell_type": "code",
+++++++++   "execution_count": 5,
+++++++++   "metadata": {},
+++++++++   "outputs": [
+++++++++    {
+++++++++     "ename": "AttributeError",
+++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++     "output_type": "error",
+++++++++     "traceback": [
+++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++     ]
+++++++++    }
+++++++++   ],
+++++++++   "source": [
+++++++++    "import torch\n",
+++++++++    "\n",
+++++++++    "# 모델 불러오기\n",
+++++++++    "model = SLRModel(\n",
+++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++    ")\n",
+++++++++    "\n",
+++++++++    "# 저장된 가중치 로드\n",
+++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++    "\n",
+++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++    "if isinstance(state_dict, dict):\n",
+++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++    "\n",
+++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++    "model.eval()\n"
+++++++++   ]
+++++++++  }
+++++++++ ],
+++++++++ "metadata": {
+++++++++  "kernelspec": {
+++++++++   "display_name": "3.9.13",
+++++++++   "language": "python",
+++++++++   "name": "python3"
+++++++++  },
+++++++++  "language_info": {
+++++++++   "codemirror_mode": {
+++++++++    "name": "ipython",
+++++++++    "version": 3
+++++++++   },
+++++++++   "file_extension": ".py",
+++++++++   "mimetype": "text/x-python",
+++++++++   "name": "python",
+++++++++   "nbconvert_exporter": "python",
+++++++++   "pygments_lexer": "ipython3",
+++++++++   "version": "3.9.13"
+++++++++  }
+++++++++ },
+++++++++ "nbformat": 4,
+++++++++ "nbformat_minor": 2
+++++++++}
++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++index cb157af..c502b5d 100644
++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++index 3877729..ac8dab6 100644
++++++++--- a/utils/decode.py
+++++++++++ b/utils/decode.py
++++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++++ import numpy as np
++++++++ from itertools import groupby
++++++++ import torch.nn.functional as F
+++++++++import matplotlib.pyplot as plt
+++++++ +
+++++++++# ⬇ 프레임 길이 저장용 전역 리스트
+++++++++frame_lengths = []
+++++++ +
+++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++-+    sample_wers = []
+++++++-+    for file_id in results:
+++++++-+        gt = results[file_id]['gloss']
+++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++-+    
+++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++-+    
+++++++-+        sample_wers.append({
+++++++-+            'file_id': file_id,
+++++++-+            'gt': gt,
+++++++-+            'conv_pred': conv_pred,
+++++++-+            'conv_wer': conv_wer,
+++++++-+            'lstm_pred': lstm_pred,
+++++++-+            'lstm_wer': lstm_wer,
+++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++-+        })
+++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++    pred_ids = torch.argmax(probs, dim=-1)
+++++++ +
+++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++-+    
+++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++-+    for sample in top5:
+++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++-+        print(f"GT   : {sample['gt']}")
+++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++-+        print("-" * 60)
+++++++++    length = int(vid_lgt[batch_idx].item())
+++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++    x = np.arange(length)
+++++++ +
+++++++++    plt.figure(figsize=(15, 4))
+++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++++ +
+++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++++    if len(blank_indices) > 0:
+++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++++ +
+++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++++++++    if sample_id:
+++++++++        title_str += f"\nSample: {sample_id}"
+++++++++    plt.title(title_str)
+++++++++    plt.xlabel("Time Step")
+++++++++    plt.ylabel("Gloss ID")
+++++++++    plt.yticks(np.unique(pred_seq))
+++++++++    plt.grid(True)
+++++++++    plt.legend()
+++++++++    plt.tight_layout()
+++++++++    plt.show()
++++++++ 
++++++++ 
++++++++ class Decode(object):
++++++++@@ -16,35 +48,27 @@ class Decode(object):
++++++++         self.search_mode = search_mode
++++++++         self.blank_id = blank_id
++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++++++++-                                                    num_processes=10)
+++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++++++++ 
++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++++++++         if not batch_first:
++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++ +
+++++++++        # ⬇ 프레임 길이 수집
+++++++++        global frame_lengths
+++++++++        for i in range(vid_lgt.size(0)):
+++++++++            frame_lengths.append(int(vid_lgt[i].item()))
+++++++ +
+++++++++        # sample_id가 존재하면 시각화
+++++++++        sample_id = sample_ids[0] if sample_ids else None
+++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+++++++ +
++++++++         if self.search_mode == "max":
++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++         else:
++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
++++++++ 
++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++++++++-        '''
++++++++-        CTCBeamDecoder Shape:
++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++++++++-                          timesteps (B, N_beams)
++++++++-                          out_lens (B, N_beams)
++++++++-        '''
++++++++-
++++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
++++++++-        batchsize, lgt = index_list.shape
++++++++-        blank_rate =[]
++++++++-        for batch_idx in range(batchsize):
++++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++++++++-            blank_rate.append(group_result)
++++++++-
++++++++-
++++++++         if not probs:
++++++++             nn_output = nn_output.softmax(-1).cpu()
++++++++         vid_lgt = vid_lgt.cpu()
++++++++@@ -54,9 +78,7 @@ class Decode(object):
++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++++++++             if len(first_result) != 0:
++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++++++++-                             enumerate(first_result)])
+++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++++++++         return ret_list
++++++++ 
++++++++     def MaxDecode(self, nn_output, vid_lgt):
++++++++@@ -65,12 +87,34 @@ class Decode(object):
++++++++         ret_list = []
++++++++         for batch_idx in range(batchsize):
++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++++++++            filtered = [x for x in group_result if x != self.blank_id]
++++++++             if len(filtered) > 0:
++++++++                 max_result = torch.stack(filtered)
++++++++                 max_result = [x[0] for x in groupby(max_result)]
++++++++             else:
++++++++                 max_result = filtered
++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++++++++-                             enumerate(max_result)])
+++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
++++++++         return ret_list
+++++++ +
+++++++ +
+++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
+++++++++def analyze_frame_lengths():
+++++++++    if not frame_lengths:
+++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
+++++++++        return
+++++++ +
+++++++++    print("\n📊 Test Video Frame Length Analysis:")
+++++++++    print(f"- Total samples: {len(frame_lengths)}")
+++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
+++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
+++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
+++++++ +
+++++++++    # 히스토그램 시각화
+++++++++    plt.figure(figsize=(10, 5))
+++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
+++++++++    plt.title("Test Video Frame Length Distribution")
+++++++++    plt.xlabel("Frame Length")
+++++++++    plt.ylabel("Number of Samples")
+++++++++    plt.grid(True)
+++++++++    plt.tight_layout()
+++++++++    plt.show()
++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++index 7d0a2aa..cd66258 100644
++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++++++index c31483a..239691c 100644
++++++++--- a/work_dirt/config.yaml
+++++++++++ b/work_dirt/config.yaml
++++++++@@ -7,7 +7,7 @@ dataset_info:
++++++++   evaluation_dir: ./evaluation/slr_eval
++++++++   evaluation_prefix: phoenix2014-groundtruth
++++++++ decode_mode: beam
++++++++-device: your_device
+++++++++device: cuda
++++++++ dist_url: env://
++++++++ eval_interval: 1
++++++++ evaluate_tool: python
++++++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
++++++++index c126e7a..e5adb9a 100644
++++++++--- a/work_dirt/dataloader_video.py
+++++++++++ b/work_dirt/dataloader_video.py
++++++++@@ -9,6 +9,7 @@ import torch
++++++++ import random
++++++++ import pandas
++++++++ import warnings
+++++++++import time
++++++++ 
++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++++++ 
++++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++++++++ 
++++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++-
+++++++++            # print('phoenix 데이터를 사용함')
+++++++++            # print(img_folder)
++++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++ #            print(img_folder)
++++++++ #            img_list = sorted(glob.glob(img_folder))
++++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++++++++     def read_features(self, index):
++++++++         # load file info
++++++++         fi = self.inputs_list[index]
+++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++        time.sleep(10)
++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++++++         return data['features'], data['label']
++++++++ 
++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
++++++++index 00c1a0e..c843502 100644
++++++++--- a/work_dirt/dev.txt
+++++++++++ b/work_dirt/dev.txt
++++++++@@ -8,3 +8,17 @@
++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++[ Wed Apr  2 17:46:58 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++++index 8a22ece..93738f8 100644
++++++++--- a/work_dirt/dirty.patch
+++++++++++ b/work_dirt/dirty.patch
++++++++@@ -1,284 +1,12575 @@
++++++++-diff --git a/README.md b/README.md
++++++++-index bdbc17f..8cb240b 100644
++++++++---- a/README.md
++++++++-+++ b/README.md
++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++- 
++++++++- 
++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++++- 
++++++++- ### Training
++++++++- 
++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++++-index bfc1da8..25ffa61 100644
++++++++---- a/configs/baseline.yaml
++++++++-+++ b/configs/baseline.yaml
++++++++-@@ -1,14 +1,14 @@
++++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++++- phase: train
++++++++--dataset: phoenix2014-T
++++++++-+dataset: phoenix2014
++++++++- #CSL-Daily
++++++++- # dataset: phoenix14-si5
++++++++- 
++++++++- work_dir: ./work_dirt/
++++++++--batch_size: 4
++++++++-+batch_size: 1
++++++++- random_seed: 0 
++++++++--test_batch_size: 4
++++++++--num_worker: 20
++++++++-+test_batch_size: 1
++++++++-+num_worker: 3
++++++++- device: 0
++++++++- log_interval: 10000
++++++++- eval_interval: 1
+++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
+++++++++index ffef81f..f4065cc 100644
+++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
+++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++++index 7ac0c3b..c89f220 100644
+++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
+++++++++index 529dabc..1c62326 100644
+++++++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
++++++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++++-index 555f4b8..c126e7a 100644
+++++++++index c126e7a..e5adb9a 100644
++++++++ --- a/dataset/dataloader_video.py
++++++++ +++ b/dataset/dataloader_video.py
++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++-+
++++++++-         print(mode, len(self))
++++++++-         self.data_aug = self.transform()
++++++++-         print("")
++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
+++++++++@@ -9,6 +9,7 @@ import torch
+++++++++ import random
+++++++++ import pandas
+++++++++ import warnings
++++++++++import time
++++++++  
++++++++-     def read_video(self, index):
++++++++--        # load file info
++++++++-         fi = self.inputs_list[index]
++++++++-+    
++++++++-         if 'phoenix' in self.dataset:
++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++++-+#            print(img_list)
++++++++-+
++++++++-+#            print("[LOG] Using phoenix")
++++++++-+
++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++++++  
++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++-+#            print(img_folder)
++++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++++-+#            print(len(img_list))
++++++++-         elif self.dataset == 'CSL-Daily':
++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++++-+    
++++++++-         img_list = sorted(glob.glob(img_folder))
++++++++-+    
++++++++-+        if len(img_list) == 0:
++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++++-+    
++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++++++-+    
++++++++-         label_list = []
++++++++--        if self.dataset=='phoenix2014':
++++++++-+        if self.dataset == 'phoenix2014':
++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++++--        if self.dataset=='phoenix2014-T':
++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++++-+        elif self.dataset == 'phoenix2014-T':
++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++++-+    
++++++++-         for phase in fi['label'].split(" "):
++++++++--            if phase == '':
++++++++--                continue
++++++++--            if phase in self.dict.keys():
++++++++-+            if phase and phase in self.dict:
++++++++-                 label_list.append(self.dict[phase][0])
++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++++-+    
++++++++-+        video = [
++++++++-+            cv2.cvtColor(
++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++-+                cv2.COLOR_BGR2RGB
++++++++-+            )   
++++++++-+            for img_path in img_list
++++++++-+        ]
++++++++-+    
++++++++-+        return video, label_list, fi
+++++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++++++++  
+++++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++-
++++++++++            # print('phoenix 데이터를 사용함')
++++++++++            # print(img_folder)
+++++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++ #            print(img_folder)
+++++++++ #            img_list = sorted(glob.glob(img_folder))
+++++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++++++++      def read_features(self, index):
++++++++          # load file info
+++++++++         fi = self.inputs_list[index]
++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++        time.sleep(10)
+++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++++++         return data['features'], data['label']
+++++++++ 
++++++++ diff --git a/main.py b/main.py
++++++++-index 9e68cee..18ac59b 100644
+++++++++index 18ac59b..8f75cf5 100644
++++++++ --- a/main.py
++++++++ +++ b/main.py
++++++++-@@ -256,7 +256,7 @@ class Processor():
++++++++-                 batch_size=batch_size,
++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++-                 num_workers=self.arg.num_worker,
++++++++--                pin_memory=True,
++++++++-+                pin_memory=False,
++++++++-                 worker_init_fn=self.init_fn,
++++++++-             )
++++++++-             return loader
++++++++-@@ -268,7 +268,7 @@ class Processor():
++++++++-                 drop_last=train_flag,
++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++--                pin_memory=True,
++++++++-+                pin_memory=False,
++++++++-                 worker_init_fn=self.init_fn,
++++++++-             )
++++++++- 
++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++++++-index 528856d..d8fcaf9 100644
++++++++---- a/seq_scripts.py
++++++++-+++ b/seq_scripts.py
++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++++-     return
++++++++- 
++++++++- 
++++++++-+import csv 
++++++++-+from jiwer import wer as jiwer_wer
++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++++-     model.eval()
++++++++--    results=defaultdict(dict)
++++++++-+    results = defaultdict(dict)
++++++++- 
++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++++-         recoder.record_timer("device")
++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++-                 results[inf]['conv_sents'] = conv_sents
++++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++++-                 results[inf]['gloss'] = gl
++++++++-+
++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++-+
++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++++--        reg_per = wer_results
++++++++--    else:
++++++++--        reg_per = wer_results_con
++++++++-+
++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++@@ -21,6 +21,7 @@ import utils
+++++++++ from seq_scripts import seq_train, seq_eval
+++++++++ from torch.cuda.amp import autocast as autocast
+++++++++ from utils.misc import *
++++++++++from utils.decode import analyze_frame_lengths
+++++++++ class Processor():
+++++++++     def __init__(self, arg):
+++++++++         self.arg = arg
+++++++++@@ -105,13 +106,25 @@ class Processor():
+++++++++                 print('Please appoint --weights.')
+++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++ +
++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++++-         f"{work_dir}/{mode}.txt")
++++++++++            train_wer = seq_eval(
++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++            )
++++++++++            dev_wer = seq_eval(
++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++            )
++++++++++            test_wer = seq_eval(
++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++            )
++++++++ +
++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++++-+        f"{work_dir}/{mode}.txt")
+++++++++             self.recoder.print_log('Evaluation Done.\n')
++++++++ +
++++++++-+    # ✅ 전체 결과 CSV로 저장
++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++++++            analyze_frame_lengths()
++++++++ +
++++++++-+    rows = []
++++++++-+    for file_id in results:
++++++++-+        gt = results[file_id]['gloss']
++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++-+
++++++++-+        rows.append([
++++++++-+            file_id,
++++++++-+            gt,
++++++++-+            conv_pred,
++++++++-+            f"{conv_wer:.4f}",
++++++++-+            lstm_pred,
++++++++-+            f"{lstm_wer:.4f}"
++++++++-+        ])
+++++++++         elif self.arg.phase == "features":
+++++++++             for mode in ["train", "dev", "test"]:
+++++++++                 seq_feature_generation(
+++++++++@@ -119,6 +132,8 @@ class Processor():
+++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++++++                 )
+++++++++ 
++++++++ +
++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++++-+        writer = csv.writer(f)
++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++++-+        writer.writerows(rows)
++++++++ +
++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++     def save_arg(self):
+++++++++         arg_dict = vars(self.arg)
+++++++++         if not os.path.exists(self.arg.work_dir):
+++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++++index 71519fd..b9664e1 100644
+++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++++diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++index d8fcaf9..77cfc71 100644
+++++++++--- a/seq_scripts.py
++++++++++++ b/seq_scripts.py
+++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++         })
+++++++++ 
+++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++-    
++++++++++    # 전체 샘플 WER 평균 계산
++++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
++++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
++++++++ +
++++++++++    print("\n📊 전체 평균 WER")
++++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
++++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
++++++++ +
+++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++     for sample in top5:
+++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++++index e69de29..0342039 100644
+++++++++--- a/tmp.ipynb
++++++++++++ b/tmp.ipynb
+++++++++@@ -0,0 +1,272 @@
++++++++++{
++++++++++ "cells": [
++++++++++  {
++++++++++   "cell_type": "code",
++++++++++   "execution_count": 2,
++++++++++   "metadata": {},
++++++++++   "outputs": [],
++++++++++   "source": [
++++++++++    "import torch\n",
++++++++++    "\n",
++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++++   ]
++++++++++  },
++++++++++  {
++++++++++   "cell_type": "code",
++++++++++   "execution_count": 3,
++++++++++   "metadata": {},
++++++++++   "outputs": [
++++++++++    {
++++++++++     "name": "stdout",
++++++++++     "output_type": "stream",
++++++++++     "text": [
++++++++++      "Model is a state_dict.\n",
++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++++     ]
++++++++++    }
++++++++++   ],
++++++++++   "source": [
++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++++   ]
++++++++++  },
++++++++++  {
++++++++++   "cell_type": "code",
++++++++++   "execution_count": 7,
++++++++++   "metadata": {},
++++++++++   "outputs": [],
++++++++++   "source": [
++++++++++    "import torch\n",
++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++++    "import torch.nn.functional as F\n"
++++++++++   ]
++++++++++  },
++++++++++  {
++++++++++   "cell_type": "code",
++++++++++   "execution_count": 1,
++++++++++   "metadata": {},
++++++++++   "outputs": [
++++++++++    {
++++++++++     "name": "stderr",
++++++++++     "output_type": "stream",
++++++++++     "text": [
++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++++     ]
++++++++++    }
++++++++++   ],
++++++++++   "source": [
++++++++++    "import torch\n",
++++++++++    "import torch.nn as nn\n",
++++++++++    "import torch.nn.functional as F\n",
++++++++++    "import torchvision.models as models\n",
++++++++++    "import numpy as np\n",
++++++++++    "import modules.resnet as resnet\n",
++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++++    "from modules.criterions import SeqKD\n",
++++++++++    "import utils\n",
++++++++++    "import modules.resnet as resnet\n",
++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++++    "class Identity(nn.Module):\n",
++++++++++    "    def __init__(self):\n",
++++++++++    "        super(Identity, self).__init__()\n",
++++++++++    "\n",
++++++++++    "    def forward(self, x):\n",
++++++++++    "        return x\n",
++++++++++    "\n",
++++++++++    "# L2 정규화 선형 레이어\n",
++++++++++    "class NormLinear(nn.Module):\n",
++++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++++    "        super(NormLinear, self).__init__()\n",
++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++++    "\n",
++++++++++    "    def forward(self, x):\n",
++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++++    "        return outputs\n",
++++++++++    "\n",
++++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++++    "class SLRModel(nn.Module):\n",
++++++++++    "    def __init__(\n",
++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++++    "    ):\n",
++++++++++    "        super(SLRModel, self).__init__()\n",
++++++++++    "        self.decoder = None\n",
++++++++++    "        self.loss = dict()\n",
++++++++++    "        self.criterion_init()\n",
++++++++++    "        self.num_classes = num_classes\n",
++++++++++    "        self.loss_weights = loss_weights\n",
++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++++    "\n",
++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++++    "                                   hidden_size=hidden_size,\n",
++++++++++    "                                   conv_type=conv_type,\n",
++++++++++    "                                   use_bn=use_bn,\n",
++++++++++    "                                   num_classes=num_classes)\n",
++++++++++    "\n",
++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++++    "\n",
++++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++++    "\n",
++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++++    "        if weight_norm:\n",
++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++++    "        else:\n",
++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++    "\n",
++++++++++    "        # Classifier 공유 여부\n",
++++++++++    "        if share_classifier:\n",
++++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++++    "\n",
++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++++    "        if len(x.shape) == 5:\n",
++++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++++    "        else:\n",
++++++++++    "            framewise = x\n",
++++++++++    "\n",
++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++++    "\n",
++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++++    "\n",
++++++++++    "        # 최종 Classifier 적용\n",
++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++++    "\n",
++++++++++    "        # Inference 모드에서 Decoding\n",
++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++++    "\n",
++++++++++    "        return {\n",
++++++++++    "            \"framewise_features\": framewise,\n",
++++++++++    "            \"visual_features\": x,\n",
++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++++    "            \"feat_len\": lgt,\n",
++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++++    "            \"sequence_logits\": outputs,\n",
++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++++    "            \"recognized_sents\": pred,\n",
++++++++++    "        }\n",
++++++++++    "\n",
++++++++++    "    def criterion_init(self):\n",
++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++++    "        return self.loss\n"
++++++++++   ]
++++++++++  },
++++++++++  {
++++++++++   "cell_type": "code",
++++++++++   "execution_count": 6,
++++++++++   "metadata": {},
++++++++++   "outputs": [
++++++++++    {
++++++++++     "ename": "KeyError",
++++++++++     "evalue": "'dataset_info'",
++++++++++     "output_type": "error",
++++++++++     "traceback": [
++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++++     ]
++++++++++    }
++++++++++   ],
++++++++++   "source": [
++++++++++    "import os\n",
++++++++++    "import numpy as np\n",
++++++++++    "import yaml\n",
++++++++++    "\n",
++++++++++    "# 환경 변수 설정\n",
++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++++    "\n",
++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++++    "with open(config_path, \"r\") as f:\n",
++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++++    "\n",
++++++++++    "# ✅ gloss_dict 로드\n",
++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++++    "\n",
++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++++   ]
++++++++++  },
++++++++++  {
++++++++++   "cell_type": "code",
++++++++++   "execution_count": 5,
++++++++++   "metadata": {},
++++++++++   "outputs": [
++++++++++    {
++++++++++     "ename": "AttributeError",
++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++++     "output_type": "error",
++++++++++     "traceback": [
++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++++     ]
++++++++++    }
++++++++++   ],
++++++++++   "source": [
++++++++++    "import torch\n",
++++++++++    "\n",
++++++++++    "# 모델 불러오기\n",
++++++++++    "model = SLRModel(\n",
++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++++    ")\n",
++++++++++    "\n",
++++++++++    "# 저장된 가중치 로드\n",
++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++++    "\n",
++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++++    "if isinstance(state_dict, dict):\n",
++++++++++    "    model.load_state_dict(state_dict)\n",
++++++++++    "\n",
++++++++++    "# 모델을 평가 모드로 설정\n",
++++++++++    "model.eval()\n"
++++++++++   ]
++++++++++  }
++++++++++ ],
++++++++++ "metadata": {
++++++++++  "kernelspec": {
++++++++++   "display_name": "3.9.13",
++++++++++   "language": "python",
++++++++++   "name": "python3"
++++++++++  },
++++++++++  "language_info": {
++++++++++   "codemirror_mode": {
++++++++++    "name": "ipython",
++++++++++    "version": 3
++++++++++   },
++++++++++   "file_extension": ".py",
++++++++++   "mimetype": "text/x-python",
++++++++++   "name": "python",
++++++++++   "nbconvert_exporter": "python",
++++++++++   "pygments_lexer": "ipython3",
++++++++++   "version": "3.9.13"
++++++++++  }
++++++++++ },
++++++++++ "nbformat": 4,
++++++++++ "nbformat_minor": 2
++++++++++}
+++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++++index cb157af..c502b5d 100644
+++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++++index 3877729..ac8dab6 100644
+++++++++--- a/utils/decode.py
++++++++++++ b/utils/decode.py
+++++++++@@ -6,6 +6,38 @@ import ctcdecode
+++++++++ import numpy as np
+++++++++ from itertools import groupby
+++++++++ import torch.nn.functional as F
++++++++++import matplotlib.pyplot as plt
++++++++ +
++++++++-+    # WER 기준 상위 5개 샘플 출력
++++++++-+    sample_wers = []
++++++++-+    for file_id in results:
++++++++-+        gt = results[file_id]['gloss']
++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++-+    
++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++-+    
++++++++-+        sample_wers.append({
++++++++-+            'file_id': file_id,
++++++++-+            'gt': gt,
++++++++-+            'conv_pred': conv_pred,
++++++++-+            'conv_wer': conv_wer,
++++++++-+            'lstm_pred': lstm_pred,
++++++++-+            'lstm_wer': lstm_wer,
++++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++++-+        })
++++++++++# ⬇ 프레임 길이 저장용 전역 리스트
++++++++++frame_lengths = []
++++++++ +
++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++-+    
++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++-+    for sample in top5:
++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++-+        print(f"GT   : {sample['gt']}")
++++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++++-+        print("-" * 60)
++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++++    pred_ids = torch.argmax(probs, dim=-1)
++++++++ +
++++++++++    length = int(vid_lgt[batch_idx].item())
++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++++    x = np.arange(length)
++++++++ +
++++++++++    plt.figure(figsize=(15, 4))
++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++++++ +
++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++++++    if len(blank_indices) > 0:
++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++++++ +
++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++++++++    if sample_id:
++++++++++        title_str += f"\nSample: {sample_id}"
++++++++++    plt.title(title_str)
++++++++++    plt.xlabel("Time Step")
++++++++++    plt.ylabel("Gloss ID")
++++++++++    plt.yticks(np.unique(pred_seq))
++++++++++    plt.grid(True)
++++++++++    plt.legend()
++++++++++    plt.tight_layout()
++++++++++    plt.show()
+++++++++ 
+++++++++ 
+++++++++ class Decode(object):
+++++++++@@ -16,35 +48,27 @@ class Decode(object):
+++++++++         self.search_mode = search_mode
+++++++++         self.blank_id = blank_id
+++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+++++++++-                                                    num_processes=10)
++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+++++++++ 
+++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+++++++++         if not batch_first:
+++++++++             nn_output = nn_output.permute(1, 0, 2)
++++++++ +
++++++++++        # ⬇ 프레임 길이 수집
++++++++++        global frame_lengths
++++++++++        for i in range(vid_lgt.size(0)):
++++++++++            frame_lengths.append(int(vid_lgt[i].item()))
++++++++ +
++++++++++        # sample_id가 존재하면 시각화
++++++++++        sample_id = sample_ids[0] if sample_ids else None
++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
++++++++ +
+++++++++         if self.search_mode == "max":
+++++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++++         else:
+++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
+++++++++ 
+++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+++++++++-        '''
+++++++++-        CTCBeamDecoder Shape:
+++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+++++++++-                          timesteps (B, N_beams)
+++++++++-                          out_lens (B, N_beams)
+++++++++-        '''
+++++++++-
+++++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
+++++++++-        batchsize, lgt = index_list.shape
+++++++++-        blank_rate =[]
+++++++++-        for batch_idx in range(batchsize):
+++++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+++++++++-            blank_rate.append(group_result)
+++++++++-
+++++++++-
+++++++++         if not probs:
+++++++++             nn_output = nn_output.softmax(-1).cpu()
+++++++++         vid_lgt = vid_lgt.cpu()
+++++++++@@ -54,9 +78,7 @@ class Decode(object):
+++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+++++++++             if len(first_result) != 0:
+++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+++++++++-                             enumerate(first_result)])
++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+++++++++         return ret_list
+++++++++ 
+++++++++     def MaxDecode(self, nn_output, vid_lgt):
+++++++++@@ -65,12 +87,34 @@ class Decode(object):
+++++++++         ret_list = []
+++++++++         for batch_idx in range(batchsize):
+++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++++++++++            filtered = [x for x in group_result if x != self.blank_id]
+++++++++             if len(filtered) > 0:
+++++++++                 max_result = torch.stack(filtered)
+++++++++                 max_result = [x[0] for x in groupby(max_result)]
+++++++++             else:
+++++++++                 max_result = filtered
+++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+++++++++-                             enumerate(max_result)])
++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++++++++         return ret_list
++++++++ +
++++++++ +
++++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
++++++++++def analyze_frame_lengths():
++++++++++    if not frame_lengths:
++++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
++++++++++        return
++++++++ +
++++++++++    print("\n📊 Test Video Frame Length Analysis:")
++++++++++    print(f"- Total samples: {len(frame_lengths)}")
++++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
++++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
++++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
++++++++ +
++++++++++    # 히스토그램 시각화
++++++++++    plt.figure(figsize=(10, 5))
++++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
++++++++++    plt.title("Test Video Frame Length Distribution")
++++++++++    plt.xlabel("Frame Length")
++++++++++    plt.ylabel("Number of Samples")
++++++++++    plt.grid(True)
++++++++++    plt.tight_layout()
++++++++++    plt.show()
+++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++++index 7d0a2aa..cd66258 100644
+++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++++++++index c31483a..239691c 100644
+++++++++--- a/work_dirt/config.yaml
++++++++++++ b/work_dirt/config.yaml
+++++++++@@ -7,7 +7,7 @@ dataset_info:
+++++++++   evaluation_dir: ./evaluation/slr_eval
+++++++++   evaluation_prefix: phoenix2014-groundtruth
+++++++++ decode_mode: beam
+++++++++-device: your_device
++++++++++device: cuda
+++++++++ dist_url: env://
+++++++++ eval_interval: 1
+++++++++ evaluate_tool: python
+++++++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
+++++++++index c126e7a..e5adb9a 100644
+++++++++--- a/work_dirt/dataloader_video.py
++++++++++++ b/work_dirt/dataloader_video.py
+++++++++@@ -9,6 +9,7 @@ import torch
+++++++++ import random
+++++++++ import pandas
+++++++++ import warnings
++++++++++import time
+++++++++ 
+++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++++++++ 
+++++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++++++++ 
+++++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++-
++++++++++            # print('phoenix 데이터를 사용함')
++++++++++            # print(img_folder)
+++++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++ #            print(img_folder)
+++++++++ #            img_list = sorted(glob.glob(img_folder))
+++++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++++++++     def read_features(self, index):
+++++++++         # load file info
+++++++++         fi = self.inputs_list[index]
++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++        time.sleep(10)
+++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++++++         return data['features'], data['label']
+++++++++ 
+++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
+++++++++index 00c1a0e..3297908 100644
+++++++++--- a/work_dirt/dev.txt
++++++++++++ b/work_dirt/dev.txt
+++++++++@@ -8,3 +8,15 @@
+++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
+++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++++index 8a22ece..d20ff39 100644
+++++++++--- a/work_dirt/dirty.patch
++++++++++++ b/work_dirt/dirty.patch
+++++++++@@ -1,284 +1,11565 @@
+++++++++-diff --git a/README.md b/README.md
+++++++++-index bdbc17f..8cb240b 100644
+++++++++---- a/README.md
+++++++++-+++ b/README.md
+++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++++++- 
+++++++++- 
+++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++++- 
+++++++++- ### Training
+++++++++- 
+++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++++-index bfc1da8..25ffa61 100644
+++++++++---- a/configs/baseline.yaml
+++++++++-+++ b/configs/baseline.yaml
+++++++++-@@ -1,14 +1,14 @@
+++++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++++- phase: train
+++++++++--dataset: phoenix2014-T
+++++++++-+dataset: phoenix2014
+++++++++- #CSL-Daily
+++++++++- # dataset: phoenix14-si5
+++++++++- 
+++++++++- work_dir: ./work_dirt/
+++++++++--batch_size: 4
+++++++++-+batch_size: 1
+++++++++- random_seed: 0 
+++++++++--test_batch_size: 4
+++++++++--num_worker: 20
+++++++++-+test_batch_size: 1
+++++++++-+num_worker: 3
+++++++++- device: 0
+++++++++- log_interval: 10000
+++++++++- eval_interval: 1
++++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
++++++++++index ffef81f..f4065cc 100644
++++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++++index 7ac0c3b..c89f220 100644
++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
++++++++++index 529dabc..f428bcc 100644
++++++++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
+++++++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++++-index 555f4b8..c126e7a 100644
++++++++++index c126e7a..199e2e9 100644
+++++++++ --- a/dataset/dataloader_video.py
+++++++++ +++ b/dataset/dataloader_video.py
+++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++++++-+
+++++++++-         print(mode, len(self))
+++++++++-         self.data_aug = self.transform()
+++++++++-         print("")
+++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++@@ -9,6 +9,7 @@ import torch
++++++++++ import random
++++++++++ import pandas
++++++++++ import warnings
+++++++++++import time
+++++++++  
+++++++++-     def read_video(self, index):
+++++++++--        # load file info
+++++++++-         fi = self.inputs_list[index]
+++++++++-+    
+++++++++-         if 'phoenix' in self.dataset:
+++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++++-+#            print(img_list)
+++++++++-+
+++++++++-+#            print("[LOG] Using phoenix")
+++++++++-+
+++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++++++++  
+++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++-+#            print(img_folder)
+++++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++++-+#            print(len(img_list))
+++++++++-         elif self.dataset == 'CSL-Daily':
+++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++++-+    
+++++++++-         img_list = sorted(glob.glob(img_folder))
+++++++++-+    
+++++++++-+        if len(img_list) == 0:
+++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++++-+    
+++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++-+    
+++++++++-         label_list = []
+++++++++--        if self.dataset=='phoenix2014':
+++++++++-+        if self.dataset == 'phoenix2014':
+++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++++--        if self.dataset=='phoenix2014-T':
+++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++++-+    
+++++++++-         for phase in fi['label'].split(" "):
+++++++++--            if phase == '':
+++++++++--                continue
+++++++++--            if phase in self.dict.keys():
+++++++++-+            if phase and phase in self.dict:
+++++++++-                 label_list.append(self.dict[phase][0])
+++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++++-+    
+++++++++-+        video = [
+++++++++-+            cv2.cvtColor(
+++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++-+                cv2.COLOR_BGR2RGB
+++++++++-+            )   
+++++++++-+            for img_path in img_list
+++++++++-+        ]
+++++++++-+    
+++++++++-+        return video, label_list, fi
++++++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++++++++  
++++++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++++-
+++++++++++            print('phoenix 데이터를 사용함')
+++++++++++            print(img_folder)
++++++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++ #            print(img_folder)
++++++++++ #            img_list = sorted(glob.glob(img_folder))
++++++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++++++++      def read_features(self, index):
+++++++++          # load file info
++++++++++         fi = self.inputs_list[index]
+++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++        time.sleep(10)
++++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++++++++         return data['features'], data['label']
++++++++++ 
+++++++++ diff --git a/main.py b/main.py
+++++++++-index 9e68cee..18ac59b 100644
++++++++++index 18ac59b..8f75cf5 100644
+++++++++ --- a/main.py
+++++++++ +++ b/main.py
+++++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++++-                 batch_size=batch_size,
+++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++-                 num_workers=self.arg.num_worker,
+++++++++--                pin_memory=True,
+++++++++-+                pin_memory=False,
+++++++++-                 worker_init_fn=self.init_fn,
+++++++++-             )
+++++++++-             return loader
+++++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++++-                 drop_last=train_flag,
+++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++--                pin_memory=True,
+++++++++-+                pin_memory=False,
+++++++++-                 worker_init_fn=self.init_fn,
+++++++++-             )
+++++++++- 
+++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++-index 528856d..d8fcaf9 100644
+++++++++---- a/seq_scripts.py
+++++++++-+++ b/seq_scripts.py
+++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++++-     return
+++++++++- 
+++++++++- 
+++++++++-+import csv 
+++++++++-+from jiwer import wer as jiwer_wer
+++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++++-     model.eval()
+++++++++--    results=defaultdict(dict)
+++++++++-+    results = defaultdict(dict)
+++++++++- 
+++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++++-         recoder.record_timer("device")
+++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++++-                 results[inf]['gloss'] = gl
+++++++++-+
+++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++-+
+++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++++--        reg_per = wer_results
+++++++++--    else:
+++++++++--        reg_per = wer_results_con
+++++++++-+
+++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++++@@ -21,6 +21,7 @@ import utils
++++++++++ from seq_scripts import seq_train, seq_eval
++++++++++ from torch.cuda.amp import autocast as autocast
++++++++++ from utils.misc import *
+++++++++++from utils.decode import analyze_frame_lengths
++++++++++ class Processor():
++++++++++     def __init__(self, arg):
++++++++++         self.arg = arg
++++++++++@@ -105,13 +106,25 @@ class Processor():
++++++++++                 print('Please appoint --weights.')
++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++ +
+++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++++-         f"{work_dir}/{mode}.txt")
+++++++++++            train_wer = seq_eval(
+++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++            )
+++++++++++            dev_wer = seq_eval(
+++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++            )
+++++++++++            test_wer = seq_eval(
+++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++            )
+++++++++ +
+++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++++-+        f"{work_dir}/{mode}.txt")
++++++++++             self.recoder.print_log('Evaluation Done.\n')
+++++++++ +
+++++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++++++            analyze_frame_lengths()
+++++++++ +
+++++++++-+    rows = []
+++++++++-+    for file_id in results:
+++++++++-+        gt = results[file_id]['gloss']
+++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++-+
+++++++++-+        rows.append([
+++++++++-+            file_id,
+++++++++-+            gt,
+++++++++-+            conv_pred,
+++++++++-+            f"{conv_wer:.4f}",
+++++++++-+            lstm_pred,
+++++++++-+            f"{lstm_wer:.4f}"
+++++++++-+        ])
++++++++++         elif self.arg.phase == "features":
++++++++++             for mode in ["train", "dev", "test"]:
++++++++++                 seq_feature_generation(
++++++++++@@ -119,6 +132,8 @@ class Processor():
++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++++++                 )
++++++++++ 
+++++++++ +
+++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++++-+        writer = csv.writer(f)
+++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++++-+        writer.writerows(rows)
+++++++++ +
+++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++++++++++     def save_arg(self):
++++++++++         arg_dict = vars(self.arg)
++++++++++         if not os.path.exists(self.arg.work_dir):
++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++++index 71519fd..b9664e1 100644
++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++++diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++index d8fcaf9..77cfc71 100644
++++++++++--- a/seq_scripts.py
+++++++++++++ b/seq_scripts.py
++++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++         })
++++++++++ 
++++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++-    
+++++++++++    # 전체 샘플 WER 평균 계산
+++++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+++++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+++++++++ +
+++++++++++    print("\n📊 전체 평균 WER")
+++++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+++++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+++++++++ +
++++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++     for sample in top5:
++++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++++index e69de29..0342039 100644
++++++++++--- a/tmp.ipynb
+++++++++++++ b/tmp.ipynb
++++++++++@@ -0,0 +1,272 @@
+++++++++++{
+++++++++++ "cells": [
+++++++++++  {
+++++++++++   "cell_type": "code",
+++++++++++   "execution_count": 2,
+++++++++++   "metadata": {},
+++++++++++   "outputs": [],
+++++++++++   "source": [
+++++++++++    "import torch\n",
+++++++++++    "\n",
+++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++++   ]
+++++++++++  },
+++++++++++  {
+++++++++++   "cell_type": "code",
+++++++++++   "execution_count": 3,
+++++++++++   "metadata": {},
+++++++++++   "outputs": [
+++++++++++    {
+++++++++++     "name": "stdout",
+++++++++++     "output_type": "stream",
+++++++++++     "text": [
+++++++++++      "Model is a state_dict.\n",
+++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++++     ]
+++++++++++    }
+++++++++++   ],
+++++++++++   "source": [
+++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++++   ]
+++++++++++  },
+++++++++++  {
+++++++++++   "cell_type": "code",
+++++++++++   "execution_count": 7,
+++++++++++   "metadata": {},
+++++++++++   "outputs": [],
+++++++++++   "source": [
+++++++++++    "import torch\n",
+++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++++    "import torch.nn.functional as F\n"
+++++++++++   ]
+++++++++++  },
+++++++++++  {
+++++++++++   "cell_type": "code",
+++++++++++   "execution_count": 1,
+++++++++++   "metadata": {},
+++++++++++   "outputs": [
+++++++++++    {
+++++++++++     "name": "stderr",
+++++++++++     "output_type": "stream",
+++++++++++     "text": [
+++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++++     ]
+++++++++++    }
+++++++++++   ],
+++++++++++   "source": [
+++++++++++    "import torch\n",
+++++++++++    "import torch.nn as nn\n",
+++++++++++    "import torch.nn.functional as F\n",
+++++++++++    "import torchvision.models as models\n",
+++++++++++    "import numpy as np\n",
+++++++++++    "import modules.resnet as resnet\n",
+++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++++    "from modules.criterions import SeqKD\n",
+++++++++++    "import utils\n",
+++++++++++    "import modules.resnet as resnet\n",
+++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++++    "class Identity(nn.Module):\n",
+++++++++++    "    def __init__(self):\n",
+++++++++++    "        super(Identity, self).__init__()\n",
+++++++++++    "\n",
+++++++++++    "    def forward(self, x):\n",
+++++++++++    "        return x\n",
+++++++++++    "\n",
+++++++++++    "# L2 정규화 선형 레이어\n",
+++++++++++    "class NormLinear(nn.Module):\n",
+++++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++++    "\n",
+++++++++++    "    def forward(self, x):\n",
+++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++++    "        return outputs\n",
+++++++++++    "\n",
+++++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++++    "class SLRModel(nn.Module):\n",
+++++++++++    "    def __init__(\n",
+++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++++    "    ):\n",
+++++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++++    "        self.decoder = None\n",
+++++++++++    "        self.loss = dict()\n",
+++++++++++    "        self.criterion_init()\n",
+++++++++++    "        self.num_classes = num_classes\n",
+++++++++++    "        self.loss_weights = loss_weights\n",
+++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++++    "\n",
+++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++++    "                                   conv_type=conv_type,\n",
+++++++++++    "                                   use_bn=use_bn,\n",
+++++++++++    "                                   num_classes=num_classes)\n",
+++++++++++    "\n",
+++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++++    "\n",
+++++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++++    "\n",
+++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++++    "        if weight_norm:\n",
+++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++    "        else:\n",
+++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++    "\n",
+++++++++++    "        # Classifier 공유 여부\n",
+++++++++++    "        if share_classifier:\n",
+++++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++++    "\n",
+++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++++    "        if len(x.shape) == 5:\n",
+++++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++++    "        else:\n",
+++++++++++    "            framewise = x\n",
+++++++++++    "\n",
+++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++++    "\n",
+++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++++    "\n",
+++++++++++    "        # 최종 Classifier 적용\n",
+++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++++    "\n",
+++++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++++    "\n",
+++++++++++    "        return {\n",
+++++++++++    "            \"framewise_features\": framewise,\n",
+++++++++++    "            \"visual_features\": x,\n",
+++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++++    "            \"feat_len\": lgt,\n",
+++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++++    "            \"recognized_sents\": pred,\n",
+++++++++++    "        }\n",
+++++++++++    "\n",
+++++++++++    "    def criterion_init(self):\n",
+++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++++    "        return self.loss\n"
+++++++++++   ]
+++++++++++  },
+++++++++++  {
+++++++++++   "cell_type": "code",
+++++++++++   "execution_count": 6,
+++++++++++   "metadata": {},
+++++++++++   "outputs": [
+++++++++++    {
+++++++++++     "ename": "KeyError",
+++++++++++     "evalue": "'dataset_info'",
+++++++++++     "output_type": "error",
+++++++++++     "traceback": [
+++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++++     ]
+++++++++++    }
+++++++++++   ],
+++++++++++   "source": [
+++++++++++    "import os\n",
+++++++++++    "import numpy as np\n",
+++++++++++    "import yaml\n",
+++++++++++    "\n",
+++++++++++    "# 환경 변수 설정\n",
+++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++++    "\n",
+++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++++    "\n",
+++++++++++    "# ✅ gloss_dict 로드\n",
+++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++++    "\n",
+++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++++   ]
+++++++++++  },
+++++++++++  {
+++++++++++   "cell_type": "code",
+++++++++++   "execution_count": 5,
+++++++++++   "metadata": {},
+++++++++++   "outputs": [
+++++++++++    {
+++++++++++     "ename": "AttributeError",
+++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++++     "output_type": "error",
+++++++++++     "traceback": [
+++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++++     ]
+++++++++++    }
+++++++++++   ],
+++++++++++   "source": [
+++++++++++    "import torch\n",
+++++++++++    "\n",
+++++++++++    "# 모델 불러오기\n",
+++++++++++    "model = SLRModel(\n",
+++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++++    ")\n",
+++++++++++    "\n",
+++++++++++    "# 저장된 가중치 로드\n",
+++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++++    "\n",
+++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++++    "if isinstance(state_dict, dict):\n",
+++++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++++    "\n",
+++++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++++    "model.eval()\n"
+++++++++++   ]
+++++++++++  }
+++++++++++ ],
+++++++++++ "metadata": {
+++++++++++  "kernelspec": {
+++++++++++   "display_name": "3.9.13",
+++++++++++   "language": "python",
+++++++++++   "name": "python3"
+++++++++++  },
+++++++++++  "language_info": {
+++++++++++   "codemirror_mode": {
+++++++++++    "name": "ipython",
+++++++++++    "version": 3
+++++++++++   },
+++++++++++   "file_extension": ".py",
+++++++++++   "mimetype": "text/x-python",
+++++++++++   "name": "python",
+++++++++++   "nbconvert_exporter": "python",
+++++++++++   "pygments_lexer": "ipython3",
+++++++++++   "version": "3.9.13"
+++++++++++  }
+++++++++++ },
+++++++++++ "nbformat": 4,
+++++++++++ "nbformat_minor": 2
+++++++++++}
++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++++index cb157af..c502b5d 100644
++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++++index 3877729..ac8dab6 100644
++++++++++--- a/utils/decode.py
+++++++++++++ b/utils/decode.py
++++++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++++++ import numpy as np
++++++++++ from itertools import groupby
++++++++++ import torch.nn.functional as F
+++++++++++import matplotlib.pyplot as plt
+++++++++ +
+++++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++++-+    sample_wers = []
+++++++++-+    for file_id in results:
+++++++++-+        gt = results[file_id]['gloss']
+++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++-+    
+++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++-+    
+++++++++-+        sample_wers.append({
+++++++++-+            'file_id': file_id,
+++++++++-+            'gt': gt,
+++++++++-+            'conv_pred': conv_pred,
+++++++++-+            'conv_wer': conv_wer,
+++++++++-+            'lstm_pred': lstm_pred,
+++++++++-+            'lstm_wer': lstm_wer,
+++++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++++-+        })
+++++++++++# ⬇ 프레임 길이 저장용 전역 리스트
+++++++++++frame_lengths = []
+++++++++ +
+++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++-+    
+++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++-+    for sample in top5:
+++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++-+        print(f"GT   : {sample['gt']}")
+++++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++++-+        print("-" * 60)
+++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++++    pred_ids = torch.argmax(probs, dim=-1)
+++++++++ +
+++++++++++    length = int(vid_lgt[batch_idx].item())
+++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++++    x = np.arange(length)
+++++++++ +
+++++++++++    plt.figure(figsize=(15, 4))
+++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++++++ +
+++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++++++    if len(blank_indices) > 0:
+++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++++++ +
+++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++++++++++    if sample_id:
+++++++++++        title_str += f"\nSample: {sample_id}"
+++++++++++    plt.title(title_str)
+++++++++++    plt.xlabel("Time Step")
+++++++++++    plt.ylabel("Gloss ID")
+++++++++++    plt.yticks(np.unique(pred_seq))
+++++++++++    plt.grid(True)
+++++++++++    plt.legend()
+++++++++++    plt.tight_layout()
+++++++++++    plt.show()
++++++++++ 
++++++++++ 
++++++++++ class Decode(object):
++++++++++@@ -16,35 +48,27 @@ class Decode(object):
++++++++++         self.search_mode = search_mode
++++++++++         self.blank_id = blank_id
++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++++++++++-                                                    num_processes=10)
+++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++++++++++ 
++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++++++++++         if not batch_first:
++++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++++ +
+++++++++++        # ⬇ 프레임 길이 수집
+++++++++++        global frame_lengths
+++++++++++        for i in range(vid_lgt.size(0)):
+++++++++++            frame_lengths.append(int(vid_lgt[i].item()))
+++++++++ +
+++++++++++        # sample_id가 존재하면 시각화
+++++++++++        sample_id = sample_ids[0] if sample_ids else None
+++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+++++++++ +
++++++++++         if self.search_mode == "max":
++++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++++         else:
++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
++++++++++ 
++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++++++++++-        '''
++++++++++-        CTCBeamDecoder Shape:
++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++++++++++-                          timesteps (B, N_beams)
++++++++++-                          out_lens (B, N_beams)
++++++++++-        '''
++++++++++-
++++++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
++++++++++-        batchsize, lgt = index_list.shape
++++++++++-        blank_rate =[]
++++++++++-        for batch_idx in range(batchsize):
++++++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++++++++++-            blank_rate.append(group_result)
++++++++++-
++++++++++-
++++++++++         if not probs:
++++++++++             nn_output = nn_output.softmax(-1).cpu()
++++++++++         vid_lgt = vid_lgt.cpu()
++++++++++@@ -54,9 +78,7 @@ class Decode(object):
++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++++++++++             if len(first_result) != 0:
++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++++++++++-                             enumerate(first_result)])
+++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++++++++++         return ret_list
++++++++++ 
++++++++++     def MaxDecode(self, nn_output, vid_lgt):
++++++++++@@ -65,12 +87,34 @@ class Decode(object):
++++++++++         ret_list = []
++++++++++         for batch_idx in range(batchsize):
++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++++++++++            filtered = [x for x in group_result if x != self.blank_id]
++++++++++             if len(filtered) > 0:
++++++++++                 max_result = torch.stack(filtered)
++++++++++                 max_result = [x[0] for x in groupby(max_result)]
++++++++++             else:
++++++++++                 max_result = filtered
++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++++++++++-                             enumerate(max_result)])
+++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
++++++++++         return ret_list
+++++++++ +
+++++++++ +
+++++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
+++++++++++def analyze_frame_lengths():
+++++++++++    if not frame_lengths:
+++++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
+++++++++++        return
+++++++++ +
+++++++++++    print("\n📊 Test Video Frame Length Analysis:")
+++++++++++    print(f"- Total samples: {len(frame_lengths)}")
+++++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
+++++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
+++++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
+++++++++ +
+++++++++++    # 히스토그램 시각화
+++++++++++    plt.figure(figsize=(10, 5))
+++++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
+++++++++++    plt.title("Test Video Frame Length Distribution")
+++++++++++    plt.xlabel("Frame Length")
+++++++++++    plt.ylabel("Number of Samples")
+++++++++++    plt.grid(True)
+++++++++++    plt.tight_layout()
+++++++++++    plt.show()
++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++++index 7d0a2aa..cd66258 100644
++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++++++++index c31483a..239691c 100644
++++++++++--- a/work_dirt/config.yaml
+++++++++++++ b/work_dirt/config.yaml
++++++++++@@ -7,7 +7,7 @@ dataset_info:
++++++++++   evaluation_dir: ./evaluation/slr_eval
++++++++++   evaluation_prefix: phoenix2014-groundtruth
++++++++++ decode_mode: beam
++++++++++-device: your_device
+++++++++++device: cuda
++++++++++ dist_url: env://
++++++++++ eval_interval: 1
++++++++++ evaluate_tool: python
++++++++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
++++++++++index c126e7a..199e2e9 100644
++++++++++--- a/work_dirt/dataloader_video.py
+++++++++++++ b/work_dirt/dataloader_video.py
++++++++++@@ -9,6 +9,7 @@ import torch
++++++++++ import random
++++++++++ import pandas
++++++++++ import warnings
+++++++++++import time
++++++++++ 
++++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++++++++ 
++++++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++++++++++ 
++++++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++++-
+++++++++++            print('phoenix 데이터를 사용함')
+++++++++++            print(img_folder)
++++++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++ #            print(img_folder)
++++++++++ #            img_list = sorted(glob.glob(img_folder))
++++++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++++++++++     def read_features(self, index):
++++++++++         # load file info
++++++++++         fi = self.inputs_list[index]
+++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++        time.sleep(10)
++++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++++++++         return data['features'], data['label']
++++++++++ 
++++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
++++++++++index 00c1a0e..3297908 100644
++++++++++--- a/work_dirt/dev.txt
+++++++++++++ b/work_dirt/dev.txt
++++++++++@@ -8,3 +8,15 @@
++++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++++++index 8a22ece..2328c40 100644
++++++++++--- a/work_dirt/dirty.patch
+++++++++++++ b/work_dirt/dirty.patch
++++++++++@@ -1,284 +1,10557 @@
++++++++++-diff --git a/README.md b/README.md
++++++++++-index bdbc17f..8cb240b 100644
++++++++++---- a/README.md
++++++++++-+++ b/README.md
++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++++- 
++++++++++- 
++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++++++- 
++++++++++- ### Training
++++++++++- 
++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++++++-index bfc1da8..25ffa61 100644
++++++++++---- a/configs/baseline.yaml
++++++++++-+++ b/configs/baseline.yaml
++++++++++-@@ -1,14 +1,14 @@
++++++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++++++- phase: train
++++++++++--dataset: phoenix2014-T
++++++++++-+dataset: phoenix2014
++++++++++- #CSL-Daily
++++++++++- # dataset: phoenix14-si5
++++++++++- 
++++++++++- work_dir: ./work_dirt/
++++++++++--batch_size: 4
++++++++++-+batch_size: 1
++++++++++- random_seed: 0 
++++++++++--test_batch_size: 4
++++++++++--num_worker: 20
++++++++++-+test_batch_size: 1
++++++++++-+num_worker: 3
++++++++++- device: 0
++++++++++- log_interval: 10000
++++++++++- eval_interval: 1
+++++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
+++++++++++index ffef81f..f4065cc 100644
+++++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
+++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++++++index 7ac0c3b..c89f220 100644
+++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
+++++++++++index 529dabc..f428bcc 100644
+++++++++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
++++++++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++++++-index 555f4b8..c126e7a 100644
+++++++++++index c126e7a..199e2e9 100644
++++++++++ --- a/dataset/dataloader_video.py
++++++++++ +++ b/dataset/dataloader_video.py
++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++++-+
++++++++++-         print(mode, len(self))
++++++++++-         self.data_aug = self.transform()
++++++++++-         print("")
++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
+++++++++++@@ -9,6 +9,7 @@ import torch
+++++++++++ import random
+++++++++++ import pandas
+++++++++++ import warnings
++++++++++++import time
++++++++++  
++++++++++-     def read_video(self, index):
++++++++++--        # load file info
++++++++++-         fi = self.inputs_list[index]
++++++++++-+    
++++++++++-         if 'phoenix' in self.dataset:
++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++++++-+#            print(img_list)
++++++++++-+
++++++++++-+#            print("[LOG] Using phoenix")
++++++++++-+
++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++++++++  
++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++-+#            print(img_folder)
++++++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++++++-+#            print(len(img_list))
++++++++++-         elif self.dataset == 'CSL-Daily':
++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++++++-+    
++++++++++-         img_list = sorted(glob.glob(img_folder))
++++++++++-+    
++++++++++-+        if len(img_list) == 0:
++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++++++-+    
++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++++++++-+    
++++++++++-         label_list = []
++++++++++--        if self.dataset=='phoenix2014':
++++++++++-+        if self.dataset == 'phoenix2014':
++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++++++--        if self.dataset=='phoenix2014-T':
++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++++++-+        elif self.dataset == 'phoenix2014-T':
++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++++++-+    
++++++++++-         for phase in fi['label'].split(" "):
++++++++++--            if phase == '':
++++++++++--                continue
++++++++++--            if phase in self.dict.keys():
++++++++++-+            if phase and phase in self.dict:
++++++++++-                 label_list.append(self.dict[phase][0])
++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++++++-+    
++++++++++-+        video = [
++++++++++-+            cv2.cvtColor(
++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++-+                cv2.COLOR_BGR2RGB
++++++++++-+            )   
++++++++++-+            for img_path in img_list
++++++++++-+        ]
++++++++++-+    
++++++++++-+        return video, label_list, fi
+++++++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
++++++++++  
+++++++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++-
++++++++++++            print('phoenix 데이터를 사용함')
++++++++++++            print(img_folder)
+++++++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++ #            print(img_folder)
+++++++++++ #            img_list = sorted(glob.glob(img_folder))
+++++++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
++++++++++      def read_features(self, index):
++++++++++          # load file info
+++++++++++         fi = self.inputs_list[index]
++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++        time.sleep(10)
+++++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++++++++         return data['features'], data['label']
+++++++++++ 
++++++++++ diff --git a/main.py b/main.py
++++++++++-index 9e68cee..18ac59b 100644
+++++++++++index 18ac59b..8f75cf5 100644
++++++++++ --- a/main.py
++++++++++ +++ b/main.py
++++++++++-@@ -256,7 +256,7 @@ class Processor():
++++++++++-                 batch_size=batch_size,
++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++-                 num_workers=self.arg.num_worker,
++++++++++--                pin_memory=True,
++++++++++-+                pin_memory=False,
++++++++++-                 worker_init_fn=self.init_fn,
++++++++++-             )
++++++++++-             return loader
++++++++++-@@ -268,7 +268,7 @@ class Processor():
++++++++++-                 drop_last=train_flag,
++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++--                pin_memory=True,
++++++++++-+                pin_memory=False,
++++++++++-                 worker_init_fn=self.init_fn,
++++++++++-             )
++++++++++- 
++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++-index 528856d..d8fcaf9 100644
++++++++++---- a/seq_scripts.py
++++++++++-+++ b/seq_scripts.py
++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++++++-     return
++++++++++- 
++++++++++- 
++++++++++-+import csv 
++++++++++-+from jiwer import wer as jiwer_wer
++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++++++-     model.eval()
++++++++++--    results=defaultdict(dict)
++++++++++-+    results = defaultdict(dict)
++++++++++- 
++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++++++-         recoder.record_timer("device")
++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++-                 results[inf]['conv_sents'] = conv_sents
++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++++++-                 results[inf]['gloss'] = gl
++++++++++-+
++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++-+
++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++++++--        reg_per = wer_results
++++++++++--    else:
++++++++++--        reg_per = wer_results_con
++++++++++-+
++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++++@@ -21,6 +21,7 @@ import utils
+++++++++++ from seq_scripts import seq_train, seq_eval
+++++++++++ from torch.cuda.amp import autocast as autocast
+++++++++++ from utils.misc import *
++++++++++++from utils.decode import analyze_frame_lengths
+++++++++++ class Processor():
+++++++++++     def __init__(self, arg):
+++++++++++         self.arg = arg
+++++++++++@@ -105,13 +106,25 @@ class Processor():
+++++++++++                 print('Please appoint --weights.')
+++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++ +
++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++++++-         f"{work_dir}/{mode}.txt")
++++++++++++            train_wer = seq_eval(
++++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++            )
++++++++++++            dev_wer = seq_eval(
++++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++            )
++++++++++++            test_wer = seq_eval(
++++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++            )
++++++++++ +
++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++++++-+        f"{work_dir}/{mode}.txt")
+++++++++++             self.recoder.print_log('Evaluation Done.\n')
++++++++++ +
++++++++++-+    # ✅ 전체 결과 CSV로 저장
++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++++++++            analyze_frame_lengths()
++++++++++ +
++++++++++-+    rows = []
++++++++++-+    for file_id in results:
++++++++++-+        gt = results[file_id]['gloss']
++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++-+
++++++++++-+        rows.append([
++++++++++-+            file_id,
++++++++++-+            gt,
++++++++++-+            conv_pred,
++++++++++-+            f"{conv_wer:.4f}",
++++++++++-+            lstm_pred,
++++++++++-+            f"{lstm_wer:.4f}"
++++++++++-+        ])
+++++++++++         elif self.arg.phase == "features":
+++++++++++             for mode in ["train", "dev", "test"]:
+++++++++++                 seq_feature_generation(
+++++++++++@@ -119,6 +132,8 @@ class Processor():
+++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++++++++                 )
+++++++++++ 
++++++++++ +
++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++++++-+        writer = csv.writer(f)
++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++++++-+        writer.writerows(rows)
++++++++++ +
++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++     def save_arg(self):
+++++++++++         arg_dict = vars(self.arg)
+++++++++++         if not os.path.exists(self.arg.work_dir):
+++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++++++index 71519fd..b9664e1 100644
+++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++++++diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++index d8fcaf9..77cfc71 100644
+++++++++++--- a/seq_scripts.py
++++++++++++++ b/seq_scripts.py
+++++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++         })
+++++++++++ 
+++++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++-    
++++++++++++    # 전체 샘플 WER 평균 계산
++++++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
++++++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
++++++++++ +
++++++++++++    print("\n📊 전체 평균 WER")
++++++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
++++++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
++++++++++ +
+++++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++     for sample in top5:
+++++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++++++index e69de29..0342039 100644
+++++++++++--- a/tmp.ipynb
++++++++++++++ b/tmp.ipynb
+++++++++++@@ -0,0 +1,272 @@
++++++++++++{
++++++++++++ "cells": [
++++++++++++  {
++++++++++++   "cell_type": "code",
++++++++++++   "execution_count": 2,
++++++++++++   "metadata": {},
++++++++++++   "outputs": [],
++++++++++++   "source": [
++++++++++++    "import torch\n",
++++++++++++    "\n",
++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++++++   ]
++++++++++++  },
++++++++++++  {
++++++++++++   "cell_type": "code",
++++++++++++   "execution_count": 3,
++++++++++++   "metadata": {},
++++++++++++   "outputs": [
++++++++++++    {
++++++++++++     "name": "stdout",
++++++++++++     "output_type": "stream",
++++++++++++     "text": [
++++++++++++      "Model is a state_dict.\n",
++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++++++     ]
++++++++++++    }
++++++++++++   ],
++++++++++++   "source": [
++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++++++   ]
++++++++++++  },
++++++++++++  {
++++++++++++   "cell_type": "code",
++++++++++++   "execution_count": 7,
++++++++++++   "metadata": {},
++++++++++++   "outputs": [],
++++++++++++   "source": [
++++++++++++    "import torch\n",
++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++++++    "import torch.nn.functional as F\n"
++++++++++++   ]
++++++++++++  },
++++++++++++  {
++++++++++++   "cell_type": "code",
++++++++++++   "execution_count": 1,
++++++++++++   "metadata": {},
++++++++++++   "outputs": [
++++++++++++    {
++++++++++++     "name": "stderr",
++++++++++++     "output_type": "stream",
++++++++++++     "text": [
++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++++++     ]
++++++++++++    }
++++++++++++   ],
++++++++++++   "source": [
++++++++++++    "import torch\n",
++++++++++++    "import torch.nn as nn\n",
++++++++++++    "import torch.nn.functional as F\n",
++++++++++++    "import torchvision.models as models\n",
++++++++++++    "import numpy as np\n",
++++++++++++    "import modules.resnet as resnet\n",
++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++++++    "from modules.criterions import SeqKD\n",
++++++++++++    "import utils\n",
++++++++++++    "import modules.resnet as resnet\n",
++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++++++    "class Identity(nn.Module):\n",
++++++++++++    "    def __init__(self):\n",
++++++++++++    "        super(Identity, self).__init__()\n",
++++++++++++    "\n",
++++++++++++    "    def forward(self, x):\n",
++++++++++++    "        return x\n",
++++++++++++    "\n",
++++++++++++    "# L2 정규화 선형 레이어\n",
++++++++++++    "class NormLinear(nn.Module):\n",
++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++++++    "        super(NormLinear, self).__init__()\n",
++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++++++    "\n",
++++++++++++    "    def forward(self, x):\n",
++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++++++    "        return outputs\n",
++++++++++++    "\n",
++++++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++++++    "class SLRModel(nn.Module):\n",
++++++++++++    "    def __init__(\n",
++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++++++    "    ):\n",
++++++++++++    "        super(SLRModel, self).__init__()\n",
++++++++++++    "        self.decoder = None\n",
++++++++++++    "        self.loss = dict()\n",
++++++++++++    "        self.criterion_init()\n",
++++++++++++    "        self.num_classes = num_classes\n",
++++++++++++    "        self.loss_weights = loss_weights\n",
++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++++++    "\n",
++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++++++    "                                   hidden_size=hidden_size,\n",
++++++++++++    "                                   conv_type=conv_type,\n",
++++++++++++    "                                   use_bn=use_bn,\n",
++++++++++++    "                                   num_classes=num_classes)\n",
++++++++++++    "\n",
++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++++++    "\n",
++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++++++    "\n",
++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++++++    "        if weight_norm:\n",
++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++    "        else:\n",
++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++    "\n",
++++++++++++    "        # Classifier 공유 여부\n",
++++++++++++    "        if share_classifier:\n",
++++++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++++++    "\n",
++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++++++    "        if len(x.shape) == 5:\n",
++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++++++    "        else:\n",
++++++++++++    "            framewise = x\n",
++++++++++++    "\n",
++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++++++    "\n",
++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++++++    "\n",
++++++++++++    "        # 최종 Classifier 적용\n",
++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++++++    "\n",
++++++++++++    "        # Inference 모드에서 Decoding\n",
++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++++++    "\n",
++++++++++++    "        return {\n",
++++++++++++    "            \"framewise_features\": framewise,\n",
++++++++++++    "            \"visual_features\": x,\n",
++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++++++    "            \"feat_len\": lgt,\n",
++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++++++    "            \"sequence_logits\": outputs,\n",
++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++++++    "            \"recognized_sents\": pred,\n",
++++++++++++    "        }\n",
++++++++++++    "\n",
++++++++++++    "    def criterion_init(self):\n",
++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++++++    "        return self.loss\n"
++++++++++++   ]
++++++++++++  },
++++++++++++  {
++++++++++++   "cell_type": "code",
++++++++++++   "execution_count": 6,
++++++++++++   "metadata": {},
++++++++++++   "outputs": [
++++++++++++    {
++++++++++++     "ename": "KeyError",
++++++++++++     "evalue": "'dataset_info'",
++++++++++++     "output_type": "error",
++++++++++++     "traceback": [
++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++++++     ]
++++++++++++    }
++++++++++++   ],
++++++++++++   "source": [
++++++++++++    "import os\n",
++++++++++++    "import numpy as np\n",
++++++++++++    "import yaml\n",
++++++++++++    "\n",
++++++++++++    "# 환경 변수 설정\n",
++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++++++    "\n",
++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++++++    "with open(config_path, \"r\") as f:\n",
++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++++++    "\n",
++++++++++++    "# ✅ gloss_dict 로드\n",
++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++++++    "\n",
++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++++++   ]
++++++++++++  },
++++++++++++  {
++++++++++++   "cell_type": "code",
++++++++++++   "execution_count": 5,
++++++++++++   "metadata": {},
++++++++++++   "outputs": [
++++++++++++    {
++++++++++++     "ename": "AttributeError",
++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++++++     "output_type": "error",
++++++++++++     "traceback": [
++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++++++     ]
++++++++++++    }
++++++++++++   ],
++++++++++++   "source": [
++++++++++++    "import torch\n",
++++++++++++    "\n",
++++++++++++    "# 모델 불러오기\n",
++++++++++++    "model = SLRModel(\n",
++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++++++    ")\n",
++++++++++++    "\n",
++++++++++++    "# 저장된 가중치 로드\n",
++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++++++    "\n",
++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++++++    "if isinstance(state_dict, dict):\n",
++++++++++++    "    model.load_state_dict(state_dict)\n",
++++++++++++    "\n",
++++++++++++    "# 모델을 평가 모드로 설정\n",
++++++++++++    "model.eval()\n"
++++++++++++   ]
++++++++++++  }
++++++++++++ ],
++++++++++++ "metadata": {
++++++++++++  "kernelspec": {
++++++++++++   "display_name": "3.9.13",
++++++++++++   "language": "python",
++++++++++++   "name": "python3"
++++++++++++  },
++++++++++++  "language_info": {
++++++++++++   "codemirror_mode": {
++++++++++++    "name": "ipython",
++++++++++++    "version": 3
++++++++++++   },
++++++++++++   "file_extension": ".py",
++++++++++++   "mimetype": "text/x-python",
++++++++++++   "name": "python",
++++++++++++   "nbconvert_exporter": "python",
++++++++++++   "pygments_lexer": "ipython3",
++++++++++++   "version": "3.9.13"
++++++++++++  }
++++++++++++ },
++++++++++++ "nbformat": 4,
++++++++++++ "nbformat_minor": 2
++++++++++++}
+++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++++++index cb157af..c502b5d 100644
+++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++++++index 3877729..ac8dab6 100644
+++++++++++--- a/utils/decode.py
++++++++++++++ b/utils/decode.py
+++++++++++@@ -6,6 +6,38 @@ import ctcdecode
+++++++++++ import numpy as np
+++++++++++ from itertools import groupby
+++++++++++ import torch.nn.functional as F
++++++++++++import matplotlib.pyplot as plt
++++++++++ +
++++++++++-+    # WER 기준 상위 5개 샘플 출력
++++++++++-+    sample_wers = []
++++++++++-+    for file_id in results:
++++++++++-+        gt = results[file_id]['gloss']
++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++-+    
++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++-+    
++++++++++-+        sample_wers.append({
++++++++++-+            'file_id': file_id,
++++++++++-+            'gt': gt,
++++++++++-+            'conv_pred': conv_pred,
++++++++++-+            'conv_wer': conv_wer,
++++++++++-+            'lstm_pred': lstm_pred,
++++++++++-+            'lstm_wer': lstm_wer,
++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++++++-+        })
++++++++++++# ⬇ 프레임 길이 저장용 전역 리스트
++++++++++++frame_lengths = []
++++++++++ +
++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++-+    
++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++-+    for sample in top5:
++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++-+        print(f"GT   : {sample['gt']}")
++++++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++++++-+        print("-" * 60)
++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
++++++++++ +
++++++++++++    length = int(vid_lgt[batch_idx].item())
++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++++++    x = np.arange(length)
++++++++++ +
++++++++++++    plt.figure(figsize=(15, 4))
++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++++++++ +
++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++++++++    if len(blank_indices) > 0:
++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++++++++ +
++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++++++++++    if sample_id:
++++++++++++        title_str += f"\nSample: {sample_id}"
++++++++++++    plt.title(title_str)
++++++++++++    plt.xlabel("Time Step")
++++++++++++    plt.ylabel("Gloss ID")
++++++++++++    plt.yticks(np.unique(pred_seq))
++++++++++++    plt.grid(True)
++++++++++++    plt.legend()
++++++++++++    plt.tight_layout()
++++++++++++    plt.show()
+++++++++++ 
+++++++++++ 
+++++++++++ class Decode(object):
+++++++++++@@ -16,35 +48,27 @@ class Decode(object):
+++++++++++         self.search_mode = search_mode
+++++++++++         self.blank_id = blank_id
+++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+++++++++++-                                                    num_processes=10)
++++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+++++++++++ 
+++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+++++++++++         if not batch_first:
+++++++++++             nn_output = nn_output.permute(1, 0, 2)
++++++++++ +
++++++++++++        # ⬇ 프레임 길이 수집
++++++++++++        global frame_lengths
++++++++++++        for i in range(vid_lgt.size(0)):
++++++++++++            frame_lengths.append(int(vid_lgt[i].item()))
++++++++++ +
++++++++++++        # sample_id가 존재하면 시각화
++++++++++++        sample_id = sample_ids[0] if sample_ids else None
++++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
++++++++++ +
+++++++++++         if self.search_mode == "max":
+++++++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++++++         else:
+++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
+++++++++++ 
+++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+++++++++++-        '''
+++++++++++-        CTCBeamDecoder Shape:
+++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+++++++++++-                          timesteps (B, N_beams)
+++++++++++-                          out_lens (B, N_beams)
+++++++++++-        '''
+++++++++++-
+++++++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
+++++++++++-        batchsize, lgt = index_list.shape
+++++++++++-        blank_rate =[]
+++++++++++-        for batch_idx in range(batchsize):
+++++++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+++++++++++-            blank_rate.append(group_result)
+++++++++++-
+++++++++++-
+++++++++++         if not probs:
+++++++++++             nn_output = nn_output.softmax(-1).cpu()
+++++++++++         vid_lgt = vid_lgt.cpu()
+++++++++++@@ -54,9 +78,7 @@ class Decode(object):
+++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+++++++++++             if len(first_result) != 0:
+++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+++++++++++-                             enumerate(first_result)])
++++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+++++++++++         return ret_list
+++++++++++ 
+++++++++++     def MaxDecode(self, nn_output, vid_lgt):
+++++++++++@@ -65,12 +87,34 @@ class Decode(object):
+++++++++++         ret_list = []
+++++++++++         for batch_idx in range(batchsize):
+++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++++++++++++            filtered = [x for x in group_result if x != self.blank_id]
+++++++++++             if len(filtered) > 0:
+++++++++++                 max_result = torch.stack(filtered)
+++++++++++                 max_result = [x[0] for x in groupby(max_result)]
+++++++++++             else:
+++++++++++                 max_result = filtered
+++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+++++++++++-                             enumerate(max_result)])
++++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++++++++++         return ret_list
++++++++++ +
++++++++++ +
++++++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
++++++++++++def analyze_frame_lengths():
++++++++++++    if not frame_lengths:
++++++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
++++++++++++        return
++++++++++ +
++++++++++++    print("\n📊 Test Video Frame Length Analysis:")
++++++++++++    print(f"- Total samples: {len(frame_lengths)}")
++++++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
++++++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
++++++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
++++++++++ +
++++++++++++    # 히스토그램 시각화
++++++++++++    plt.figure(figsize=(10, 5))
++++++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
++++++++++++    plt.title("Test Video Frame Length Distribution")
++++++++++++    plt.xlabel("Frame Length")
++++++++++++    plt.ylabel("Number of Samples")
++++++++++++    plt.grid(True)
++++++++++++    plt.tight_layout()
++++++++++++    plt.show()
+++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++++++index 7d0a2aa..cd66258 100644
+++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++++++++++index c31483a..239691c 100644
+++++++++++--- a/work_dirt/config.yaml
++++++++++++++ b/work_dirt/config.yaml
+++++++++++@@ -7,7 +7,7 @@ dataset_info:
+++++++++++   evaluation_dir: ./evaluation/slr_eval
+++++++++++   evaluation_prefix: phoenix2014-groundtruth
+++++++++++ decode_mode: beam
+++++++++++-device: your_device
++++++++++++device: cuda
+++++++++++ dist_url: env://
+++++++++++ eval_interval: 1
+++++++++++ evaluate_tool: python
+++++++++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
+++++++++++index c126e7a..199e2e9 100644
+++++++++++--- a/work_dirt/dataloader_video.py
++++++++++++++ b/work_dirt/dataloader_video.py
+++++++++++@@ -9,6 +9,7 @@ import torch
+++++++++++ import random
+++++++++++ import pandas
+++++++++++ import warnings
++++++++++++import time
+++++++++++ 
+++++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++++++++++ 
+++++++++++@@ -74,7 +75,8 @@ class BaseFeeder(data.Dataset):
+++++++++++ 
+++++++++++             self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++             img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++-
++++++++++++            print('phoenix 데이터를 사용함')
++++++++++++            print(img_folder)
+++++++++++ #            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++ #            print(img_folder)
+++++++++++ #            img_list = sorted(glob.glob(img_folder))
+++++++++++@@ -113,6 +115,12 @@ class BaseFeeder(data.Dataset):
+++++++++++     def read_features(self, index):
+++++++++++         # load file info
+++++++++++         fi = self.inputs_list[index]
++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++        time.sleep(10)
+++++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++++++++         return data['features'], data['label']
+++++++++++ 
+++++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
+++++++++++index 00c1a0e..3297908 100644
+++++++++++--- a/work_dirt/dev.txt
++++++++++++++ b/work_dirt/dev.txt
+++++++++++@@ -8,3 +8,15 @@
+++++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
+++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++[ Wed Apr  2 16:52:25 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++++++index 8a22ece..d8a76ef 100644
+++++++++++--- a/work_dirt/dirty.patch
++++++++++++++ b/work_dirt/dirty.patch
+++++++++++@@ -1,284 +1,9549 @@
+++++++++++-diff --git a/README.md b/README.md
+++++++++++-index bdbc17f..8cb240b 100644
+++++++++++---- a/README.md
+++++++++++-+++ b/README.md
+++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++++++++- 
+++++++++++- 
+++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++++++- 
+++++++++++- ### Training
+++++++++++- 
+++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++++++-index bfc1da8..25ffa61 100644
+++++++++++---- a/configs/baseline.yaml
+++++++++++-+++ b/configs/baseline.yaml
+++++++++++-@@ -1,14 +1,14 @@
+++++++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++++++- phase: train
+++++++++++--dataset: phoenix2014-T
+++++++++++-+dataset: phoenix2014
+++++++++++- #CSL-Daily
+++++++++++- # dataset: phoenix14-si5
+++++++++++- 
+++++++++++- work_dir: ./work_dirt/
+++++++++++--batch_size: 4
+++++++++++-+batch_size: 1
+++++++++++- random_seed: 0 
+++++++++++--test_batch_size: 4
+++++++++++--num_worker: 20
+++++++++++-+test_batch_size: 1
+++++++++++-+num_worker: 3
+++++++++++- device: 0
+++++++++++- log_interval: 10000
+++++++++++- eval_interval: 1
++++++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
++++++++++++index ffef81f..f4065cc 100644
++++++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++++++index 7ac0c3b..c89f220 100644
++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++++++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
++++++++++++index 529dabc..8e107c3 100644
++++++++++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
+++++++++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++++++-index 555f4b8..c126e7a 100644
++++++++++++index c126e7a..2b79570 100644
+++++++++++ --- a/dataset/dataloader_video.py
+++++++++++ +++ b/dataset/dataloader_video.py
+++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++++++++-+
+++++++++++-         print(mode, len(self))
+++++++++++-         self.data_aug = self.transform()
+++++++++++-         print("")
+++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++@@ -9,6 +9,7 @@ import torch
++++++++++++ import random
++++++++++++ import pandas
++++++++++++ import warnings
+++++++++++++import time
+++++++++++  
+++++++++++-     def read_video(self, index):
+++++++++++--        # load file info
+++++++++++-         fi = self.inputs_list[index]
+++++++++++-+    
+++++++++++-         if 'phoenix' in self.dataset:
+++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++++++-+#            print(img_list)
+++++++++++-+
+++++++++++-+#            print("[LOG] Using phoenix")
+++++++++++-+
+++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++- 
+++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++-+#            print(img_folder)
+++++++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++++++-+#            print(len(img_list))
+++++++++++-         elif self.dataset == 'CSL-Daily':
+++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++++++-+    
+++++++++++-         img_list = sorted(glob.glob(img_folder))
+++++++++++-+    
+++++++++++-+        if len(img_list) == 0:
+++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++++++-+    
+++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++++-+    
+++++++++++-         label_list = []
+++++++++++--        if self.dataset=='phoenix2014':
+++++++++++-+        if self.dataset == 'phoenix2014':
+++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++++++--        if self.dataset=='phoenix2014-T':
+++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++++++-+    
+++++++++++-         for phase in fi['label'].split(" "):
+++++++++++--            if phase == '':
+++++++++++--                continue
+++++++++++--            if phase in self.dict.keys():
+++++++++++-+            if phase and phase in self.dict:
+++++++++++-                 label_list.append(self.dict[phase][0])
+++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++++++-+    
+++++++++++-+        video = [
+++++++++++-+            cv2.cvtColor(
+++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++-+                cv2.COLOR_BGR2RGB
+++++++++++-+            )   
+++++++++++-+            for img_path in img_list
+++++++++++-+        ]
+++++++++++-+    
+++++++++++-+        return video, label_list, fi
++++++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
+++++++++++  
++++++++++++@@ -113,6 +114,12 @@ class BaseFeeder(data.Dataset):
+++++++++++      def read_features(self, index):
+++++++++++          # load file info
++++++++++++         fi = self.inputs_list[index]
+++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++        time.sleep(10)
++++++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++++++++++         return data['features'], data['label']
++++++++++++ 
+++++++++++ diff --git a/main.py b/main.py
+++++++++++-index 9e68cee..18ac59b 100644
++++++++++++index 18ac59b..8f75cf5 100644
+++++++++++ --- a/main.py
+++++++++++ +++ b/main.py
+++++++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++++++-                 batch_size=batch_size,
+++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++-                 num_workers=self.arg.num_worker,
+++++++++++--                pin_memory=True,
+++++++++++-+                pin_memory=False,
+++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++-             )
+++++++++++-             return loader
+++++++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++++++-                 drop_last=train_flag,
+++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++--                pin_memory=True,
+++++++++++-+                pin_memory=False,
+++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++-             )
+++++++++++- 
+++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++-index 528856d..d8fcaf9 100644
+++++++++++---- a/seq_scripts.py
+++++++++++-+++ b/seq_scripts.py
+++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++++++-     return
+++++++++++- 
+++++++++++- 
+++++++++++-+import csv 
+++++++++++-+from jiwer import wer as jiwer_wer
+++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++++++-     model.eval()
+++++++++++--    results=defaultdict(dict)
+++++++++++-+    results = defaultdict(dict)
+++++++++++- 
+++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++++++-         recoder.record_timer("device")
+++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++++++-                 results[inf]['gloss'] = gl
+++++++++++-+
+++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++-+
+++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++++++--        reg_per = wer_results
+++++++++++--    else:
+++++++++++--        reg_per = wer_results_con
+++++++++++-+
+++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++++++@@ -21,6 +21,7 @@ import utils
++++++++++++ from seq_scripts import seq_train, seq_eval
++++++++++++ from torch.cuda.amp import autocast as autocast
++++++++++++ from utils.misc import *
+++++++++++++from utils.decode import analyze_frame_lengths
++++++++++++ class Processor():
++++++++++++     def __init__(self, arg):
++++++++++++         self.arg = arg
++++++++++++@@ -105,13 +106,25 @@ class Processor():
++++++++++++                 print('Please appoint --weights.')
++++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++ +
+++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++++++-         f"{work_dir}/{mode}.txt")
+++++++++++++            train_wer = seq_eval(
+++++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++            )
+++++++++++++            dev_wer = seq_eval(
+++++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++            )
+++++++++++++            test_wer = seq_eval(
+++++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++            )
+++++++++++ +
+++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++++++-+        f"{work_dir}/{mode}.txt")
++++++++++++             self.recoder.print_log('Evaluation Done.\n')
+++++++++++ +
+++++++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++++++++            analyze_frame_lengths()
+++++++++++ +
+++++++++++-+    rows = []
+++++++++++-+    for file_id in results:
+++++++++++-+        gt = results[file_id]['gloss']
+++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++-+
+++++++++++-+        rows.append([
+++++++++++-+            file_id,
+++++++++++-+            gt,
+++++++++++-+            conv_pred,
+++++++++++-+            f"{conv_wer:.4f}",
+++++++++++-+            lstm_pred,
+++++++++++-+            f"{lstm_wer:.4f}"
+++++++++++-+        ])
+++++++++++-+
+++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++++++-+        writer = csv.writer(f)
+++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++++++-+        writer.writerows(rows)
++++++++++++         elif self.arg.phase == "features":
++++++++++++             for mode in ["train", "dev", "test"]:
++++++++++++                 seq_feature_generation(
++++++++++++@@ -119,6 +132,8 @@ class Processor():
++++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++++++++                 )
++++++++++++ 
+++++++++++ +
+++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++ +
++++++++++++     def save_arg(self):
++++++++++++         arg_dict = vars(self.arg)
++++++++++++         if not os.path.exists(self.arg.work_dir):
++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++++++index 71519fd..b9664e1 100644
++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++++++diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++++index d8fcaf9..77cfc71 100644
++++++++++++--- a/seq_scripts.py
+++++++++++++++ b/seq_scripts.py
++++++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++         })
++++++++++++ 
++++++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++-    
+++++++++++++    # 전체 샘플 WER 평균 계산
+++++++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+++++++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+++++++++++ +
+++++++++++++    print("\n📊 전체 평균 WER")
+++++++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+++++++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+++++++++++ +
+++++++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++++++-+    sample_wers = []
+++++++++++-+    for file_id in results:
+++++++++++-+        gt = results[file_id]['gloss']
+++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++-+    
+++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++-+    
+++++++++++-+        sample_wers.append({
+++++++++++-+            'file_id': file_id,
+++++++++++-+            'gt': gt,
+++++++++++-+            'conv_pred': conv_pred,
+++++++++++-+            'conv_wer': conv_wer,
+++++++++++-+            'lstm_pred': lstm_pred,
+++++++++++-+            'lstm_wer': lstm_wer,
+++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++++++-+        })
++++++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++     for sample in top5:
++++++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++++++index e69de29..0342039 100644
++++++++++++--- a/tmp.ipynb
+++++++++++++++ b/tmp.ipynb
++++++++++++@@ -0,0 +1,272 @@
+++++++++++++{
+++++++++++++ "cells": [
+++++++++++++  {
+++++++++++++   "cell_type": "code",
+++++++++++++   "execution_count": 2,
+++++++++++++   "metadata": {},
+++++++++++++   "outputs": [],
+++++++++++++   "source": [
+++++++++++++    "import torch\n",
+++++++++++++    "\n",
+++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++++++   ]
+++++++++++++  },
+++++++++++++  {
+++++++++++++   "cell_type": "code",
+++++++++++++   "execution_count": 3,
+++++++++++++   "metadata": {},
+++++++++++++   "outputs": [
+++++++++++++    {
+++++++++++++     "name": "stdout",
+++++++++++++     "output_type": "stream",
+++++++++++++     "text": [
+++++++++++++      "Model is a state_dict.\n",
+++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++++++     ]
+++++++++++++    }
+++++++++++++   ],
+++++++++++++   "source": [
+++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++++++   ]
+++++++++++++  },
+++++++++++++  {
+++++++++++++   "cell_type": "code",
+++++++++++++   "execution_count": 7,
+++++++++++++   "metadata": {},
+++++++++++++   "outputs": [],
+++++++++++++   "source": [
+++++++++++++    "import torch\n",
+++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++++++    "import torch.nn.functional as F\n"
+++++++++++++   ]
+++++++++++++  },
+++++++++++++  {
+++++++++++++   "cell_type": "code",
+++++++++++++   "execution_count": 1,
+++++++++++++   "metadata": {},
+++++++++++++   "outputs": [
+++++++++++++    {
+++++++++++++     "name": "stderr",
+++++++++++++     "output_type": "stream",
+++++++++++++     "text": [
+++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++++++     ]
+++++++++++++    }
+++++++++++++   ],
+++++++++++++   "source": [
+++++++++++++    "import torch\n",
+++++++++++++    "import torch.nn as nn\n",
+++++++++++++    "import torch.nn.functional as F\n",
+++++++++++++    "import torchvision.models as models\n",
+++++++++++++    "import numpy as np\n",
+++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++++++    "from modules.criterions import SeqKD\n",
+++++++++++++    "import utils\n",
+++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++++++    "class Identity(nn.Module):\n",
+++++++++++++    "    def __init__(self):\n",
+++++++++++++    "        super(Identity, self).__init__()\n",
+++++++++++++    "\n",
+++++++++++++    "    def forward(self, x):\n",
+++++++++++++    "        return x\n",
+++++++++++++    "\n",
+++++++++++++    "# L2 정규화 선형 레이어\n",
+++++++++++++    "class NormLinear(nn.Module):\n",
+++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++++++    "\n",
+++++++++++++    "    def forward(self, x):\n",
+++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++++++    "        return outputs\n",
+++++++++++++    "\n",
+++++++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++++++    "class SLRModel(nn.Module):\n",
+++++++++++++    "    def __init__(\n",
+++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++++++    "    ):\n",
+++++++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++++++    "        self.decoder = None\n",
+++++++++++++    "        self.loss = dict()\n",
+++++++++++++    "        self.criterion_init()\n",
+++++++++++++    "        self.num_classes = num_classes\n",
+++++++++++++    "        self.loss_weights = loss_weights\n",
+++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++++++    "\n",
+++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++++++    "                                   conv_type=conv_type,\n",
+++++++++++++    "                                   use_bn=use_bn,\n",
+++++++++++++    "                                   num_classes=num_classes)\n",
+++++++++++++    "\n",
+++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++++++    "\n",
+++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++++++    "\n",
+++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++++++    "        if weight_norm:\n",
+++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++    "        else:\n",
+++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++    "\n",
+++++++++++++    "        # Classifier 공유 여부\n",
+++++++++++++    "        if share_classifier:\n",
+++++++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++++++    "\n",
+++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++++++    "        if len(x.shape) == 5:\n",
+++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++++++    "        else:\n",
+++++++++++++    "            framewise = x\n",
+++++++++++++    "\n",
+++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++++++    "\n",
+++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++++++    "\n",
+++++++++++++    "        # 최종 Classifier 적용\n",
+++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++++++    "\n",
+++++++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++++++    "\n",
+++++++++++++    "        return {\n",
+++++++++++++    "            \"framewise_features\": framewise,\n",
+++++++++++++    "            \"visual_features\": x,\n",
+++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++++++    "            \"feat_len\": lgt,\n",
+++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++++++    "            \"recognized_sents\": pred,\n",
+++++++++++++    "        }\n",
+++++++++++++    "\n",
+++++++++++++    "    def criterion_init(self):\n",
+++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++++++    "        return self.loss\n"
+++++++++++++   ]
+++++++++++++  },
+++++++++++++  {
+++++++++++++   "cell_type": "code",
+++++++++++++   "execution_count": 6,
+++++++++++++   "metadata": {},
+++++++++++++   "outputs": [
+++++++++++++    {
+++++++++++++     "ename": "KeyError",
+++++++++++++     "evalue": "'dataset_info'",
+++++++++++++     "output_type": "error",
+++++++++++++     "traceback": [
+++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++++++     ]
+++++++++++++    }
+++++++++++++   ],
+++++++++++++   "source": [
+++++++++++++    "import os\n",
+++++++++++++    "import numpy as np\n",
+++++++++++++    "import yaml\n",
+++++++++++++    "\n",
+++++++++++++    "# 환경 변수 설정\n",
+++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++++++    "\n",
+++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++++++    "\n",
+++++++++++++    "# ✅ gloss_dict 로드\n",
+++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++++++    "\n",
+++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++++++   ]
+++++++++++++  },
+++++++++++++  {
+++++++++++++   "cell_type": "code",
+++++++++++++   "execution_count": 5,
+++++++++++++   "metadata": {},
+++++++++++++   "outputs": [
+++++++++++++    {
+++++++++++++     "ename": "AttributeError",
+++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++++++     "output_type": "error",
+++++++++++++     "traceback": [
+++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++++++     ]
+++++++++++++    }
+++++++++++++   ],
+++++++++++++   "source": [
+++++++++++++    "import torch\n",
+++++++++++++    "\n",
+++++++++++++    "# 모델 불러오기\n",
+++++++++++++    "model = SLRModel(\n",
+++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++++++    ")\n",
+++++++++++++    "\n",
+++++++++++++    "# 저장된 가중치 로드\n",
+++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++++++    "\n",
+++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++++++    "if isinstance(state_dict, dict):\n",
+++++++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++++++    "\n",
+++++++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++++++    "model.eval()\n"
+++++++++++++   ]
+++++++++++++  }
+++++++++++++ ],
+++++++++++++ "metadata": {
+++++++++++++  "kernelspec": {
+++++++++++++   "display_name": "3.9.13",
+++++++++++++   "language": "python",
+++++++++++++   "name": "python3"
+++++++++++++  },
+++++++++++++  "language_info": {
+++++++++++++   "codemirror_mode": {
+++++++++++++    "name": "ipython",
+++++++++++++    "version": 3
+++++++++++++   },
+++++++++++++   "file_extension": ".py",
+++++++++++++   "mimetype": "text/x-python",
+++++++++++++   "name": "python",
+++++++++++++   "nbconvert_exporter": "python",
+++++++++++++   "pygments_lexer": "ipython3",
+++++++++++++   "version": "3.9.13"
+++++++++++++  }
+++++++++++++ },
+++++++++++++ "nbformat": 4,
+++++++++++++ "nbformat_minor": 2
+++++++++++++}
++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++++++index cb157af..c502b5d 100644
++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++++++index 3877729..ac8dab6 100644
++++++++++++--- a/utils/decode.py
+++++++++++++++ b/utils/decode.py
++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++++++++ import numpy as np
++++++++++++ from itertools import groupby
++++++++++++ import torch.nn.functional as F
+++++++++++++import matplotlib.pyplot as plt
+++++++++++ +
+++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++-+    
+++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++-+    for sample in top5:
+++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++-+        print(f"GT   : {sample['gt']}")
+++++++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++++++-+        print("-" * 60)
+++++++++++++# ⬇ 프레임 길이 저장용 전역 리스트
+++++++++++++frame_lengths = []
+++++++++++ +
+++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
+++++++++++ +
+++++++++++++    length = int(vid_lgt[batch_idx].item())
+++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++++++    x = np.arange(length)
+++++++++++ +
+++++++++++++    plt.figure(figsize=(15, 4))
+++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++++++++ +
+++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++++++++    if len(blank_indices) > 0:
+++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++++++++ +
+++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++++++++++++    if sample_id:
+++++++++++++        title_str += f"\nSample: {sample_id}"
+++++++++++++    plt.title(title_str)
+++++++++++++    plt.xlabel("Time Step")
+++++++++++++    plt.ylabel("Gloss ID")
+++++++++++++    plt.yticks(np.unique(pred_seq))
+++++++++++++    plt.grid(True)
+++++++++++++    plt.legend()
+++++++++++++    plt.tight_layout()
+++++++++++++    plt.show()
++++++++++++ 
++++++++++++ 
++++++++++++ class Decode(object):
++++++++++++@@ -16,35 +48,27 @@ class Decode(object):
++++++++++++         self.search_mode = search_mode
++++++++++++         self.blank_id = blank_id
++++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++++++++++++-                                                    num_processes=10)
+++++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++++++++++++ 
++++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++++++++++++         if not batch_first:
++++++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++++++ +
+++++++++++++        # ⬇ 프레임 길이 수집
+++++++++++++        global frame_lengths
+++++++++++++        for i in range(vid_lgt.size(0)):
+++++++++++++            frame_lengths.append(int(vid_lgt[i].item()))
+++++++++++ +
+++++++++++++        # sample_id가 존재하면 시각화
+++++++++++++        sample_id = sample_ids[0] if sample_ids else None
+++++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+++++++++++ +
++++++++++++         if self.search_mode == "max":
++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++++++         else:
++++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
++++++++++++ 
++++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++++++++++++-        '''
++++++++++++-        CTCBeamDecoder Shape:
++++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++++++++++++-                          timesteps (B, N_beams)
++++++++++++-                          out_lens (B, N_beams)
++++++++++++-        '''
++++++++++++-
++++++++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
++++++++++++-        batchsize, lgt = index_list.shape
++++++++++++-        blank_rate =[]
++++++++++++-        for batch_idx in range(batchsize):
++++++++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++++++++++++-            blank_rate.append(group_result)
++++++++++++-
++++++++++++-
++++++++++++         if not probs:
++++++++++++             nn_output = nn_output.softmax(-1).cpu()
++++++++++++         vid_lgt = vid_lgt.cpu()
++++++++++++@@ -54,9 +78,7 @@ class Decode(object):
++++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++++++++++++             if len(first_result) != 0:
++++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++++++++++++-                             enumerate(first_result)])
+++++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++++++++++++         return ret_list
++++++++++++ 
++++++++++++     def MaxDecode(self, nn_output, vid_lgt):
++++++++++++@@ -65,12 +87,34 @@ class Decode(object):
++++++++++++         ret_list = []
++++++++++++         for batch_idx in range(batchsize):
++++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++++++++++++            filtered = [x for x in group_result if x != self.blank_id]
++++++++++++             if len(filtered) > 0:
++++++++++++                 max_result = torch.stack(filtered)
++++++++++++                 max_result = [x[0] for x in groupby(max_result)]
++++++++++++             else:
++++++++++++                 max_result = filtered
++++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++++++++++++-                             enumerate(max_result)])
+++++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
++++++++++++         return ret_list
+++++++++++ +
+++++++++++ +
+++++++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
+++++++++++++def analyze_frame_lengths():
+++++++++++++    if not frame_lengths:
+++++++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
+++++++++++++        return
+++++++++++ +
+++++++++++++    print("\n📊 Test Video Frame Length Analysis:")
+++++++++++++    print(f"- Total samples: {len(frame_lengths)}")
+++++++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
+++++++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
+++++++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
+++++++++++ +
+++++++++++++    # 히스토그램 시각화
+++++++++++++    plt.figure(figsize=(10, 5))
+++++++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
+++++++++++++    plt.title("Test Video Frame Length Distribution")
+++++++++++++    plt.xlabel("Frame Length")
+++++++++++++    plt.ylabel("Number of Samples")
+++++++++++++    plt.grid(True)
+++++++++++++    plt.tight_layout()
+++++++++++++    plt.show()
++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++++++index 7d0a2aa..cd66258 100644
++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++++++++++index c31483a..239691c 100644
++++++++++++--- a/work_dirt/config.yaml
+++++++++++++++ b/work_dirt/config.yaml
++++++++++++@@ -7,7 +7,7 @@ dataset_info:
++++++++++++   evaluation_dir: ./evaluation/slr_eval
++++++++++++   evaluation_prefix: phoenix2014-groundtruth
++++++++++++ decode_mode: beam
++++++++++++-device: your_device
+++++++++++++device: cuda
++++++++++++ dist_url: env://
++++++++++++ eval_interval: 1
++++++++++++ evaluate_tool: python
++++++++++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
++++++++++++index c126e7a..2b79570 100644
++++++++++++--- a/work_dirt/dataloader_video.py
+++++++++++++++ b/work_dirt/dataloader_video.py
++++++++++++@@ -9,6 +9,7 @@ import torch
++++++++++++ import random
++++++++++++ import pandas
++++++++++++ import warnings
+++++++++++++import time
++++++++++++ 
++++++++++++ warnings.simplefilter(action='ignore', category=FutureWarning)
++++++++++++ 
++++++++++++@@ -113,6 +114,12 @@ class BaseFeeder(data.Dataset):
++++++++++++     def read_features(self, index):
++++++++++++         # load file info
++++++++++++         fi = self.inputs_list[index]
+++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
+++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++        time.sleep(10)
++++++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
++++++++++++         return data['features'], data['label']
++++++++++++ 
++++++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
++++++++++++index 00c1a0e..2595445 100644
++++++++++++--- a/work_dirt/dev.txt
+++++++++++++++ b/work_dirt/dev.txt
++++++++++++@@ -8,3 +8,13 @@
++++++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++[ Wed Apr  2 16:50:02 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++++++++index 8a22ece..f11db01 100644
++++++++++++--- a/work_dirt/dirty.patch
+++++++++++++++ b/work_dirt/dirty.patch
++++++++++++@@ -1,284 +1,8568 @@
++++++++++++-diff --git a/README.md b/README.md
++++++++++++-index bdbc17f..8cb240b 100644
++++++++++++---- a/README.md
++++++++++++-+++ b/README.md
++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++++++- 
++++++++++++- 
++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++++++++- 
++++++++++++- ### Training
++++++++++++- 
++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++++++++-index bfc1da8..25ffa61 100644
++++++++++++---- a/configs/baseline.yaml
++++++++++++-+++ b/configs/baseline.yaml
++++++++++++-@@ -1,14 +1,14 @@
++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++++++++- phase: train
++++++++++++--dataset: phoenix2014-T
++++++++++++-+dataset: phoenix2014
++++++++++++- #CSL-Daily
++++++++++++- # dataset: phoenix14-si5
++++++++++++- 
++++++++++++- work_dir: ./work_dirt/
++++++++++++--batch_size: 4
++++++++++++-+batch_size: 1
++++++++++++- random_seed: 0 
++++++++++++--test_batch_size: 4
++++++++++++--num_worker: 20
++++++++++++-+test_batch_size: 1
++++++++++++-+num_worker: 3
++++++++++++- device: 0
++++++++++++- log_interval: 10000
++++++++++++- eval_interval: 1
+++++++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
+++++++++++++index ffef81f..f4065cc 100644
+++++++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
+++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++++++++index 7ac0c3b..c89f220 100644
+++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++++++diff --git a/dataset/__pycache__/dataloader_video.cpython-39.pyc b/dataset/__pycache__/dataloader_video.cpython-39.pyc
+++++++++++++index 529dabc..7c8bcf7 100644
+++++++++++++Binary files a/dataset/__pycache__/dataloader_video.cpython-39.pyc and b/dataset/__pycache__/dataloader_video.cpython-39.pyc differ
++++++++++++ diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++++++++-index 555f4b8..c126e7a 100644
+++++++++++++index c126e7a..342e7f8 100644
++++++++++++ --- a/dataset/dataloader_video.py
++++++++++++ +++ b/dataset/dataloader_video.py
++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++++++-+
++++++++++++-         print(mode, len(self))
++++++++++++-         self.data_aug = self.transform()
++++++++++++-         print("")
++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++- 
++++++++++++-     def read_video(self, index):
++++++++++++--        # load file info
++++++++++++-         fi = self.inputs_list[index]
++++++++++++-+    
++++++++++++-         if 'phoenix' in self.dataset:
++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++++++++-+#            print(img_list)
++++++++++++-+
++++++++++++-+#            print("[LOG] Using phoenix")
++++++++++++-+
++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++++++- 
++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++++-+#            print(img_folder)
++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++++++++-+#            print(len(img_list))
++++++++++++-         elif self.dataset == 'CSL-Daily':
++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++++++++-+    
++++++++++++-         img_list = sorted(glob.glob(img_folder))
++++++++++++-+    
++++++++++++-+        if len(img_list) == 0:
++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++++++++-+    
++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++++++++++-+    
++++++++++++-         label_list = []
++++++++++++--        if self.dataset=='phoenix2014':
++++++++++++-+        if self.dataset == 'phoenix2014':
++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++++++++--        if self.dataset=='phoenix2014-T':
++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++++++++-+        elif self.dataset == 'phoenix2014-T':
++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++++++++-+    
++++++++++++-         for phase in fi['label'].split(" "):
++++++++++++--            if phase == '':
++++++++++++--                continue
++++++++++++--            if phase in self.dict.keys():
++++++++++++-+            if phase and phase in self.dict:
++++++++++++-                 label_list.append(self.dict[phase][0])
++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++++++++-+    
++++++++++++-+        video = [
++++++++++++-+            cv2.cvtColor(
++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++-+                cv2.COLOR_BGR2RGB
++++++++++++-+            )   
++++++++++++-+            for img_path in img_list
++++++++++++-+        ]
++++++++++++-+    
++++++++++++-+        return video, label_list, fi
++++++++++++- 
+++++++++++++@@ -113,6 +113,11 @@ class BaseFeeder(data.Dataset):
++++++++++++      def read_features(self, index):
++++++++++++          # load file info
+++++++++++++         fi = self.inputs_list[index]
++++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++++++++++         return data['features'], data['label']
+++++++++++++ 
++++++++++++ diff --git a/main.py b/main.py
++++++++++++-index 9e68cee..18ac59b 100644
+++++++++++++index 18ac59b..8f75cf5 100644
++++++++++++ --- a/main.py
++++++++++++ +++ b/main.py
++++++++++++-@@ -256,7 +256,7 @@ class Processor():
++++++++++++-                 batch_size=batch_size,
++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++-                 num_workers=self.arg.num_worker,
++++++++++++--                pin_memory=True,
++++++++++++-+                pin_memory=False,
++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++-             )
++++++++++++-             return loader
++++++++++++-@@ -268,7 +268,7 @@ class Processor():
++++++++++++-                 drop_last=train_flag,
++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++--                pin_memory=True,
++++++++++++-+                pin_memory=False,
++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++-             )
++++++++++++- 
++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++++-index 528856d..d8fcaf9 100644
++++++++++++---- a/seq_scripts.py
++++++++++++-+++ b/seq_scripts.py
++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++++++++-     return
++++++++++++- 
++++++++++++- 
++++++++++++-+import csv 
++++++++++++-+from jiwer import wer as jiwer_wer
++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++++++++-     model.eval()
++++++++++++--    results=defaultdict(dict)
++++++++++++-+    results = defaultdict(dict)
++++++++++++- 
++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++++++++-         recoder.record_timer("device")
++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++-                 results[inf]['conv_sents'] = conv_sents
++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++++++++-                 results[inf]['gloss'] = gl
++++++++++++-+
++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++@@ -21,6 +21,7 @@ import utils
+++++++++++++ from seq_scripts import seq_train, seq_eval
+++++++++++++ from torch.cuda.amp import autocast as autocast
+++++++++++++ from utils.misc import *
++++++++++++++from utils.decode import analyze_frame_lengths
+++++++++++++ class Processor():
+++++++++++++     def __init__(self, arg):
+++++++++++++         self.arg = arg
+++++++++++++@@ -105,13 +106,25 @@ class Processor():
+++++++++++++                 print('Please appoint --weights.')
+++++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++ +
++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++++++++--        reg_per = wer_results
++++++++++++--    else:
++++++++++++--        reg_per = wer_results_con
++++++++++++++            train_wer = seq_eval(
++++++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++++            )
++++++++++++++            dev_wer = seq_eval(
++++++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++++            )
++++++++++++++            test_wer = seq_eval(
++++++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++++            )
++++++++++++ +
++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++++++             self.recoder.print_log('Evaluation Done.\n')
++++++++++++ +
++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++++++++-         f"{work_dir}/{mode}.txt")
++++++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++++++++++            analyze_frame_lengths()
++++++++++++ +
++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++++++++-+        f"{work_dir}/{mode}.txt")
++++++++++++-+
++++++++++++-+    # ✅ 전체 결과 CSV로 저장
++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++++++++++++-+
++++++++++++-+    rows = []
++++++++++++-+    for file_id in results:
++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++-+
++++++++++++-+        rows.append([
++++++++++++-+            file_id,
++++++++++++-+            gt,
++++++++++++-+            conv_pred,
++++++++++++-+            f"{conv_wer:.4f}",
++++++++++++-+            lstm_pred,
++++++++++++-+            f"{lstm_wer:.4f}"
++++++++++++-+        ])
++++++++++++-+
++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++++++++-+        writer = csv.writer(f)
++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++++++++-+        writer.writerows(rows)
++++++++++++-+
++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++++         elif self.arg.phase == "features":
+++++++++++++             for mode in ["train", "dev", "test"]:
+++++++++++++                 seq_feature_generation(
+++++++++++++@@ -119,6 +132,8 @@ class Processor():
+++++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++++++++++                 )
+++++++++++++ 
++++++++++++ +
++++++++++++ +
+++++++++++++     def save_arg(self):
+++++++++++++         arg_dict = vars(self.arg)
+++++++++++++         if not os.path.exists(self.arg.work_dir):
+++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++++++++index 71519fd..b9664e1 100644
+++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++++++++diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++++index d8fcaf9..77cfc71 100644
+++++++++++++--- a/seq_scripts.py
++++++++++++++++ b/seq_scripts.py
+++++++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++++         })
+++++++++++++ 
+++++++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++++-    
++++++++++++++    # 전체 샘플 WER 평균 계산
++++++++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
++++++++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
++++++++++++ +
++++++++++++-+    # WER 기준 상위 5개 샘플 출력
++++++++++++-+    sample_wers = []
++++++++++++-+    for file_id in results:
++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++-+    
++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++-+    
++++++++++++-+        sample_wers.append({
++++++++++++-+            'file_id': file_id,
++++++++++++-+            'gt': gt,
++++++++++++-+            'conv_pred': conv_pred,
++++++++++++-+            'conv_wer': conv_wer,
++++++++++++-+            'lstm_pred': lstm_pred,
++++++++++++-+            'lstm_wer': lstm_wer,
++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++++++++-+        })
++++++++++++++    print("\n📊 전체 평균 WER")
++++++++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
++++++++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
++++++++++++ +
++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++-+    
++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++-+    for sample in top5:
++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++-+        print(f"GT   : {sample['gt']}")
++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++++++++-+        print("-" * 60)
+++++++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++++     for sample in top5:
+++++++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++++++++index e69de29..0342039 100644
+++++++++++++--- a/tmp.ipynb
++++++++++++++++ b/tmp.ipynb
+++++++++++++@@ -0,0 +1,272 @@
++++++++++++++{
++++++++++++++ "cells": [
++++++++++++++  {
++++++++++++++   "cell_type": "code",
++++++++++++++   "execution_count": 2,
++++++++++++++   "metadata": {},
++++++++++++++   "outputs": [],
++++++++++++++   "source": [
++++++++++++++    "import torch\n",
++++++++++++++    "\n",
++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++++++++   ]
++++++++++++++  },
++++++++++++++  {
++++++++++++++   "cell_type": "code",
++++++++++++++   "execution_count": 3,
++++++++++++++   "metadata": {},
++++++++++++++   "outputs": [
++++++++++++++    {
++++++++++++++     "name": "stdout",
++++++++++++++     "output_type": "stream",
++++++++++++++     "text": [
++++++++++++++      "Model is a state_dict.\n",
++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++++++++     ]
++++++++++++++    }
++++++++++++++   ],
++++++++++++++   "source": [
++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++++++++   ]
++++++++++++++  },
++++++++++++++  {
++++++++++++++   "cell_type": "code",
++++++++++++++   "execution_count": 7,
++++++++++++++   "metadata": {},
++++++++++++++   "outputs": [],
++++++++++++++   "source": [
++++++++++++++    "import torch\n",
++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++++++++    "import torch.nn.functional as F\n"
++++++++++++++   ]
++++++++++++++  },
++++++++++++++  {
++++++++++++++   "cell_type": "code",
++++++++++++++   "execution_count": 1,
++++++++++++++   "metadata": {},
++++++++++++++   "outputs": [
++++++++++++++    {
++++++++++++++     "name": "stderr",
++++++++++++++     "output_type": "stream",
++++++++++++++     "text": [
++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++++++++     ]
++++++++++++++    }
++++++++++++++   ],
++++++++++++++   "source": [
++++++++++++++    "import torch\n",
++++++++++++++    "import torch.nn as nn\n",
++++++++++++++    "import torch.nn.functional as F\n",
++++++++++++++    "import torchvision.models as models\n",
++++++++++++++    "import numpy as np\n",
++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++++++++    "from modules.criterions import SeqKD\n",
++++++++++++++    "import utils\n",
++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++++++++    "class Identity(nn.Module):\n",
++++++++++++++    "    def __init__(self):\n",
++++++++++++++    "        super(Identity, self).__init__()\n",
++++++++++++++    "\n",
++++++++++++++    "    def forward(self, x):\n",
++++++++++++++    "        return x\n",
++++++++++++++    "\n",
++++++++++++++    "# L2 정규화 선형 레이어\n",
++++++++++++++    "class NormLinear(nn.Module):\n",
++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++++++++    "        super(NormLinear, self).__init__()\n",
++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++++++++    "\n",
++++++++++++++    "    def forward(self, x):\n",
++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++++++++    "        return outputs\n",
++++++++++++++    "\n",
++++++++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++++++++    "class SLRModel(nn.Module):\n",
++++++++++++++    "    def __init__(\n",
++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++++++++    "    ):\n",
++++++++++++++    "        super(SLRModel, self).__init__()\n",
++++++++++++++    "        self.decoder = None\n",
++++++++++++++    "        self.loss = dict()\n",
++++++++++++++    "        self.criterion_init()\n",
++++++++++++++    "        self.num_classes = num_classes\n",
++++++++++++++    "        self.loss_weights = loss_weights\n",
++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++++++++    "\n",
++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++++++++    "                                   hidden_size=hidden_size,\n",
++++++++++++++    "                                   conv_type=conv_type,\n",
++++++++++++++    "                                   use_bn=use_bn,\n",
++++++++++++++    "                                   num_classes=num_classes)\n",
++++++++++++++    "\n",
++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++++++++    "\n",
++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++++++++    "\n",
++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++++++++    "        if weight_norm:\n",
++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++    "        else:\n",
++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++    "\n",
++++++++++++++    "        # Classifier 공유 여부\n",
++++++++++++++    "        if share_classifier:\n",
++++++++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++++++++    "\n",
++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++++++++    "        if len(x.shape) == 5:\n",
++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++++++++    "        else:\n",
++++++++++++++    "            framewise = x\n",
++++++++++++++    "\n",
++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++++++++    "\n",
++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++++++++    "\n",
++++++++++++++    "        # 최종 Classifier 적용\n",
++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++++++++    "\n",
++++++++++++++    "        # Inference 모드에서 Decoding\n",
++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++++++++    "\n",
++++++++++++++    "        return {\n",
++++++++++++++    "            \"framewise_features\": framewise,\n",
++++++++++++++    "            \"visual_features\": x,\n",
++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++++++++    "            \"feat_len\": lgt,\n",
++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++++++++    "            \"sequence_logits\": outputs,\n",
++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++++++++    "            \"recognized_sents\": pred,\n",
++++++++++++++    "        }\n",
++++++++++++++    "\n",
++++++++++++++    "    def criterion_init(self):\n",
++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++++++++    "        return self.loss\n"
++++++++++++++   ]
++++++++++++++  },
++++++++++++++  {
++++++++++++++   "cell_type": "code",
++++++++++++++   "execution_count": 6,
++++++++++++++   "metadata": {},
++++++++++++++   "outputs": [
++++++++++++++    {
++++++++++++++     "ename": "KeyError",
++++++++++++++     "evalue": "'dataset_info'",
++++++++++++++     "output_type": "error",
++++++++++++++     "traceback": [
++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++++++++     ]
++++++++++++++    }
++++++++++++++   ],
++++++++++++++   "source": [
++++++++++++++    "import os\n",
++++++++++++++    "import numpy as np\n",
++++++++++++++    "import yaml\n",
++++++++++++++    "\n",
++++++++++++++    "# 환경 변수 설정\n",
++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++++++++    "\n",
++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++++++++    "with open(config_path, \"r\") as f:\n",
++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++++++++    "\n",
++++++++++++++    "# ✅ gloss_dict 로드\n",
++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++++++++    "\n",
++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++++++++   ]
++++++++++++++  },
++++++++++++++  {
++++++++++++++   "cell_type": "code",
++++++++++++++   "execution_count": 5,
++++++++++++++   "metadata": {},
++++++++++++++   "outputs": [
++++++++++++++    {
++++++++++++++     "ename": "AttributeError",
++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++++++++     "output_type": "error",
++++++++++++++     "traceback": [
++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++++++++     ]
++++++++++++++    }
++++++++++++++   ],
++++++++++++++   "source": [
++++++++++++++    "import torch\n",
++++++++++++++    "\n",
++++++++++++++    "# 모델 불러오기\n",
++++++++++++++    "model = SLRModel(\n",
++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++++++++    ")\n",
++++++++++++++    "\n",
++++++++++++++    "# 저장된 가중치 로드\n",
++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++++++++    "\n",
++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++++++++    "if isinstance(state_dict, dict):\n",
++++++++++++++    "    model.load_state_dict(state_dict)\n",
++++++++++++++    "\n",
++++++++++++++    "# 모델을 평가 모드로 설정\n",
++++++++++++++    "model.eval()\n"
++++++++++++++   ]
++++++++++++++  }
++++++++++++++ ],
++++++++++++++ "metadata": {
++++++++++++++  "kernelspec": {
++++++++++++++   "display_name": "3.9.13",
++++++++++++++   "language": "python",
++++++++++++++   "name": "python3"
++++++++++++++  },
++++++++++++++  "language_info": {
++++++++++++++   "codemirror_mode": {
++++++++++++++    "name": "ipython",
++++++++++++++    "version": 3
++++++++++++++   },
++++++++++++++   "file_extension": ".py",
++++++++++++++   "mimetype": "text/x-python",
++++++++++++++   "name": "python",
++++++++++++++   "nbconvert_exporter": "python",
++++++++++++++   "pygments_lexer": "ipython3",
++++++++++++++   "version": "3.9.13"
++++++++++++++  }
++++++++++++++ },
++++++++++++++ "nbformat": 4,
++++++++++++++ "nbformat_minor": 2
++++++++++++++}
+++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++++++++index cb157af..c502b5d 100644
+++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++++++++index 3877729..ac8dab6 100644
+++++++++++++--- a/utils/decode.py
++++++++++++++++ b/utils/decode.py
+++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
+++++++++++++ import numpy as np
+++++++++++++ from itertools import groupby
+++++++++++++ import torch.nn.functional as F
++++++++++++++import matplotlib.pyplot as plt
++++++++++++ +
++++++++++++++# ⬇ 프레임 길이 저장용 전역 리스트
++++++++++++++frame_lengths = []
++++++++++++ +
++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
++++++++++++ +
++++++++++++++    length = int(vid_lgt[batch_idx].item())
++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++++++++    x = np.arange(length)
++++++++++++ +
++++++++++++++    plt.figure(figsize=(15, 4))
++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++++++++++ +
++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++++++++++    if len(blank_indices) > 0:
++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++++++++++ +
++++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++++++++++++    if sample_id:
++++++++++++++        title_str += f"\nSample: {sample_id}"
++++++++++++++    plt.title(title_str)
++++++++++++++    plt.xlabel("Time Step")
++++++++++++++    plt.ylabel("Gloss ID")
++++++++++++++    plt.yticks(np.unique(pred_seq))
++++++++++++++    plt.grid(True)
++++++++++++++    plt.legend()
++++++++++++++    plt.tight_layout()
++++++++++++++    plt.show()
+++++++++++++ 
+++++++++++++ 
+++++++++++++ class Decode(object):
+++++++++++++@@ -16,35 +48,27 @@ class Decode(object):
+++++++++++++         self.search_mode = search_mode
+++++++++++++         self.blank_id = blank_id
+++++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+++++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+++++++++++++-                                                    num_processes=10)
++++++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+++++++++++++ 
+++++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+++++++++++++         if not batch_first:
+++++++++++++             nn_output = nn_output.permute(1, 0, 2)
++++++++++++ +
++++++++++++++        # ⬇ 프레임 길이 수집
++++++++++++++        global frame_lengths
++++++++++++++        for i in range(vid_lgt.size(0)):
++++++++++++++            frame_lengths.append(int(vid_lgt[i].item()))
++++++++++++ +
++++++++++++++        # sample_id가 존재하면 시각화
++++++++++++++        sample_id = sample_ids[0] if sample_ids else None
++++++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
++++++++++++ +
+++++++++++++         if self.search_mode == "max":
+++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++++++++         else:
+++++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
+++++++++++++ 
+++++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+++++++++++++-        '''
+++++++++++++-        CTCBeamDecoder Shape:
+++++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+++++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+++++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+++++++++++++-                          timesteps (B, N_beams)
+++++++++++++-                          out_lens (B, N_beams)
+++++++++++++-        '''
+++++++++++++-
+++++++++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
+++++++++++++-        batchsize, lgt = index_list.shape
+++++++++++++-        blank_rate =[]
+++++++++++++-        for batch_idx in range(batchsize):
+++++++++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+++++++++++++-            blank_rate.append(group_result)
+++++++++++++-
+++++++++++++-
+++++++++++++         if not probs:
+++++++++++++             nn_output = nn_output.softmax(-1).cpu()
+++++++++++++         vid_lgt = vid_lgt.cpu()
+++++++++++++@@ -54,9 +78,7 @@ class Decode(object):
+++++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+++++++++++++             if len(first_result) != 0:
+++++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+++++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+++++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+++++++++++++-                             enumerate(first_result)])
++++++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+++++++++++++         return ret_list
+++++++++++++ 
+++++++++++++     def MaxDecode(self, nn_output, vid_lgt):
+++++++++++++@@ -65,12 +87,34 @@ class Decode(object):
+++++++++++++         ret_list = []
+++++++++++++         for batch_idx in range(batchsize):
+++++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+++++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++++++++++++++            filtered = [x for x in group_result if x != self.blank_id]
+++++++++++++             if len(filtered) > 0:
+++++++++++++                 max_result = torch.stack(filtered)
+++++++++++++                 max_result = [x[0] for x in groupby(max_result)]
+++++++++++++             else:
+++++++++++++                 max_result = filtered
+++++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+++++++++++++-                             enumerate(max_result)])
++++++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++++++++++++         return ret_list
++++++++++++ +
++++++++++++ +
++++++++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
++++++++++++++def analyze_frame_lengths():
++++++++++++++    if not frame_lengths:
++++++++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
++++++++++++++        return
++++++++++++ +
++++++++++++++    print("\n📊 Test Video Frame Length Analysis:")
++++++++++++++    print(f"- Total samples: {len(frame_lengths)}")
++++++++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
++++++++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
++++++++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
++++++++++++ +
++++++++++++++    # 히스토그램 시각화
++++++++++++++    plt.figure(figsize=(10, 5))
++++++++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
++++++++++++++    plt.title("Test Video Frame Length Distribution")
++++++++++++++    plt.xlabel("Frame Length")
++++++++++++++    plt.ylabel("Number of Samples")
++++++++++++++    plt.grid(True)
++++++++++++++    plt.tight_layout()
++++++++++++++    plt.show()
+++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++++++++index 7d0a2aa..cd66258 100644
+++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++++++++++++index c31483a..239691c 100644
+++++++++++++--- a/work_dirt/config.yaml
++++++++++++++++ b/work_dirt/config.yaml
+++++++++++++@@ -7,7 +7,7 @@ dataset_info:
+++++++++++++   evaluation_dir: ./evaluation/slr_eval
+++++++++++++   evaluation_prefix: phoenix2014-groundtruth
+++++++++++++ decode_mode: beam
+++++++++++++-device: your_device
++++++++++++++device: cuda
+++++++++++++ dist_url: env://
+++++++++++++ eval_interval: 1
+++++++++++++ evaluate_tool: python
+++++++++++++diff --git a/work_dirt/dataloader_video.py b/work_dirt/dataloader_video.py
+++++++++++++index c126e7a..342e7f8 100644
+++++++++++++--- a/work_dirt/dataloader_video.py
++++++++++++++++ b/work_dirt/dataloader_video.py
+++++++++++++@@ -113,6 +113,11 @@ class BaseFeeder(data.Dataset):
+++++++++++++     def read_features(self, index):
+++++++++++++         # load file info
+++++++++++++         fi = self.inputs_list[index]
++++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++++        print(f"./features/{self.mode}/{fi['fileid']}_features.npy")
++++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
++++++++++++++        print('111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111')
+++++++++++++         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
+++++++++++++         return data['features'], data['label']
+++++++++++++ 
+++++++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
+++++++++++++index 00c1a0e..7f1d26c 100644
+++++++++++++--- a/work_dirt/dev.txt
++++++++++++++++ b/work_dirt/dev.txt
+++++++++++++@@ -8,3 +8,11 @@
+++++++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
+++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++++[ Wed Apr  2 16:07:40 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++++++++index 8a22ece..774458a 100644
+++++++++++++--- a/work_dirt/dirty.patch
++++++++++++++++ b/work_dirt/dirty.patch
+++++++++++++@@ -1,284 +1,7609 @@
+++++++++++++-diff --git a/README.md b/README.md
+++++++++++++-index bdbc17f..8cb240b 100644
+++++++++++++---- a/README.md
+++++++++++++-+++ b/README.md
+++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++++++++++- 
+++++++++++++- 
+++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++++++++- 
+++++++++++++- ### Training
+++++++++++++- 
+++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++++++++-index bfc1da8..25ffa61 100644
+++++++++++++---- a/configs/baseline.yaml
+++++++++++++-+++ b/configs/baseline.yaml
+++++++++++++-@@ -1,14 +1,14 @@
+++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++++++++- phase: train
+++++++++++++--dataset: phoenix2014-T
+++++++++++++-+dataset: phoenix2014
+++++++++++++- #CSL-Daily
+++++++++++++- # dataset: phoenix14-si5
+++++++++++++- 
+++++++++++++- work_dir: ./work_dirt/
+++++++++++++--batch_size: 4
+++++++++++++-+batch_size: 1
+++++++++++++- random_seed: 0 
+++++++++++++--test_batch_size: 4
+++++++++++++--num_worker: 20
+++++++++++++-+test_batch_size: 1
+++++++++++++-+num_worker: 3
+++++++++++++- device: 0
+++++++++++++- log_interval: 10000
+++++++++++++- eval_interval: 1
+++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++++++++-index 555f4b8..c126e7a 100644
+++++++++++++---- a/dataset/dataloader_video.py
+++++++++++++-+++ b/dataset/dataloader_video.py
+++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++++++++++-+
+++++++++++++-         print(mode, len(self))
+++++++++++++-         self.data_aug = self.transform()
+++++++++++++-         print("")
+++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
+++++++++++++- 
+++++++++++++-     def read_video(self, index):
+++++++++++++--        # load file info
+++++++++++++-         fi = self.inputs_list[index]
+++++++++++++-+    
+++++++++++++-         if 'phoenix' in self.dataset:
+++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++++++++-+#            print(img_list)
+++++++++++++-+
+++++++++++++-+#            print("[LOG] Using phoenix")
+++++++++++++-+
+++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++++- 
+++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++++-+#            print(img_folder)
+++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++++++++-+#            print(len(img_list))
+++++++++++++-         elif self.dataset == 'CSL-Daily':
+++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++++++++-+    
+++++++++++++-         img_list = sorted(glob.glob(img_folder))
+++++++++++++-+    
+++++++++++++-+        if len(img_list) == 0:
+++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++++++++-+    
+++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++++++-+    
+++++++++++++-         label_list = []
+++++++++++++--        if self.dataset=='phoenix2014':
+++++++++++++-+        if self.dataset == 'phoenix2014':
+++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++++++++--        if self.dataset=='phoenix2014-T':
+++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++++++++-+    
+++++++++++++-         for phase in fi['label'].split(" "):
+++++++++++++--            if phase == '':
+++++++++++++--                continue
+++++++++++++--            if phase in self.dict.keys():
+++++++++++++-+            if phase and phase in self.dict:
+++++++++++++-                 label_list.append(self.dict[phase][0])
+++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++++++++-+    
+++++++++++++-+        video = [
+++++++++++++-+            cv2.cvtColor(
+++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++-+                cv2.COLOR_BGR2RGB
+++++++++++++-+            )   
+++++++++++++-+            for img_path in img_list
+++++++++++++-+        ]
+++++++++++++-+    
+++++++++++++-+        return video, label_list, fi
+++++++++++++- 
+++++++++++++-     def read_features(self, index):
+++++++++++++-         # load file info
++++++++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
++++++++++++++index ffef81f..f4065cc 100644
++++++++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++++++++index 7ac0c3b..c89f220 100644
++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++++++ diff --git a/main.py b/main.py
+++++++++++++-index 9e68cee..18ac59b 100644
++++++++++++++index 18ac59b..8f75cf5 100644
+++++++++++++ --- a/main.py
+++++++++++++ +++ b/main.py
+++++++++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++++++++-                 batch_size=batch_size,
+++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++-                 num_workers=self.arg.num_worker,
+++++++++++++--                pin_memory=True,
+++++++++++++-+                pin_memory=False,
+++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++-             )
+++++++++++++-             return loader
+++++++++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++++++++-                 drop_last=train_flag,
+++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++--                pin_memory=True,
+++++++++++++-+                pin_memory=False,
+++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++-             )
+++++++++++++- 
+++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++++-index 528856d..d8fcaf9 100644
+++++++++++++---- a/seq_scripts.py
+++++++++++++-+++ b/seq_scripts.py
+++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++++++++-     return
+++++++++++++- 
+++++++++++++- 
+++++++++++++-+import csv 
+++++++++++++-+from jiwer import wer as jiwer_wer
+++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++++++++-     model.eval()
+++++++++++++--    results=defaultdict(dict)
+++++++++++++-+    results = defaultdict(dict)
+++++++++++++- 
+++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++++++++-         recoder.record_timer("device")
+++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++++++++-                 results[inf]['gloss'] = gl
+++++++++++++-+
+++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++-+
+++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++++++++--        reg_per = wer_results
+++++++++++++--    else:
+++++++++++++--        reg_per = wer_results_con
++++++++++++++@@ -21,6 +21,7 @@ import utils
++++++++++++++ from seq_scripts import seq_train, seq_eval
++++++++++++++ from torch.cuda.amp import autocast as autocast
++++++++++++++ from utils.misc import *
+++++++++++++++from utils.decode import analyze_frame_lengths
++++++++++++++ class Processor():
++++++++++++++     def __init__(self, arg):
++++++++++++++         self.arg = arg
++++++++++++++@@ -105,13 +106,25 @@ class Processor():
++++++++++++++                 print('Please appoint --weights.')
++++++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++++ +
+++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++++++++            train_wer = seq_eval(
+++++++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++++            )
+++++++++++++++            dev_wer = seq_eval(
+++++++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++++            )
+++++++++++++++            test_wer = seq_eval(
+++++++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++++            )
+++++++++++++ +
+++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++++++++-         f"{work_dir}/{mode}.txt")
++++++++++++++             self.recoder.print_log('Evaluation Done.\n')
+++++++++++++ +
+++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++++++++-+        f"{work_dir}/{mode}.txt")
+++++++++++++-+
+++++++++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++++++-+
+++++++++++++-+    rows = []
+++++++++++++-+    for file_id in results:
+++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++-+
+++++++++++++-+        rows.append([
+++++++++++++-+            file_id,
+++++++++++++-+            gt,
+++++++++++++-+            conv_pred,
+++++++++++++-+            f"{conv_wer:.4f}",
+++++++++++++-+            lstm_pred,
+++++++++++++-+            f"{lstm_wer:.4f}"
+++++++++++++-+        ])
+++++++++++++-+
+++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++++++++-+        writer = csv.writer(f)
+++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++++++++-+        writer.writerows(rows)
+++++++++++++-+
+++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++++++++++            analyze_frame_lengths()
+++++++++++++ +
++++++++++++++         elif self.arg.phase == "features":
++++++++++++++             for mode in ["train", "dev", "test"]:
++++++++++++++                 seq_feature_generation(
++++++++++++++@@ -119,6 +132,8 @@ class Processor():
++++++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++++++++++                 )
++++++++++++++ 
+++++++++++++ +
+++++++++++++ +
+++++++++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++++++++-+    sample_wers = []
+++++++++++++-+    for file_id in results:
+++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++-+    
+++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++-+    
+++++++++++++-+        sample_wers.append({
+++++++++++++-+            'file_id': file_id,
+++++++++++++-+            'gt': gt,
+++++++++++++-+            'conv_pred': conv_pred,
+++++++++++++-+            'conv_wer': conv_wer,
+++++++++++++-+            'lstm_pred': lstm_pred,
+++++++++++++-+            'lstm_wer': lstm_wer,
+++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++++++++-+        })
++++++++++++++     def save_arg(self):
++++++++++++++         arg_dict = vars(self.arg)
++++++++++++++         if not os.path.exists(self.arg.work_dir):
++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++++++++index 71519fd..b9664e1 100644
++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++++++++diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++++++index d8fcaf9..77cfc71 100644
++++++++++++++--- a/seq_scripts.py
+++++++++++++++++ b/seq_scripts.py
++++++++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++++         })
++++++++++++++ 
++++++++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++++-    
+++++++++++++++    # 전체 샘플 WER 평균 계산
+++++++++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+++++++++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+++++++++++++ +
+++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++++-+    
+++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++++-+    for sample in top5:
+++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++++-+        print(f"GT   : {sample['gt']}")
+++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++++++++-+        print("-" * 60)
+++++++++++++++    print("\n📊 전체 평균 WER")
+++++++++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+++++++++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+++++++++++++ +
++++++++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++++     for sample in top5:
++++++++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++++++++index e69de29..0342039 100644
++++++++++++++--- a/tmp.ipynb
+++++++++++++++++ b/tmp.ipynb
++++++++++++++@@ -0,0 +1,272 @@
+++++++++++++++{
+++++++++++++++ "cells": [
+++++++++++++++  {
+++++++++++++++   "cell_type": "code",
+++++++++++++++   "execution_count": 2,
+++++++++++++++   "metadata": {},
+++++++++++++++   "outputs": [],
+++++++++++++++   "source": [
+++++++++++++++    "import torch\n",
+++++++++++++++    "\n",
+++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++++++++   ]
+++++++++++++++  },
+++++++++++++++  {
+++++++++++++++   "cell_type": "code",
+++++++++++++++   "execution_count": 3,
+++++++++++++++   "metadata": {},
+++++++++++++++   "outputs": [
+++++++++++++++    {
+++++++++++++++     "name": "stdout",
+++++++++++++++     "output_type": "stream",
+++++++++++++++     "text": [
+++++++++++++++      "Model is a state_dict.\n",
+++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++++++++     ]
+++++++++++++++    }
+++++++++++++++   ],
+++++++++++++++   "source": [
+++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++++++++   ]
+++++++++++++++  },
+++++++++++++++  {
+++++++++++++++   "cell_type": "code",
+++++++++++++++   "execution_count": 7,
+++++++++++++++   "metadata": {},
+++++++++++++++   "outputs": [],
+++++++++++++++   "source": [
+++++++++++++++    "import torch\n",
+++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++++++++    "import torch.nn.functional as F\n"
+++++++++++++++   ]
+++++++++++++++  },
+++++++++++++++  {
+++++++++++++++   "cell_type": "code",
+++++++++++++++   "execution_count": 1,
+++++++++++++++   "metadata": {},
+++++++++++++++   "outputs": [
+++++++++++++++    {
+++++++++++++++     "name": "stderr",
+++++++++++++++     "output_type": "stream",
+++++++++++++++     "text": [
+++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++++++++     ]
+++++++++++++++    }
+++++++++++++++   ],
+++++++++++++++   "source": [
+++++++++++++++    "import torch\n",
+++++++++++++++    "import torch.nn as nn\n",
+++++++++++++++    "import torch.nn.functional as F\n",
+++++++++++++++    "import torchvision.models as models\n",
+++++++++++++++    "import numpy as np\n",
+++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++++++++    "from modules.criterions import SeqKD\n",
+++++++++++++++    "import utils\n",
+++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++++++++    "class Identity(nn.Module):\n",
+++++++++++++++    "    def __init__(self):\n",
+++++++++++++++    "        super(Identity, self).__init__()\n",
+++++++++++++++    "\n",
+++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++    "        return x\n",
+++++++++++++++    "\n",
+++++++++++++++    "# L2 정규화 선형 레이어\n",
+++++++++++++++    "class NormLinear(nn.Module):\n",
+++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++++++++    "\n",
+++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++++++++    "        return outputs\n",
+++++++++++++++    "\n",
+++++++++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++++++++    "class SLRModel(nn.Module):\n",
+++++++++++++++    "    def __init__(\n",
+++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++++++++    "    ):\n",
+++++++++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++++++++    "        self.decoder = None\n",
+++++++++++++++    "        self.loss = dict()\n",
+++++++++++++++    "        self.criterion_init()\n",
+++++++++++++++    "        self.num_classes = num_classes\n",
+++++++++++++++    "        self.loss_weights = loss_weights\n",
+++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++++++++    "\n",
+++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++++++++    "                                   conv_type=conv_type,\n",
+++++++++++++++    "                                   use_bn=use_bn,\n",
+++++++++++++++    "                                   num_classes=num_classes)\n",
+++++++++++++++    "\n",
+++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++++++++    "\n",
+++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++++++++    "\n",
+++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++++++++    "        if weight_norm:\n",
+++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++    "        else:\n",
+++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++    "\n",
+++++++++++++++    "        # Classifier 공유 여부\n",
+++++++++++++++    "        if share_classifier:\n",
+++++++++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++++++++    "\n",
+++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++++++++    "        if len(x.shape) == 5:\n",
+++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++++++++    "        else:\n",
+++++++++++++++    "            framewise = x\n",
+++++++++++++++    "\n",
+++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++++++++    "\n",
+++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++++++++    "\n",
+++++++++++++++    "        # 최종 Classifier 적용\n",
+++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++++++++    "\n",
+++++++++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++++++++    "\n",
+++++++++++++++    "        return {\n",
+++++++++++++++    "            \"framewise_features\": framewise,\n",
+++++++++++++++    "            \"visual_features\": x,\n",
+++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++++++++    "            \"feat_len\": lgt,\n",
+++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++++++++    "            \"recognized_sents\": pred,\n",
+++++++++++++++    "        }\n",
+++++++++++++++    "\n",
+++++++++++++++    "    def criterion_init(self):\n",
+++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++++++++    "        return self.loss\n"
+++++++++++++++   ]
+++++++++++++++  },
+++++++++++++++  {
+++++++++++++++   "cell_type": "code",
+++++++++++++++   "execution_count": 6,
+++++++++++++++   "metadata": {},
+++++++++++++++   "outputs": [
+++++++++++++++    {
+++++++++++++++     "ename": "KeyError",
+++++++++++++++     "evalue": "'dataset_info'",
+++++++++++++++     "output_type": "error",
+++++++++++++++     "traceback": [
+++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++++++++     ]
+++++++++++++++    }
+++++++++++++++   ],
+++++++++++++++   "source": [
+++++++++++++++    "import os\n",
+++++++++++++++    "import numpy as np\n",
+++++++++++++++    "import yaml\n",
+++++++++++++++    "\n",
+++++++++++++++    "# 환경 변수 설정\n",
+++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++++++++    "\n",
+++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++++++++    "\n",
+++++++++++++++    "# ✅ gloss_dict 로드\n",
+++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++++++++    "\n",
+++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++++++++   ]
+++++++++++++++  },
+++++++++++++++  {
+++++++++++++++   "cell_type": "code",
+++++++++++++++   "execution_count": 5,
+++++++++++++++   "metadata": {},
+++++++++++++++   "outputs": [
+++++++++++++++    {
+++++++++++++++     "ename": "AttributeError",
+++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++++++++     "output_type": "error",
+++++++++++++++     "traceback": [
+++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++++++++     ]
+++++++++++++++    }
+++++++++++++++   ],
+++++++++++++++   "source": [
+++++++++++++++    "import torch\n",
+++++++++++++++    "\n",
+++++++++++++++    "# 모델 불러오기\n",
+++++++++++++++    "model = SLRModel(\n",
+++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++++++++    ")\n",
+++++++++++++++    "\n",
+++++++++++++++    "# 저장된 가중치 로드\n",
+++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++++++++    "\n",
+++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++++++++    "if isinstance(state_dict, dict):\n",
+++++++++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++++++++    "\n",
+++++++++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++++++++    "model.eval()\n"
+++++++++++++++   ]
+++++++++++++++  }
+++++++++++++++ ],
+++++++++++++++ "metadata": {
+++++++++++++++  "kernelspec": {
+++++++++++++++   "display_name": "3.9.13",
+++++++++++++++   "language": "python",
+++++++++++++++   "name": "python3"
+++++++++++++++  },
+++++++++++++++  "language_info": {
+++++++++++++++   "codemirror_mode": {
+++++++++++++++    "name": "ipython",
+++++++++++++++    "version": 3
+++++++++++++++   },
+++++++++++++++   "file_extension": ".py",
+++++++++++++++   "mimetype": "text/x-python",
+++++++++++++++   "name": "python",
+++++++++++++++   "nbconvert_exporter": "python",
+++++++++++++++   "pygments_lexer": "ipython3",
+++++++++++++++   "version": "3.9.13"
+++++++++++++++  }
+++++++++++++++ },
+++++++++++++++ "nbformat": 4,
+++++++++++++++ "nbformat_minor": 2
+++++++++++++++}
++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++++++++index cb157af..c502b5d 100644
++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++++++++index 3877729..ac8dab6 100644
++++++++++++++--- a/utils/decode.py
+++++++++++++++++ b/utils/decode.py
++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++++++++++ import numpy as np
++++++++++++++ from itertools import groupby
++++++++++++++ import torch.nn.functional as F
+++++++++++++++import matplotlib.pyplot as plt
+++++++++++++ +
+++++++++++++++# ⬇ 프레임 길이 저장용 전역 리스트
+++++++++++++++frame_lengths = []
+++++++++++++ +
+++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
+++++++++++++ +
+++++++++++++++    length = int(vid_lgt[batch_idx].item())
+++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++++++++    x = np.arange(length)
+++++++++++++ +
+++++++++++++++    plt.figure(figsize=(15, 4))
+++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++++++++++ +
+++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++++++++++    if len(blank_indices) > 0:
+++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++++++++++ +
+++++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++++++++++++++    if sample_id:
+++++++++++++++        title_str += f"\nSample: {sample_id}"
+++++++++++++++    plt.title(title_str)
+++++++++++++++    plt.xlabel("Time Step")
+++++++++++++++    plt.ylabel("Gloss ID")
+++++++++++++++    plt.yticks(np.unique(pred_seq))
+++++++++++++++    plt.grid(True)
+++++++++++++++    plt.legend()
+++++++++++++++    plt.tight_layout()
+++++++++++++++    plt.show()
++++++++++++++ 
++++++++++++++ 
++++++++++++++ class Decode(object):
++++++++++++++@@ -16,35 +48,27 @@ class Decode(object):
++++++++++++++         self.search_mode = search_mode
++++++++++++++         self.blank_id = blank_id
++++++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++++++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++++++++++++++-                                                    num_processes=10)
+++++++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++++++++++++++ 
++++++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++++++++++++++         if not batch_first:
++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++++++++ +
+++++++++++++++        # ⬇ 프레임 길이 수집
+++++++++++++++        global frame_lengths
+++++++++++++++        for i in range(vid_lgt.size(0)):
+++++++++++++++            frame_lengths.append(int(vid_lgt[i].item()))
+++++++++++++ +
+++++++++++++++        # sample_id가 존재하면 시각화
+++++++++++++++        sample_id = sample_ids[0] if sample_ids else None
+++++++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+++++++++++++ +
++++++++++++++         if self.search_mode == "max":
++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++++++++         else:
++++++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
++++++++++++++ 
++++++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++++++++++++++-        '''
++++++++++++++-        CTCBeamDecoder Shape:
++++++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++++++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++++++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++++++++++++++-                          timesteps (B, N_beams)
++++++++++++++-                          out_lens (B, N_beams)
++++++++++++++-        '''
++++++++++++++-
++++++++++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
++++++++++++++-        batchsize, lgt = index_list.shape
++++++++++++++-        blank_rate =[]
++++++++++++++-        for batch_idx in range(batchsize):
++++++++++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++++++++++++++-            blank_rate.append(group_result)
++++++++++++++-
++++++++++++++-
++++++++++++++         if not probs:
++++++++++++++             nn_output = nn_output.softmax(-1).cpu()
++++++++++++++         vid_lgt = vid_lgt.cpu()
++++++++++++++@@ -54,9 +78,7 @@ class Decode(object):
++++++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++++++++++++++             if len(first_result) != 0:
++++++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++++++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++++++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++++++++++++++-                             enumerate(first_result)])
+++++++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++++++++++++++         return ret_list
++++++++++++++ 
++++++++++++++     def MaxDecode(self, nn_output, vid_lgt):
++++++++++++++@@ -65,12 +87,34 @@ class Decode(object):
++++++++++++++         ret_list = []
++++++++++++++         for batch_idx in range(batchsize):
++++++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++++++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++++++++++++++            filtered = [x for x in group_result if x != self.blank_id]
++++++++++++++             if len(filtered) > 0:
++++++++++++++                 max_result = torch.stack(filtered)
++++++++++++++                 max_result = [x[0] for x in groupby(max_result)]
++++++++++++++             else:
++++++++++++++                 max_result = filtered
++++++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++++++++++++++-                             enumerate(max_result)])
+++++++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
++++++++++++++         return ret_list
+++++++++++++ +
+++++++++++++ +
+++++++++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
+++++++++++++++def analyze_frame_lengths():
+++++++++++++++    if not frame_lengths:
+++++++++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
+++++++++++++++        return
+++++++++++++ +
+++++++++++++++    print("\n📊 Test Video Frame Length Analysis:")
+++++++++++++++    print(f"- Total samples: {len(frame_lengths)}")
+++++++++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
+++++++++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
+++++++++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
+++++++++++++ +
+++++++++++++++    # 히스토그램 시각화
+++++++++++++++    plt.figure(figsize=(10, 5))
+++++++++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
+++++++++++++++    plt.title("Test Video Frame Length Distribution")
+++++++++++++++    plt.xlabel("Frame Length")
+++++++++++++++    plt.ylabel("Number of Samples")
+++++++++++++++    plt.grid(True)
+++++++++++++++    plt.tight_layout()
+++++++++++++++    plt.show()
++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++++++++index 7d0a2aa..cd66258 100644
++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++++++++++++index c31483a..239691c 100644
++++++++++++++--- a/work_dirt/config.yaml
+++++++++++++++++ b/work_dirt/config.yaml
++++++++++++++@@ -7,7 +7,7 @@ dataset_info:
++++++++++++++   evaluation_dir: ./evaluation/slr_eval
++++++++++++++   evaluation_prefix: phoenix2014-groundtruth
++++++++++++++ decode_mode: beam
++++++++++++++-device: your_device
+++++++++++++++device: cuda
++++++++++++++ dist_url: env://
++++++++++++++ eval_interval: 1
++++++++++++++ evaluate_tool: python
++++++++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
++++++++++++++index 00c1a0e..2952bcf 100644
++++++++++++++--- a/work_dirt/dev.txt
+++++++++++++++++ b/work_dirt/dev.txt
++++++++++++++@@ -8,3 +8,9 @@
++++++++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
++++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++++[ Wed Apr  2 15:43:54 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++++++++++index 8a22ece..ace4a9e 100644
++++++++++++++--- a/work_dirt/dirty.patch
+++++++++++++++++ b/work_dirt/dirty.patch
++++++++++++++@@ -1,284 +1,6683 @@
++++++++++++++-diff --git a/README.md b/README.md
++++++++++++++-index bdbc17f..8cb240b 100644
++++++++++++++---- a/README.md
++++++++++++++-+++ b/README.md
++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++++++++- 
++++++++++++++- 
++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++++++++++- 
++++++++++++++- ### Training
++++++++++++++- 
++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++++++++++-index bfc1da8..25ffa61 100644
++++++++++++++---- a/configs/baseline.yaml
++++++++++++++-+++ b/configs/baseline.yaml
++++++++++++++-@@ -1,14 +1,14 @@
++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++++++++++- phase: train
++++++++++++++--dataset: phoenix2014-T
++++++++++++++-+dataset: phoenix2014
++++++++++++++- #CSL-Daily
++++++++++++++- # dataset: phoenix14-si5
++++++++++++++- 
++++++++++++++- work_dir: ./work_dirt/
++++++++++++++--batch_size: 4
++++++++++++++-+batch_size: 1
++++++++++++++- random_seed: 0 
++++++++++++++--test_batch_size: 4
++++++++++++++--num_worker: 20
++++++++++++++-+test_batch_size: 1
++++++++++++++-+num_worker: 3
++++++++++++++- device: 0
++++++++++++++- log_interval: 10000
++++++++++++++- eval_interval: 1
++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++++++++++-index 555f4b8..c126e7a 100644
++++++++++++++---- a/dataset/dataloader_video.py
++++++++++++++-+++ b/dataset/dataloader_video.py
++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++++++++-+
++++++++++++++-         print(mode, len(self))
++++++++++++++-         self.data_aug = self.transform()
++++++++++++++-         print("")
++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++++- 
++++++++++++++-     def read_video(self, index):
++++++++++++++--        # load file info
++++++++++++++-         fi = self.inputs_list[index]
++++++++++++++-+    
++++++++++++++-         if 'phoenix' in self.dataset:
++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++++++++++-+#            print(img_list)
++++++++++++++-+
++++++++++++++-+#            print("[LOG] Using phoenix")
++++++++++++++-+
++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++++++++- 
++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++++++-+#            print(img_folder)
++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++++++++++-+#            print(len(img_list))
++++++++++++++-         elif self.dataset == 'CSL-Daily':
++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++++++++++-+    
++++++++++++++-         img_list = sorted(glob.glob(img_folder))
++++++++++++++-+    
++++++++++++++-+        if len(img_list) == 0:
++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++++++++++-+    
++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++++++++++++-+    
++++++++++++++-         label_list = []
++++++++++++++--        if self.dataset=='phoenix2014':
++++++++++++++-+        if self.dataset == 'phoenix2014':
++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++++++++++--        if self.dataset=='phoenix2014-T':
++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++++++++++-+    
++++++++++++++-         for phase in fi['label'].split(" "):
++++++++++++++--            if phase == '':
++++++++++++++--                continue
++++++++++++++--            if phase in self.dict.keys():
++++++++++++++-+            if phase and phase in self.dict:
++++++++++++++-                 label_list.append(self.dict[phase][0])
++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++++++++++-+    
++++++++++++++-+        video = [
++++++++++++++-+            cv2.cvtColor(
++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++-+                cv2.COLOR_BGR2RGB
++++++++++++++-+            )   
++++++++++++++-+            for img_path in img_list
++++++++++++++-+        ]
++++++++++++++-+    
++++++++++++++-+        return video, label_list, fi
++++++++++++++- 
++++++++++++++-     def read_features(self, index):
++++++++++++++-         # load file info
++++++++++++++-diff --git a/main.py b/main.py
++++++++++++++-index 9e68cee..18ac59b 100644
++++++++++++++---- a/main.py
++++++++++++++-+++ b/main.py
++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
++++++++++++++-                 batch_size=batch_size,
++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++-                 num_workers=self.arg.num_worker,
++++++++++++++--                pin_memory=True,
++++++++++++++-+                pin_memory=False,
++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++-             )
++++++++++++++-             return loader
++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
++++++++++++++-                 drop_last=train_flag,
++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++--                pin_memory=True,
++++++++++++++-+                pin_memory=False,
++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++-             )
++++++++++++++- 
+++++++++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
+++++++++++++++index ffef81f..f4065cc 100644
+++++++++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
+++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++++++++++index 7ac0c3b..c89f220 100644
+++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++++++++++index 71519fd..b9664e1 100644
+++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++++++++ diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++++++-index 528856d..d8fcaf9 100644
+++++++++++++++index d8fcaf9..77cfc71 100644
++++++++++++++ --- a/seq_scripts.py
++++++++++++++ +++ b/seq_scripts.py
++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++++++++++-     return
++++++++++++++- 
+++++++++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++++++         })
++++++++++++++  
++++++++++++++-+import csv 
++++++++++++++-+from jiwer import wer as jiwer_wer
++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++++++++++-     model.eval()
++++++++++++++--    results=defaultdict(dict)
++++++++++++++-+    results = defaultdict(dict)
++++++++++++++- 
++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++++++++++-         recoder.record_timer("device")
++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++++++++++-                 results[inf]['gloss'] = gl
++++++++++++++-+
++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++-+
++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++++++++++--        reg_per = wer_results
++++++++++++++--    else:
++++++++++++++--        reg_per = wer_results_con
++++++++++++++-+
++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++++++++-+
++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++++++++++-         f"{work_dir}/{mode}.txt")
++++++++++++++-+
++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++++++++++-+        f"{work_dir}/{mode}.txt")
++++++++++++++-+
++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++++++++++++++-+
++++++++++++++-+    rows = []
++++++++++++++-+    for file_id in results:
++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++-+
++++++++++++++-+        rows.append([
++++++++++++++-+            file_id,
++++++++++++++-+            gt,
++++++++++++++-+            conv_pred,
++++++++++++++-+            f"{conv_wer:.4f}",
++++++++++++++-+            lstm_pred,
++++++++++++++-+            f"{lstm_wer:.4f}"
++++++++++++++-+        ])
++++++++++++++-+
++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++++++++++-+        writer = csv.writer(f)
++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++++++++++-+        writer.writerows(rows)
++++++++++++++-+
++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++++++++++++++-+
++++++++++++++-+
++++++++++++++-+
++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
++++++++++++++-+    sample_wers = []
++++++++++++++-+    for file_id in results:
++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++-+    
++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++-+    
++++++++++++++-+        sample_wers.append({
++++++++++++++-+            'file_id': file_id,
++++++++++++++-+            'gt': gt,
++++++++++++++-+            'conv_pred': conv_pred,
++++++++++++++-+            'conv_wer': conv_wer,
++++++++++++++-+            'lstm_pred': lstm_pred,
++++++++++++++-+            'lstm_wer': lstm_wer,
++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++++++++++-+        })
++++++++++++++-+
++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++++-+    
++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++++-+    for sample in top5:
++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++++-+        print(f"GT   : {sample['gt']}")
++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++++++++++-+        print("-" * 60)
++++++++++++++-+
++++++++++++++-+
++++++++++++++-+
++++++++++++++-+
++++++++++++++-+
++++++++++++++-+
+++++++++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++++++-    
++++++++++++++++    # 전체 샘플 WER 평균 계산
++++++++++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
++++++++++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
++++++++++++++ +
++++++++++++++++    print("\n📊 전체 평균 WER")
++++++++++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
++++++++++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
++++++++++++++ +
+++++++++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++++++     for sample in top5:
+++++++++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++++++++++index e69de29..0342039 100644
+++++++++++++++--- a/tmp.ipynb
++++++++++++++++++ b/tmp.ipynb
+++++++++++++++@@ -0,0 +1,272 @@
++++++++++++++++{
++++++++++++++++ "cells": [
++++++++++++++++  {
++++++++++++++++   "cell_type": "code",
++++++++++++++++   "execution_count": 2,
++++++++++++++++   "metadata": {},
++++++++++++++++   "outputs": [],
++++++++++++++++   "source": [
++++++++++++++++    "import torch\n",
++++++++++++++++    "\n",
++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++++++++++   ]
++++++++++++++++  },
++++++++++++++++  {
++++++++++++++++   "cell_type": "code",
++++++++++++++++   "execution_count": 3,
++++++++++++++++   "metadata": {},
++++++++++++++++   "outputs": [
++++++++++++++++    {
++++++++++++++++     "name": "stdout",
++++++++++++++++     "output_type": "stream",
++++++++++++++++     "text": [
++++++++++++++++      "Model is a state_dict.\n",
++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++++++++++     ]
++++++++++++++++    }
++++++++++++++++   ],
++++++++++++++++   "source": [
++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++++++++++   ]
++++++++++++++++  },
++++++++++++++++  {
++++++++++++++++   "cell_type": "code",
++++++++++++++++   "execution_count": 7,
++++++++++++++++   "metadata": {},
++++++++++++++++   "outputs": [],
++++++++++++++++   "source": [
++++++++++++++++    "import torch\n",
++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++++++++++    "import torch.nn.functional as F\n"
++++++++++++++++   ]
++++++++++++++++  },
++++++++++++++++  {
++++++++++++++++   "cell_type": "code",
++++++++++++++++   "execution_count": 1,
++++++++++++++++   "metadata": {},
++++++++++++++++   "outputs": [
++++++++++++++++    {
++++++++++++++++     "name": "stderr",
++++++++++++++++     "output_type": "stream",
++++++++++++++++     "text": [
++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++++++++++     ]
++++++++++++++++    }
++++++++++++++++   ],
++++++++++++++++   "source": [
++++++++++++++++    "import torch\n",
++++++++++++++++    "import torch.nn as nn\n",
++++++++++++++++    "import torch.nn.functional as F\n",
++++++++++++++++    "import torchvision.models as models\n",
++++++++++++++++    "import numpy as np\n",
++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++++++++++    "from modules.criterions import SeqKD\n",
++++++++++++++++    "import utils\n",
++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++++++++++    "class Identity(nn.Module):\n",
++++++++++++++++    "    def __init__(self):\n",
++++++++++++++++    "        super(Identity, self).__init__()\n",
++++++++++++++++    "\n",
++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++    "        return x\n",
++++++++++++++++    "\n",
++++++++++++++++    "# L2 정규화 선형 레이어\n",
++++++++++++++++    "class NormLinear(nn.Module):\n",
++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++++++++++    "        super(NormLinear, self).__init__()\n",
++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++++++++++    "\n",
++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++++++++++    "        return outputs\n",
++++++++++++++++    "\n",
++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++++++++++    "class SLRModel(nn.Module):\n",
++++++++++++++++    "    def __init__(\n",
++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++++++++++    "    ):\n",
++++++++++++++++    "        super(SLRModel, self).__init__()\n",
++++++++++++++++    "        self.decoder = None\n",
++++++++++++++++    "        self.loss = dict()\n",
++++++++++++++++    "        self.criterion_init()\n",
++++++++++++++++    "        self.num_classes = num_classes\n",
++++++++++++++++    "        self.loss_weights = loss_weights\n",
++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++++++++++    "\n",
++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++++++++++    "                                   hidden_size=hidden_size,\n",
++++++++++++++++    "                                   conv_type=conv_type,\n",
++++++++++++++++    "                                   use_bn=use_bn,\n",
++++++++++++++++    "                                   num_classes=num_classes)\n",
++++++++++++++++    "\n",
++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++++++++++    "\n",
++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++++++++++    "\n",
++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++++++++++    "        if weight_norm:\n",
++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++    "        else:\n",
++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++    "\n",
++++++++++++++++    "        # Classifier 공유 여부\n",
++++++++++++++++    "        if share_classifier:\n",
++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++++++++++    "\n",
++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++++++++++    "        if len(x.shape) == 5:\n",
++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++++++++++    "        else:\n",
++++++++++++++++    "            framewise = x\n",
++++++++++++++++    "\n",
++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++++++++++    "\n",
++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++++++++++    "\n",
++++++++++++++++    "        # 최종 Classifier 적용\n",
++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++++++++++    "\n",
++++++++++++++++    "        # Inference 모드에서 Decoding\n",
++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++++++++++    "\n",
++++++++++++++++    "        return {\n",
++++++++++++++++    "            \"framewise_features\": framewise,\n",
++++++++++++++++    "            \"visual_features\": x,\n",
++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++++++++++    "            \"feat_len\": lgt,\n",
++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++++++++++    "            \"sequence_logits\": outputs,\n",
++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++++++++++    "            \"recognized_sents\": pred,\n",
++++++++++++++++    "        }\n",
++++++++++++++++    "\n",
++++++++++++++++    "    def criterion_init(self):\n",
++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++++++++++    "        return self.loss\n"
++++++++++++++++   ]
++++++++++++++++  },
++++++++++++++++  {
++++++++++++++++   "cell_type": "code",
++++++++++++++++   "execution_count": 6,
++++++++++++++++   "metadata": {},
++++++++++++++++   "outputs": [
++++++++++++++++    {
++++++++++++++++     "ename": "KeyError",
++++++++++++++++     "evalue": "'dataset_info'",
++++++++++++++++     "output_type": "error",
++++++++++++++++     "traceback": [
++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++++++++++     ]
++++++++++++++++    }
++++++++++++++++   ],
++++++++++++++++   "source": [
++++++++++++++++    "import os\n",
++++++++++++++++    "import numpy as np\n",
++++++++++++++++    "import yaml\n",
++++++++++++++++    "\n",
++++++++++++++++    "# 환경 변수 설정\n",
++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++++++++++    "\n",
++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++++++++++    "with open(config_path, \"r\") as f:\n",
++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++++++++++    "\n",
++++++++++++++++    "# ✅ gloss_dict 로드\n",
++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++++++++++    "\n",
++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++++++++++   ]
++++++++++++++++  },
++++++++++++++++  {
++++++++++++++++   "cell_type": "code",
++++++++++++++++   "execution_count": 5,
++++++++++++++++   "metadata": {},
++++++++++++++++   "outputs": [
++++++++++++++++    {
++++++++++++++++     "ename": "AttributeError",
++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++++++++++     "output_type": "error",
++++++++++++++++     "traceback": [
++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++++++++++     ]
++++++++++++++++    }
++++++++++++++++   ],
++++++++++++++++   "source": [
++++++++++++++++    "import torch\n",
++++++++++++++++    "\n",
++++++++++++++++    "# 모델 불러오기\n",
++++++++++++++++    "model = SLRModel(\n",
++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++++++++++    ")\n",
++++++++++++++++    "\n",
++++++++++++++++    "# 저장된 가중치 로드\n",
++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++++++++++    "\n",
++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++++++++++    "if isinstance(state_dict, dict):\n",
++++++++++++++++    "    model.load_state_dict(state_dict)\n",
++++++++++++++++    "\n",
++++++++++++++++    "# 모델을 평가 모드로 설정\n",
++++++++++++++++    "model.eval()\n"
++++++++++++++++   ]
++++++++++++++++  }
++++++++++++++++ ],
++++++++++++++++ "metadata": {
++++++++++++++++  "kernelspec": {
++++++++++++++++   "display_name": "3.9.13",
++++++++++++++++   "language": "python",
++++++++++++++++   "name": "python3"
++++++++++++++++  },
++++++++++++++++  "language_info": {
++++++++++++++++   "codemirror_mode": {
++++++++++++++++    "name": "ipython",
++++++++++++++++    "version": 3
++++++++++++++++   },
++++++++++++++++   "file_extension": ".py",
++++++++++++++++   "mimetype": "text/x-python",
++++++++++++++++   "name": "python",
++++++++++++++++   "nbconvert_exporter": "python",
++++++++++++++++   "pygments_lexer": "ipython3",
++++++++++++++++   "version": "3.9.13"
++++++++++++++++  }
++++++++++++++++ },
++++++++++++++++ "nbformat": 4,
++++++++++++++++ "nbformat_minor": 2
++++++++++++++++}
+++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++++++++++index cb157af..c502b5d 100644
+++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++++++++++index 3877729..ac8dab6 100644
+++++++++++++++--- a/utils/decode.py
++++++++++++++++++ b/utils/decode.py
+++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
+++++++++++++++ import numpy as np
+++++++++++++++ from itertools import groupby
+++++++++++++++ import torch.nn.functional as F
++++++++++++++++import matplotlib.pyplot as plt
++++++++++++++ +
++++++++++++++++# ⬇ 프레임 길이 저장용 전역 리스트
++++++++++++++++frame_lengths = []
++++++++++++++ +
++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
++++++++++++++ +
++++++++++++++++    length = int(vid_lgt[batch_idx].item())
++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++++++++++    x = np.arange(length)
++++++++++++++ +
++++++++++++++++    plt.figure(figsize=(15, 4))
++++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++++++++++++ +
++++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++++++++++++    if len(blank_indices) > 0:
++++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++++++++++++ +
++++++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++++++++++++++    if sample_id:
++++++++++++++++        title_str += f"\nSample: {sample_id}"
++++++++++++++++    plt.title(title_str)
++++++++++++++++    plt.xlabel("Time Step")
++++++++++++++++    plt.ylabel("Gloss ID")
++++++++++++++++    plt.yticks(np.unique(pred_seq))
++++++++++++++++    plt.grid(True)
++++++++++++++++    plt.legend()
++++++++++++++++    plt.tight_layout()
++++++++++++++++    plt.show()
+++++++++++++++ 
+++++++++++++++ 
+++++++++++++++ class Decode(object):
+++++++++++++++@@ -16,35 +48,27 @@ class Decode(object):
+++++++++++++++         self.search_mode = search_mode
+++++++++++++++         self.blank_id = blank_id
+++++++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+++++++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+++++++++++++++-                                                    num_processes=10)
++++++++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+++++++++++++++ 
+++++++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+++++++++++++++         if not batch_first:
+++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
++++++++++++++ +
++++++++++++++++        # ⬇ 프레임 길이 수집
++++++++++++++++        global frame_lengths
++++++++++++++++        for i in range(vid_lgt.size(0)):
++++++++++++++++            frame_lengths.append(int(vid_lgt[i].item()))
++++++++++++++ +
++++++++++++++++        # sample_id가 존재하면 시각화
++++++++++++++++        sample_id = sample_ids[0] if sample_ids else None
++++++++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
++++++++++++++ +
+++++++++++++++         if self.search_mode == "max":
+++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++++++++++         else:
+++++++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
+++++++++++++++ 
+++++++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+++++++++++++++-        '''
+++++++++++++++-        CTCBeamDecoder Shape:
+++++++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+++++++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+++++++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+++++++++++++++-                          timesteps (B, N_beams)
+++++++++++++++-                          out_lens (B, N_beams)
+++++++++++++++-        '''
+++++++++++++++-
+++++++++++++++-        index_list = torch.argmax(nn_output.cpu(), axis=2)
+++++++++++++++-        batchsize, lgt = index_list.shape
+++++++++++++++-        blank_rate =[]
+++++++++++++++-        for batch_idx in range(batchsize):
+++++++++++++++-            group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+++++++++++++++-            blank_rate.append(group_result)
+++++++++++++++-
+++++++++++++++-
+++++++++++++++         if not probs:
+++++++++++++++             nn_output = nn_output.softmax(-1).cpu()
+++++++++++++++         vid_lgt = vid_lgt.cpu()
+++++++++++++++@@ -54,9 +78,7 @@ class Decode(object):
+++++++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+++++++++++++++             if len(first_result) != 0:
+++++++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+++++++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+++++++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+++++++++++++++-                             enumerate(first_result)])
++++++++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+++++++++++++++         return ret_list
+++++++++++++++ 
+++++++++++++++     def MaxDecode(self, nn_output, vid_lgt):
+++++++++++++++@@ -65,12 +87,34 @@ class Decode(object):
+++++++++++++++         ret_list = []
+++++++++++++++         for batch_idx in range(batchsize):
+++++++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+++++++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++++++++++++++++            filtered = [x for x in group_result if x != self.blank_id]
+++++++++++++++             if len(filtered) > 0:
+++++++++++++++                 max_result = torch.stack(filtered)
+++++++++++++++                 max_result = [x[0] for x in groupby(max_result)]
+++++++++++++++             else:
+++++++++++++++                 max_result = filtered
+++++++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+++++++++++++++-                             enumerate(max_result)])
++++++++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++++++++++++++         return ret_list
++++++++++++++ +
++++++++++++++ +
++++++++++++++++# ✅ 테스트 루프 끝에서 프레임 길이 분석 (예: seq_eval() 마지막에)
++++++++++++++++def analyze_frame_lengths():
++++++++++++++++    if not frame_lengths:
++++++++++++++++        print("⚠ 분석할 frame_lengths가 없습니다.")
++++++++++++++++        return
++++++++++++++ +
++++++++++++++++    print("\n📊 Test Video Frame Length Analysis:")
++++++++++++++++    print(f"- Total samples: {len(frame_lengths)}")
++++++++++++++++    print(f"- Mean length : {np.mean(frame_lengths):.2f}")
++++++++++++++++    print(f"- Min length  : {np.min(frame_lengths)}")
++++++++++++++++    print(f"- Max length  : {np.max(frame_lengths)}")
++++++++++++++ +
++++++++++++++++    # 히스토그램 시각화
++++++++++++++++    plt.figure(figsize=(10, 5))
++++++++++++++++    plt.hist(frame_lengths, bins=20, color='skyblue', edgecolor='black')
++++++++++++++++    plt.title("Test Video Frame Length Distribution")
++++++++++++++++    plt.xlabel("Frame Length")
++++++++++++++++    plt.ylabel("Number of Samples")
++++++++++++++++    plt.grid(True)
++++++++++++++++    plt.tight_layout()
++++++++++++++++    plt.show()
+++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++++++++++index 7d0a2aa..cd66258 100644
+++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++++++++++++++index c31483a..239691c 100644
+++++++++++++++--- a/work_dirt/config.yaml
++++++++++++++++++ b/work_dirt/config.yaml
+++++++++++++++@@ -7,7 +7,7 @@ dataset_info:
+++++++++++++++   evaluation_dir: ./evaluation/slr_eval
+++++++++++++++   evaluation_prefix: phoenix2014-groundtruth
+++++++++++++++ decode_mode: beam
+++++++++++++++-device: your_device
++++++++++++++++device: cuda
+++++++++++++++ dist_url: env://
+++++++++++++++ eval_interval: 1
+++++++++++++++ evaluate_tool: python
+++++++++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
+++++++++++++++index 00c1a0e..10430dc 100644
+++++++++++++++--- a/work_dirt/dev.txt
++++++++++++++++++ b/work_dirt/dev.txt
+++++++++++++++@@ -8,3 +8,7 @@
+++++++++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
+++++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++++++[ Wed Apr  2 14:44:09 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++++++++++index 8a22ece..17d1fbe 100644
+++++++++++++++--- a/work_dirt/dirty.patch
++++++++++++++++++ b/work_dirt/dirty.patch
+++++++++++++++@@ -1,284 +1,5868 @@
+++++++++++++++-diff --git a/README.md b/README.md
+++++++++++++++-index bdbc17f..8cb240b 100644
+++++++++++++++---- a/README.md
+++++++++++++++-+++ b/README.md
+++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++++++++++++- 
+++++++++++++++- 
+++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++++++++++- 
+++++++++++++++- ### Training
+++++++++++++++- 
+++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++++++++++-index bfc1da8..25ffa61 100644
+++++++++++++++---- a/configs/baseline.yaml
+++++++++++++++-+++ b/configs/baseline.yaml
+++++++++++++++-@@ -1,14 +1,14 @@
+++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++++++++++- phase: train
+++++++++++++++--dataset: phoenix2014-T
+++++++++++++++-+dataset: phoenix2014
+++++++++++++++- #CSL-Daily
+++++++++++++++- # dataset: phoenix14-si5
+++++++++++++++- 
+++++++++++++++- work_dir: ./work_dirt/
+++++++++++++++--batch_size: 4
+++++++++++++++-+batch_size: 1
+++++++++++++++- random_seed: 0 
+++++++++++++++--test_batch_size: 4
+++++++++++++++--num_worker: 20
+++++++++++++++-+test_batch_size: 1
+++++++++++++++-+num_worker: 3
+++++++++++++++- device: 0
+++++++++++++++- log_interval: 10000
+++++++++++++++- eval_interval: 1
+++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++++++++++-index 555f4b8..c126e7a 100644
+++++++++++++++---- a/dataset/dataloader_video.py
+++++++++++++++-+++ b/dataset/dataloader_video.py
+++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++++++++++++-+
+++++++++++++++-         print(mode, len(self))
+++++++++++++++-         self.data_aug = self.transform()
+++++++++++++++-         print("")
+++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
+++++++++++++++- 
+++++++++++++++-     def read_video(self, index):
+++++++++++++++--        # load file info
+++++++++++++++-         fi = self.inputs_list[index]
+++++++++++++++-+    
+++++++++++++++-         if 'phoenix' in self.dataset:
+++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++++++++++-+#            print(img_list)
+++++++++++++++-+
+++++++++++++++-+#            print("[LOG] Using phoenix")
+++++++++++++++-+
+++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++++++- 
+++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++++++-+#            print(img_folder)
+++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++++++++++-+#            print(len(img_list))
+++++++++++++++-         elif self.dataset == 'CSL-Daily':
+++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++++++++++-+    
+++++++++++++++-         img_list = sorted(glob.glob(img_folder))
+++++++++++++++-+    
+++++++++++++++-+        if len(img_list) == 0:
+++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++++++++++-+    
+++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++++++++-+    
+++++++++++++++-         label_list = []
+++++++++++++++--        if self.dataset=='phoenix2014':
+++++++++++++++-+        if self.dataset == 'phoenix2014':
+++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++++++++++--        if self.dataset=='phoenix2014-T':
+++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++++++++++-+    
+++++++++++++++-         for phase in fi['label'].split(" "):
+++++++++++++++--            if phase == '':
+++++++++++++++--                continue
+++++++++++++++--            if phase in self.dict.keys():
+++++++++++++++-+            if phase and phase in self.dict:
+++++++++++++++-                 label_list.append(self.dict[phase][0])
+++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++++++++++-+    
+++++++++++++++-+        video = [
+++++++++++++++-+            cv2.cvtColor(
+++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++-+                cv2.COLOR_BGR2RGB
+++++++++++++++-+            )   
+++++++++++++++-+            for img_path in img_list
+++++++++++++++-+        ]
+++++++++++++++-+    
+++++++++++++++-+        return video, label_list, fi
+++++++++++++++- 
+++++++++++++++-     def read_features(self, index):
+++++++++++++++-         # load file info
+++++++++++++++-diff --git a/main.py b/main.py
+++++++++++++++-index 9e68cee..18ac59b 100644
+++++++++++++++---- a/main.py
+++++++++++++++-+++ b/main.py
+++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++++++++++-                 batch_size=batch_size,
+++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++-                 num_workers=self.arg.num_worker,
+++++++++++++++--                pin_memory=True,
+++++++++++++++-+                pin_memory=False,
+++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++-             )
+++++++++++++++-             return loader
+++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++++++++++-                 drop_last=train_flag,
+++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++--                pin_memory=True,
+++++++++++++++-+                pin_memory=False,
+++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++-             )
+++++++++++++++- 
++++++++++++++++diff --git a/__pycache__/seq_scripts.cpython-39.pyc b/__pycache__/seq_scripts.cpython-39.pyc
++++++++++++++++index ffef81f..f4065cc 100644
++++++++++++++++Binary files a/__pycache__/seq_scripts.cpython-39.pyc and b/__pycache__/seq_scripts.cpython-39.pyc differ
++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++++++++++index 7ac0c3b..c89f220 100644
++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++++++++++index 71519fd..b9664e1 100644
++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++++++++++ diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++++++-index 528856d..d8fcaf9 100644
++++++++++++++++index d8fcaf9..77cfc71 100644
+++++++++++++++ --- a/seq_scripts.py
+++++++++++++++ +++ b/seq_scripts.py
+++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++++++++++-     return
+++++++++++++++- 
++++++++++++++++@@ -151,7 +151,14 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++++++         })
+++++++++++++++  
+++++++++++++++-+import csv 
+++++++++++++++-+from jiwer import wer as jiwer_wer
+++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++++++++++-     model.eval()
+++++++++++++++--    results=defaultdict(dict)
+++++++++++++++-+    results = defaultdict(dict)
+++++++++++++++- 
+++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++++++++++-         recoder.record_timer("device")
+++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++++++++++-                 results[inf]['gloss'] = gl
+++++++++++++++-+
+++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++-+
+++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++++++++++--        reg_per = wer_results
+++++++++++++++--    else:
+++++++++++++++--        reg_per = wer_results_con
+++++++++++++++-+
+++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++++++++-+
+++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++++++++++-         f"{work_dir}/{mode}.txt")
+++++++++++++++-+
+++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++++++++++-+        f"{work_dir}/{mode}.txt")
+++++++++++++++-+
+++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++++++++-+
+++++++++++++++-+    rows = []
+++++++++++++++-+    for file_id in results:
+++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++-+
+++++++++++++++-+        rows.append([
+++++++++++++++-+            file_id,
+++++++++++++++-+            gt,
+++++++++++++++-+            conv_pred,
+++++++++++++++-+            f"{conv_wer:.4f}",
+++++++++++++++-+            lstm_pred,
+++++++++++++++-+            f"{lstm_wer:.4f}"
+++++++++++++++-+        ])
+++++++++++++++-+
+++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++++++++++-+        writer = csv.writer(f)
+++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++++++++++-+        writer.writerows(rows)
+++++++++++++++-+
+++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++++++++++-+    sample_wers = []
+++++++++++++++-+    for file_id in results:
+++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++-+    
+++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++-+    
+++++++++++++++-+        sample_wers.append({
+++++++++++++++-+            'file_id': file_id,
+++++++++++++++-+            'gt': gt,
+++++++++++++++-+            'conv_pred': conv_pred,
+++++++++++++++-+            'conv_wer': conv_wer,
+++++++++++++++-+            'lstm_pred': lstm_pred,
+++++++++++++++-+            'lstm_wer': lstm_wer,
+++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++++++++++-+        })
+++++++++++++++-+
+++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++++++-+    
+++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++++++-+    for sample in top5:
+++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++++++-+        print(f"GT   : {sample['gt']}")
+++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++++++++++-+        print("-" * 60)
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
+++++++++++++++-+
++++++++++++++++     top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++++++-    
+++++++++++++++++    # 전체 샘플 WER 평균 계산
+++++++++++++++++    avg_conv_wer = 100 * np.mean([sample['conv_wer'] for sample in sample_wers])
+++++++++++++++++    avg_lstm_wer = 100 * np.mean([sample['lstm_wer'] for sample in sample_wers])
+++++++++++++++ +
+++++++++++++++++    print("\n📊 전체 평균 WER")
+++++++++++++++++    print(f"- Conv 방식 평균 WER: {avg_conv_wer:.2f}%")
+++++++++++++++++    print(f"- LSTM 방식 평균 WER: {avg_lstm_wer:.2f}%")
+++++++++++++++ +
++++++++++++++++     print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++++++     for sample in top5:
++++++++++++++++         print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++++++++++index e69de29..0342039 100644
++++++++++++++++--- a/tmp.ipynb
+++++++++++++++++++ b/tmp.ipynb
++++++++++++++++@@ -0,0 +1,272 @@
+++++++++++++++++{
+++++++++++++++++ "cells": [
+++++++++++++++++  {
+++++++++++++++++   "cell_type": "code",
+++++++++++++++++   "execution_count": 2,
+++++++++++++++++   "metadata": {},
+++++++++++++++++   "outputs": [],
+++++++++++++++++   "source": [
+++++++++++++++++    "import torch\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++++++++++   ]
+++++++++++++++++  },
+++++++++++++++++  {
+++++++++++++++++   "cell_type": "code",
+++++++++++++++++   "execution_count": 3,
+++++++++++++++++   "metadata": {},
+++++++++++++++++   "outputs": [
+++++++++++++++++    {
+++++++++++++++++     "name": "stdout",
+++++++++++++++++     "output_type": "stream",
+++++++++++++++++     "text": [
+++++++++++++++++      "Model is a state_dict.\n",
+++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++++++++++     ]
+++++++++++++++++    }
+++++++++++++++++   ],
+++++++++++++++++   "source": [
+++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++++++++++   ]
+++++++++++++++++  },
+++++++++++++++++  {
+++++++++++++++++   "cell_type": "code",
+++++++++++++++++   "execution_count": 7,
+++++++++++++++++   "metadata": {},
+++++++++++++++++   "outputs": [],
+++++++++++++++++   "source": [
+++++++++++++++++    "import torch\n",
+++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++++++++++    "import torch.nn.functional as F\n"
+++++++++++++++++   ]
+++++++++++++++++  },
+++++++++++++++++  {
+++++++++++++++++   "cell_type": "code",
+++++++++++++++++   "execution_count": 1,
+++++++++++++++++   "metadata": {},
+++++++++++++++++   "outputs": [
+++++++++++++++++    {
+++++++++++++++++     "name": "stderr",
+++++++++++++++++     "output_type": "stream",
+++++++++++++++++     "text": [
+++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++++++++++     ]
+++++++++++++++++    }
+++++++++++++++++   ],
+++++++++++++++++   "source": [
+++++++++++++++++    "import torch\n",
+++++++++++++++++    "import torch.nn as nn\n",
+++++++++++++++++    "import torch.nn.functional as F\n",
+++++++++++++++++    "import torchvision.models as models\n",
+++++++++++++++++    "import numpy as np\n",
+++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++++++++++    "from modules.criterions import SeqKD\n",
+++++++++++++++++    "import utils\n",
+++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++++++++++    "class Identity(nn.Module):\n",
+++++++++++++++++    "    def __init__(self):\n",
+++++++++++++++++    "        super(Identity, self).__init__()\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++    "        return x\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# L2 정규화 선형 레이어\n",
+++++++++++++++++    "class NormLinear(nn.Module):\n",
+++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++++++++++    "        return outputs\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++++++++++    "class SLRModel(nn.Module):\n",
+++++++++++++++++    "    def __init__(\n",
+++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++++++++++    "    ):\n",
+++++++++++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++++++++++    "        self.decoder = None\n",
+++++++++++++++++    "        self.loss = dict()\n",
+++++++++++++++++    "        self.criterion_init()\n",
+++++++++++++++++    "        self.num_classes = num_classes\n",
+++++++++++++++++    "        self.loss_weights = loss_weights\n",
+++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++++++++++    "                                   conv_type=conv_type,\n",
+++++++++++++++++    "                                   use_bn=use_bn,\n",
+++++++++++++++++    "                                   num_classes=num_classes)\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++++++++++    "        if weight_norm:\n",
+++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++    "        else:\n",
+++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        # Classifier 공유 여부\n",
+++++++++++++++++    "        if share_classifier:\n",
+++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++++++++++    "        if len(x.shape) == 5:\n",
+++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++++++++++    "        else:\n",
+++++++++++++++++    "            framewise = x\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        # 최종 Classifier 적용\n",
+++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "        return {\n",
+++++++++++++++++    "            \"framewise_features\": framewise,\n",
+++++++++++++++++    "            \"visual_features\": x,\n",
+++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++++++++++    "            \"feat_len\": lgt,\n",
+++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++++++++++    "            \"recognized_sents\": pred,\n",
+++++++++++++++++    "        }\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "    def criterion_init(self):\n",
+++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++++++++++    "        return self.loss\n"
+++++++++++++++++   ]
+++++++++++++++++  },
+++++++++++++++++  {
+++++++++++++++++   "cell_type": "code",
+++++++++++++++++   "execution_count": 6,
+++++++++++++++++   "metadata": {},
+++++++++++++++++   "outputs": [
+++++++++++++++++    {
+++++++++++++++++     "ename": "KeyError",
+++++++++++++++++     "evalue": "'dataset_info'",
+++++++++++++++++     "output_type": "error",
+++++++++++++++++     "traceback": [
+++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++++++++++     ]
+++++++++++++++++    }
+++++++++++++++++   ],
+++++++++++++++++   "source": [
+++++++++++++++++    "import os\n",
+++++++++++++++++    "import numpy as np\n",
+++++++++++++++++    "import yaml\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# 환경 변수 설정\n",
+++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# ✅ gloss_dict 로드\n",
+++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++++++++++   ]
+++++++++++++++++  },
+++++++++++++++++  {
+++++++++++++++++   "cell_type": "code",
+++++++++++++++++   "execution_count": 5,
+++++++++++++++++   "metadata": {},
+++++++++++++++++   "outputs": [
+++++++++++++++++    {
+++++++++++++++++     "ename": "AttributeError",
+++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++++++++++     "output_type": "error",
+++++++++++++++++     "traceback": [
+++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++++++++++     ]
+++++++++++++++++    }
+++++++++++++++++   ],
+++++++++++++++++   "source": [
+++++++++++++++++    "import torch\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# 모델 불러오기\n",
+++++++++++++++++    "model = SLRModel(\n",
+++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++++++++++    ")\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# 저장된 가중치 로드\n",
+++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++++++++++    "if isinstance(state_dict, dict):\n",
+++++++++++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++++++++++    "\n",
+++++++++++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++++++++++    "model.eval()\n"
+++++++++++++++++   ]
+++++++++++++++++  }
+++++++++++++++++ ],
+++++++++++++++++ "metadata": {
+++++++++++++++++  "kernelspec": {
+++++++++++++++++   "display_name": "3.9.13",
+++++++++++++++++   "language": "python",
+++++++++++++++++   "name": "python3"
+++++++++++++++++  },
+++++++++++++++++  "language_info": {
+++++++++++++++++   "codemirror_mode": {
+++++++++++++++++    "name": "ipython",
+++++++++++++++++    "version": 3
+++++++++++++++++   },
+++++++++++++++++   "file_extension": ".py",
+++++++++++++++++   "mimetype": "text/x-python",
+++++++++++++++++   "name": "python",
+++++++++++++++++   "nbconvert_exporter": "python",
+++++++++++++++++   "pygments_lexer": "ipython3",
+++++++++++++++++   "version": "3.9.13"
+++++++++++++++++  }
+++++++++++++++++ },
+++++++++++++++++ "nbformat": 4,
+++++++++++++++++ "nbformat_minor": 2
+++++++++++++++++}
++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++++++++++index cb157af..6e543f8 100644
++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++++++++++index 3877729..006ac5d 100644
++++++++++++++++--- a/utils/decode.py
+++++++++++++++++++ b/utils/decode.py
++++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++++++++++++ import numpy as np
++++++++++++++++ from itertools import groupby
++++++++++++++++ import torch.nn.functional as F
+++++++++++++++++import torch
+++++++++++++++++import matplotlib.pyplot as plt
+++++++++++++++++import numpy as np
+++++++++++++++ +
+++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
+++++++++++++++ +
+++++++++++++++++    length = int(vid_lgt[batch_idx].item())
+++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++++++++++    x = np.arange(length)
+++++++++++++++ +
+++++++++++++++++    plt.figure(figsize=(15, 4))
+++++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++++++++++++ +
+++++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++++++++++++    if len(blank_indices) > 0:
+++++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++++++++++++ +
+++++++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++++++++++++++++    if sample_id:
+++++++++++++++++        title_str += f"\nSample: {sample_id}"
+++++++++++++++++    plt.title(title_str)
+++++++++++++++++    plt.xlabel("Time Step")
+++++++++++++++++    plt.ylabel("Gloss ID")
+++++++++++++++++    plt.yticks(np.unique(pred_seq))
+++++++++++++++++    plt.grid(True)
+++++++++++++++++    plt.legend()
+++++++++++++++++    plt.tight_layout()
+++++++++++++++++    plt.show()
+++++++++++++++ +
++++++++++++++++ 
++++++++++++++++ 
++++++++++++++++ class Decode(object):
++++++++++++++++@@ -16,35 +48,29 @@ class Decode(object):
++++++++++++++++         self.search_mode = search_mode
++++++++++++++++         self.blank_id = blank_id
++++++++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++++++++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++++++++++++++++-                                                    num_processes=10)
+++++++++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++++++++++++++++ 
++++++++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++++++++++++++++         if not batch_first:
++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++++++++++ +
+++++++++++++++++        # sample_id가 존재하면 시각화에 함께 넘김
+++++++++++++++++        sample_id = sample_ids[0] if sample_ids else None
+++++++++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+++++++++++++++ +
++++++++++++++++         if self.search_mode == "max":
++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++++++++++         else:
++++++++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
++++++++++++++++ 
++++++++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++++++++++++++++-        '''
++++++++++++++++-        CTCBeamDecoder Shape:
++++++++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++++++++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++++++++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++++++++++++++++-                          timesteps (B, N_beams)
++++++++++++++++-                          out_lens (B, N_beams)
++++++++++++++++-        '''
++++++++++++++++-
++++++++++++++++         index_list = torch.argmax(nn_output.cpu(), axis=2)
++++++++++++++++         batchsize, lgt = index_list.shape
++++++++++++++++-        blank_rate =[]
+++++++++++++++++        blank_rate = []
++++++++++++++++         for batch_idx in range(batchsize):
++++++++++++++++             group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++++++++++++++++             blank_rate.append(group_result)
+++++++++++++++  
+++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++++++++++++-
++++++++++++++++         if not probs:
++++++++++++++++             nn_output = nn_output.softmax(-1).cpu()
++++++++++++++++         vid_lgt = vid_lgt.cpu()
++++++++++++++++@@ -54,9 +80,7 @@ class Decode(object):
++++++++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++++++++++++++++             if len(first_result) != 0:
++++++++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++++++++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++++++++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++++++++++++++++-                             enumerate(first_result)])
+++++++++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++++++++++++++++         return ret_list
++++++++++++++++ 
++++++++++++++++     def MaxDecode(self, nn_output, vid_lgt):
++++++++++++++++@@ -65,12 +89,11 @@ class Decode(object):
++++++++++++++++         ret_list = []
++++++++++++++++         for batch_idx in range(batchsize):
++++++++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++++++++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++++++++++++++++            filtered = [x for x in group_result if x != self.blank_id]
++++++++++++++++             if len(filtered) > 0:
++++++++++++++++                 max_result = torch.stack(filtered)
++++++++++++++++                 max_result = [x[0] for x in groupby(max_result)]
++++++++++++++++             else:
++++++++++++++++                 max_result = filtered
++++++++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++++++++++++++++-                             enumerate(max_result)])
++++++++++++++++-        return ret_list
+++++++++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++++++++++++++++        return ret_list
++++++++++++++++\ No newline at end of file
++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++++++++++index 7d0a2aa..cd66258 100644
++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++++++++++++++index c31483a..239691c 100644
++++++++++++++++--- a/work_dirt/config.yaml
+++++++++++++++++++ b/work_dirt/config.yaml
++++++++++++++++@@ -7,7 +7,7 @@ dataset_info:
++++++++++++++++   evaluation_dir: ./evaluation/slr_eval
++++++++++++++++   evaluation_prefix: phoenix2014-groundtruth
++++++++++++++++ decode_mode: beam
++++++++++++++++-device: your_device
+++++++++++++++++device: cuda
++++++++++++++++ dist_url: env://
++++++++++++++++ eval_interval: 1
++++++++++++++++ evaluate_tool: python
++++++++++++++++diff --git a/work_dirt/dev.txt b/work_dirt/dev.txt
++++++++++++++++index 00c1a0e..2463106 100644
++++++++++++++++--- a/work_dirt/dev.txt
+++++++++++++++++++ b/work_dirt/dev.txt
++++++++++++++++@@ -8,3 +8,5 @@
++++++++++++++++ [ Fri Mar 21 16:16:00 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1274  ins:2.1680, del:5.9801
++++++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
++++++++++++++++ [ Fri Mar 21 17:53:14 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
+++++++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. Conv wer: 18.5976  ins:1.4228, del:7.6220
+++++++++++++++++[ Wed Apr  2 14:08:37 2025 ] 	Epoch: 6667 dev done. LSTM wer: 17.1748  ins:1.0163, del:6.6057
++++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++++++++++++index 8a22ece..affdac0 100644
++++++++++++++++--- a/work_dirt/dirty.patch
+++++++++++++++++++ b/work_dirt/dirty.patch
++++++++++++++++@@ -1,284 +1,5082 @@
++++++++++++++++-diff --git a/README.md b/README.md
++++++++++++++++-index bdbc17f..8cb240b 100644
++++++++++++++++---- a/README.md
++++++++++++++++-+++ b/README.md
++++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++++++++++- 
++++++++++++++++- 
++++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++++++++++++- 
++++++++++++++++- ### Training
++++++++++++++++- 
++++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++++++++++++-index bfc1da8..25ffa61 100644
++++++++++++++++---- a/configs/baseline.yaml
++++++++++++++++-+++ b/configs/baseline.yaml
++++++++++++++++-@@ -1,14 +1,14 @@
++++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++++++++++++- phase: train
++++++++++++++++--dataset: phoenix2014-T
++++++++++++++++-+dataset: phoenix2014
++++++++++++++++- #CSL-Daily
++++++++++++++++- # dataset: phoenix14-si5
++++++++++++++++- 
++++++++++++++++- work_dir: ./work_dirt/
++++++++++++++++--batch_size: 4
++++++++++++++++-+batch_size: 1
++++++++++++++++- random_seed: 0 
++++++++++++++++--test_batch_size: 4
++++++++++++++++--num_worker: 20
++++++++++++++++-+test_batch_size: 1
++++++++++++++++-+num_worker: 3
++++++++++++++++- device: 0
++++++++++++++++- log_interval: 10000
++++++++++++++++- eval_interval: 1
++++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++++++++++++-index 555f4b8..c126e7a 100644
++++++++++++++++---- a/dataset/dataloader_video.py
++++++++++++++++-+++ b/dataset/dataloader_video.py
++++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++++++++++-+
++++++++++++++++-         print(mode, len(self))
++++++++++++++++-         self.data_aug = self.transform()
++++++++++++++++-         print("")
++++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++++++- 
++++++++++++++++-     def read_video(self, index):
++++++++++++++++--        # load file info
++++++++++++++++-         fi = self.inputs_list[index]
++++++++++++++++-+    
++++++++++++++++-         if 'phoenix' in self.dataset:
++++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++++++++++++-+#            print(img_list)
++++++++++++++++-+
++++++++++++++++-+#            print("[LOG] Using phoenix")
++++++++++++++++-+
++++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++++++ - 
+++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++++++++++-diff --git a/slr_network.py b/slr_network.py
+++++++++++++++-index 45295cb..ede70cf 100644
+++++++++++++++---- a/slr_network.py
+++++++++++++++-+++ b/slr_network.py
+++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++++++++-+#            print(img_folder)
++++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++++++++++++-+#            print(len(img_list))
++++++++++++++++-         elif self.dataset == 'CSL-Daily':
++++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++++++++++++-+    
++++++++++++++++-         img_list = sorted(glob.glob(img_folder))
++++++++++++++++-+    
++++++++++++++++-+        if len(img_list) == 0:
++++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++++++++++++-+    
++++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++++++++++++++-+    
++++++++++++++++-         label_list = []
++++++++++++++++--        if self.dataset=='phoenix2014':
++++++++++++++++-+        if self.dataset == 'phoenix2014':
++++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++++++++++++--        if self.dataset=='phoenix2014-T':
++++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
++++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++++++++++++-+    
++++++++++++++++-         for phase in fi['label'].split(" "):
++++++++++++++++--            if phase == '':
++++++++++++++++--                continue
++++++++++++++++--            if phase in self.dict.keys():
++++++++++++++++-+            if phase and phase in self.dict:
++++++++++++++++-                 label_list.append(self.dict[phase][0])
++++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++++++++++++-+    
++++++++++++++++-+        video = [
++++++++++++++++-+            cv2.cvtColor(
++++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++++-+                cv2.COLOR_BGR2RGB
++++++++++++++++-+            )   
++++++++++++++++-+            for img_path in img_list
++++++++++++++++-+        ]
++++++++++++++++-+    
++++++++++++++++-+        return video, label_list, fi
++++++++++++++++- 
++++++++++++++++-     def read_features(self, index):
++++++++++++++++-         # load file info
++++++++++++++++-diff --git a/main.py b/main.py
++++++++++++++++-index 9e68cee..18ac59b 100644
++++++++++++++++---- a/main.py
++++++++++++++++-+++ b/main.py
++++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
++++++++++++++++-                 batch_size=batch_size,
++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++++-                 num_workers=self.arg.num_worker,
++++++++++++++++--                pin_memory=True,
++++++++++++++++-+                pin_memory=False,
++++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++++-             )
++++++++++++++++-             return loader
++++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
++++++++++++++++-                 drop_last=train_flag,
++++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++++--                pin_memory=True,
++++++++++++++++-+                pin_memory=False,
++++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++++-             )
++++++++++++++++- 
++++++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++++++++-index 528856d..d8fcaf9 100644
++++++++++++++++---- a/seq_scripts.py
++++++++++++++++-+++ b/seq_scripts.py
++++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++++++++++++-     return
++++++++++++++++- 
++++++++++++++++- 
++++++++++++++++-+import csv 
++++++++++++++++-+from jiwer import wer as jiwer_wer
++++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++++++++++++-     model.eval()
++++++++++++++++--    results=defaultdict(dict)
++++++++++++++++-+    results = defaultdict(dict)
++++++++++++++++- 
++++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++++++++++++-         recoder.record_timer("device")
++++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
++++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++++++++++++-                 results[inf]['gloss'] = gl
++++++++++++++++-+
++++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++++-+
++++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++++++++++++--        reg_per = wer_results
++++++++++++++++--    else:
++++++++++++++++--        reg_per = wer_results_con
++++++++++++++++-+
++++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++++++++++-+
++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++++++++++++-         f"{work_dir}/{mode}.txt")
++++++++++++++++-+
++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++++++++++++-+        f"{work_dir}/{mode}.txt")
++++++++++++++++-+
++++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
++++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++++++++++++++++-+
++++++++++++++++-+    rows = []
++++++++++++++++-+    for file_id in results:
++++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++++-+
++++++++++++++++-+        rows.append([
++++++++++++++++-+            file_id,
++++++++++++++++-+            gt,
++++++++++++++++-+            conv_pred,
++++++++++++++++-+            f"{conv_wer:.4f}",
++++++++++++++++-+            lstm_pred,
++++++++++++++++-+            f"{lstm_wer:.4f}"
++++++++++++++++-+        ])
++++++++++++++++-+
++++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++++++++++++-+        writer = csv.writer(f)
++++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++++++++++++-+        writer.writerows(rows)
++++++++++++++++-+
++++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
++++++++++++++++-+    sample_wers = []
++++++++++++++++-+    for file_id in results:
++++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++++-+    
++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++++-+    
++++++++++++++++-+        sample_wers.append({
++++++++++++++++-+            'file_id': file_id,
++++++++++++++++-+            'gt': gt,
++++++++++++++++-+            'conv_pred': conv_pred,
++++++++++++++++-+            'conv_wer': conv_wer,
++++++++++++++++-+            'lstm_pred': lstm_pred,
++++++++++++++++-+            'lstm_wer': lstm_wer,
++++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++++++++++++-+        })
++++++++++++++++-+
++++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++++++-+    
++++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++++++-+    for sample in top5:
++++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++++++-+        print(f"GT   : {sample['gt']}")
++++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++++++++++++-+        print("-" * 60)
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
++++++++++++++++-+
+++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++++++++++++index 7ac0c3b..c89f220 100644
+++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++++++++++++index 71519fd..b9664e1 100644
+++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++++++++++++index e69de29..0342039 100644
+++++++++++++++++--- a/tmp.ipynb
++++++++++++++++++++ b/tmp.ipynb
+++++++++++++++++@@ -0,0 +1,272 @@
++++++++++++++++++{
++++++++++++++++++ "cells": [
++++++++++++++++++  {
++++++++++++++++++   "cell_type": "code",
++++++++++++++++++   "execution_count": 2,
++++++++++++++++++   "metadata": {},
++++++++++++++++++   "outputs": [],
++++++++++++++++++   "source": [
++++++++++++++++++    "import torch\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++++++++++++   ]
++++++++++++++++++  },
++++++++++++++++++  {
++++++++++++++++++   "cell_type": "code",
++++++++++++++++++   "execution_count": 3,
++++++++++++++++++   "metadata": {},
++++++++++++++++++   "outputs": [
++++++++++++++++++    {
++++++++++++++++++     "name": "stdout",
++++++++++++++++++     "output_type": "stream",
++++++++++++++++++     "text": [
++++++++++++++++++      "Model is a state_dict.\n",
++++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++++++++++++     ]
++++++++++++++++++    }
++++++++++++++++++   ],
++++++++++++++++++   "source": [
++++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++++++++++++   ]
++++++++++++++++++  },
++++++++++++++++++  {
++++++++++++++++++   "cell_type": "code",
++++++++++++++++++   "execution_count": 7,
++++++++++++++++++   "metadata": {},
++++++++++++++++++   "outputs": [],
++++++++++++++++++   "source": [
++++++++++++++++++    "import torch\n",
++++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++++++++++++    "import torch.nn.functional as F\n"
++++++++++++++++++   ]
++++++++++++++++++  },
++++++++++++++++++  {
++++++++++++++++++   "cell_type": "code",
++++++++++++++++++   "execution_count": 1,
++++++++++++++++++   "metadata": {},
++++++++++++++++++   "outputs": [
++++++++++++++++++    {
++++++++++++++++++     "name": "stderr",
++++++++++++++++++     "output_type": "stream",
++++++++++++++++++     "text": [
++++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++++++++++++     ]
++++++++++++++++++    }
++++++++++++++++++   ],
++++++++++++++++++   "source": [
++++++++++++++++++    "import torch\n",
++++++++++++++++++    "import torch.nn as nn\n",
++++++++++++++++++    "import torch.nn.functional as F\n",
++++++++++++++++++    "import torchvision.models as models\n",
++++++++++++++++++    "import numpy as np\n",
++++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++++++++++++    "from modules.criterions import SeqKD\n",
++++++++++++++++++    "import utils\n",
++++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++++++++++++    "class Identity(nn.Module):\n",
++++++++++++++++++    "    def __init__(self):\n",
++++++++++++++++++    "        super(Identity, self).__init__()\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++++    "        return x\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# L2 정규화 선형 레이어\n",
++++++++++++++++++    "class NormLinear(nn.Module):\n",
++++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++++++++++++    "        super(NormLinear, self).__init__()\n",
++++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++++++++++++    "        return outputs\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++++++++++++    "class SLRModel(nn.Module):\n",
++++++++++++++++++    "    def __init__(\n",
++++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++++++++++++    "    ):\n",
++++++++++++++++++    "        super(SLRModel, self).__init__()\n",
++++++++++++++++++    "        self.decoder = None\n",
++++++++++++++++++    "        self.loss = dict()\n",
++++++++++++++++++    "        self.criterion_init()\n",
++++++++++++++++++    "        self.num_classes = num_classes\n",
++++++++++++++++++    "        self.loss_weights = loss_weights\n",
++++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++++++++++++    "                                   hidden_size=hidden_size,\n",
++++++++++++++++++    "                                   conv_type=conv_type,\n",
++++++++++++++++++    "                                   use_bn=use_bn,\n",
++++++++++++++++++    "                                   num_classes=num_classes)\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++++++++++++    "        if weight_norm:\n",
++++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++++    "        else:\n",
++++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        # Classifier 공유 여부\n",
++++++++++++++++++    "        if share_classifier:\n",
++++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++++++++++++    "        if len(x.shape) == 5:\n",
++++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++++++++++++    "        else:\n",
++++++++++++++++++    "            framewise = x\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        # 최종 Classifier 적용\n",
++++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        # Inference 모드에서 Decoding\n",
++++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "        return {\n",
++++++++++++++++++    "            \"framewise_features\": framewise,\n",
++++++++++++++++++    "            \"visual_features\": x,\n",
++++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++++++++++++    "            \"feat_len\": lgt,\n",
++++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++++++++++++    "            \"sequence_logits\": outputs,\n",
++++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++++++++++++    "            \"recognized_sents\": pred,\n",
++++++++++++++++++    "        }\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "    def criterion_init(self):\n",
++++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++++++++++++    "        return self.loss\n"
++++++++++++++++++   ]
++++++++++++++++++  },
++++++++++++++++++  {
++++++++++++++++++   "cell_type": "code",
++++++++++++++++++   "execution_count": 6,
++++++++++++++++++   "metadata": {},
++++++++++++++++++   "outputs": [
++++++++++++++++++    {
++++++++++++++++++     "ename": "KeyError",
++++++++++++++++++     "evalue": "'dataset_info'",
++++++++++++++++++     "output_type": "error",
++++++++++++++++++     "traceback": [
++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++++++++++++     ]
++++++++++++++++++    }
++++++++++++++++++   ],
++++++++++++++++++   "source": [
++++++++++++++++++    "import os\n",
++++++++++++++++++    "import numpy as np\n",
++++++++++++++++++    "import yaml\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# 환경 변수 설정\n",
++++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++++++++++++    "with open(config_path, \"r\") as f:\n",
++++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# ✅ gloss_dict 로드\n",
++++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++++++++++++   ]
++++++++++++++++++  },
++++++++++++++++++  {
++++++++++++++++++   "cell_type": "code",
++++++++++++++++++   "execution_count": 5,
++++++++++++++++++   "metadata": {},
++++++++++++++++++   "outputs": [
++++++++++++++++++    {
++++++++++++++++++     "ename": "AttributeError",
++++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++++++++++++     "output_type": "error",
++++++++++++++++++     "traceback": [
++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++++++++++++     ]
++++++++++++++++++    }
++++++++++++++++++   ],
++++++++++++++++++   "source": [
++++++++++++++++++    "import torch\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# 모델 불러오기\n",
++++++++++++++++++    "model = SLRModel(\n",
++++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++++++++++++    ")\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# 저장된 가중치 로드\n",
++++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++++++++++++    "if isinstance(state_dict, dict):\n",
++++++++++++++++++    "    model.load_state_dict(state_dict)\n",
++++++++++++++++++    "\n",
++++++++++++++++++    "# 모델을 평가 모드로 설정\n",
++++++++++++++++++    "model.eval()\n"
++++++++++++++++++   ]
++++++++++++++++++  }
++++++++++++++++++ ],
++++++++++++++++++ "metadata": {
++++++++++++++++++  "kernelspec": {
++++++++++++++++++   "display_name": "3.9.13",
++++++++++++++++++   "language": "python",
++++++++++++++++++   "name": "python3"
++++++++++++++++++  },
++++++++++++++++++  "language_info": {
++++++++++++++++++   "codemirror_mode": {
++++++++++++++++++    "name": "ipython",
++++++++++++++++++    "version": 3
++++++++++++++++++   },
++++++++++++++++++   "file_extension": ".py",
++++++++++++++++++   "mimetype": "text/x-python",
++++++++++++++++++   "name": "python",
++++++++++++++++++   "nbconvert_exporter": "python",
++++++++++++++++++   "pygments_lexer": "ipython3",
++++++++++++++++++   "version": "3.9.13"
++++++++++++++++++  }
++++++++++++++++++ },
++++++++++++++++++ "nbformat": 4,
++++++++++++++++++ "nbformat_minor": 2
++++++++++++++++++}
+++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++++++++++++index cb157af..6e543f8 100644
+++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++++++++++++index 3877729..006ac5d 100644
+++++++++++++++++--- a/utils/decode.py
++++++++++++++++++++ b/utils/decode.py
+++++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
+++++++++++++++++ import numpy as np
+++++++++++++++++ from itertools import groupby
+++++++++++++++++ import torch.nn.functional as F
++++++++++++++++++import torch
++++++++++++++++++import matplotlib.pyplot as plt
++++++++++++++++++import numpy as np
++++++++++++++++ +
++++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
++++++++++++++++ +
++++++++++++++++++    length = int(vid_lgt[batch_idx].item())
++++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++++++++++++    x = np.arange(length)
++++++++++++++++ +
++++++++++++++++++    plt.figure(figsize=(15, 4))
++++++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++++++++++++++ +
++++++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++++++++++++++    if len(blank_indices) > 0:
++++++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++++++++++++++ +
++++++++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++++++++++++++++    if sample_id:
++++++++++++++++++        title_str += f"\nSample: {sample_id}"
++++++++++++++++++    plt.title(title_str)
++++++++++++++++++    plt.xlabel("Time Step")
++++++++++++++++++    plt.ylabel("Gloss ID")
++++++++++++++++++    plt.yticks(np.unique(pred_seq))
++++++++++++++++++    plt.grid(True)
++++++++++++++++++    plt.legend()
++++++++++++++++++    plt.tight_layout()
++++++++++++++++++    plt.show()
++++++++++++++++ +
+++++++++++++++++ 
+++++++++++++++++ 
+++++++++++++++++ class Decode(object):
+++++++++++++++++@@ -16,35 +48,29 @@ class Decode(object):
+++++++++++++++++         self.search_mode = search_mode
+++++++++++++++++         self.blank_id = blank_id
+++++++++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
+++++++++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
+++++++++++++++++-                                                    num_processes=10)
++++++++++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
+++++++++++++++++ 
+++++++++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
+++++++++++++++++         if not batch_first:
+++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
++++++++++++++++ +
++++++++++++++++++        # sample_id가 존재하면 시각화에 함께 넘김
++++++++++++++++++        sample_id = sample_ids[0] if sample_ids else None
++++++++++++++++++        # plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
++++++++++++++++ +
+++++++++++++++++         if self.search_mode == "max":
+++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++++++++++++         else:
+++++++++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
++++++++++++++++  
++++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
+++++++++++++++++-        '''
+++++++++++++++++-        CTCBeamDecoder Shape:
+++++++++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
+++++++++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
+++++++++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
+++++++++++++++++-                          timesteps (B, N_beams)
+++++++++++++++++-                          out_lens (B, N_beams)
+++++++++++++++++-        '''
+++++++++++++++++-
+++++++++++++++++         index_list = torch.argmax(nn_output.cpu(), axis=2)
+++++++++++++++++         batchsize, lgt = index_list.shape
+++++++++++++++++-        blank_rate =[]
++++++++++++++++++        blank_rate = []
+++++++++++++++++         for batch_idx in range(batchsize):
+++++++++++++++++             group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
+++++++++++++++++             blank_rate.append(group_result)
+++++++++++++++++ 
+++++++++++++++++-
+++++++++++++++++         if not probs:
+++++++++++++++++             nn_output = nn_output.softmax(-1).cpu()
+++++++++++++++++         vid_lgt = vid_lgt.cpu()
+++++++++++++++++@@ -54,9 +80,7 @@ class Decode(object):
+++++++++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+++++++++++++++++             if len(first_result) != 0:
+++++++++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
+++++++++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
+++++++++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
+++++++++++++++++-                             enumerate(first_result)])
++++++++++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
+++++++++++++++++         return ret_list
+++++++++++++++++ 
+++++++++++++++++     def MaxDecode(self, nn_output, vid_lgt):
+++++++++++++++++@@ -65,12 +89,11 @@ class Decode(object):
+++++++++++++++++         ret_list = []
+++++++++++++++++         for batch_idx in range(batchsize):
+++++++++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
+++++++++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
++++++++++++++++++            filtered = [x for x in group_result if x != self.blank_id]
+++++++++++++++++             if len(filtered) > 0:
+++++++++++++++++                 max_result = torch.stack(filtered)
+++++++++++++++++                 max_result = [x[0] for x in groupby(max_result)]
+++++++++++++++++             else:
+++++++++++++++++                 max_result = filtered
+++++++++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
+++++++++++++++++-                             enumerate(max_result)])
+++++++++++++++++-        return ret_list
++++++++++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
++++++++++++++++++        return ret_list
+++++++++++++++++\ No newline at end of file
+++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++++++++++++index 7d0a2aa..cd66258 100644
+++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++++++++++++++++index c31483a..239691c 100644
+++++++++++++++++--- a/work_dirt/config.yaml
++++++++++++++++++++ b/work_dirt/config.yaml
+++++++++++++++++@@ -7,7 +7,7 @@ dataset_info:
+++++++++++++++++   evaluation_dir: ./evaluation/slr_eval
+++++++++++++++++   evaluation_prefix: phoenix2014-groundtruth
+++++++++++++++++ decode_mode: beam
+++++++++++++++++-device: your_device
++++++++++++++++++device: cuda
+++++++++++++++++ dist_url: env://
+++++++++++++++++ eval_interval: 1
+++++++++++++++++ evaluate_tool: python
+++++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++++++++++++index 8a22ece..b32b6a4 100644
+++++++++++++++++--- a/work_dirt/dirty.patch
++++++++++++++++++++ b/work_dirt/dirty.patch
+++++++++++++++++@@ -1,284 +1,4353 @@
+++++++++++++++++-diff --git a/README.md b/README.md
+++++++++++++++++-index bdbc17f..8cb240b 100644
+++++++++++++++++---- a/README.md
+++++++++++++++++-+++ b/README.md
+++++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++++++++++++++- 
+++++++++++++++++- 
+++++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++++++++++++- 
+++++++++++++++++- ### Training
+++++++++++++++++- 
+++++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++++++++++++-index bfc1da8..25ffa61 100644
+++++++++++++++++---- a/configs/baseline.yaml
+++++++++++++++++-+++ b/configs/baseline.yaml
+++++++++++++++++-@@ -1,14 +1,14 @@
+++++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++++++++++++- phase: train
+++++++++++++++++--dataset: phoenix2014-T
+++++++++++++++++-+dataset: phoenix2014
+++++++++++++++++- #CSL-Daily
+++++++++++++++++- # dataset: phoenix14-si5
+++++++++++++++++- 
+++++++++++++++++- work_dir: ./work_dirt/
+++++++++++++++++--batch_size: 4
+++++++++++++++++-+batch_size: 1
+++++++++++++++++- random_seed: 0 
+++++++++++++++++--test_batch_size: 4
+++++++++++++++++--num_worker: 20
+++++++++++++++++-+test_batch_size: 1
+++++++++++++++++-+num_worker: 3
+++++++++++++++++- device: 0
+++++++++++++++++- log_interval: 10000
+++++++++++++++++- eval_interval: 1
+++++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++++++++++++-index 555f4b8..c126e7a 100644
+++++++++++++++++---- a/dataset/dataloader_video.py
+++++++++++++++++-+++ b/dataset/dataloader_video.py
+++++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++++++++++++++-+
+++++++++++++++++-         print(mode, len(self))
+++++++++++++++++-         self.data_aug = self.transform()
+++++++++++++++++-         print("")
+++++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++++++ - 
++++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++++++++++++-diff --git a/slr_network.py b/slr_network.py
++++++++++++++++-index 45295cb..ede70cf 100644
++++++++++++++++---- a/slr_network.py
++++++++++++++++-+++ b/slr_network.py
++++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++++++++++++-     def read_video(self, index):
+++++++++++++++++--        # load file info
+++++++++++++++++-         fi = self.inputs_list[index]
+++++++++++++++++-+    
+++++++++++++++++-         if 'phoenix' in self.dataset:
+++++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++++++++++++-+#            print(img_list)
+++++++++++++++++-+
+++++++++++++++++-+#            print("[LOG] Using phoenix")
+++++++++++++++++-+
+++++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++++++++- 
+++++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++++++++-+#            print(img_folder)
+++++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++++++++++++-+#            print(len(img_list))
+++++++++++++++++-         elif self.dataset == 'CSL-Daily':
+++++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++++++++++++-+    
+++++++++++++++++-         img_list = sorted(glob.glob(img_folder))
+++++++++++++++++-+    
+++++++++++++++++-+        if len(img_list) == 0:
+++++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++++++++++++-+    
+++++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++++++++++-+    
+++++++++++++++++-         label_list = []
+++++++++++++++++--        if self.dataset=='phoenix2014':
+++++++++++++++++-+        if self.dataset == 'phoenix2014':
+++++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++++++++++++--        if self.dataset=='phoenix2014-T':
+++++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++++++++++++-+    
+++++++++++++++++-         for phase in fi['label'].split(" "):
+++++++++++++++++--            if phase == '':
+++++++++++++++++--                continue
+++++++++++++++++--            if phase in self.dict.keys():
+++++++++++++++++-+            if phase and phase in self.dict:
+++++++++++++++++-                 label_list.append(self.dict[phase][0])
+++++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++++++++++++-+    
+++++++++++++++++-+        video = [
+++++++++++++++++-+            cv2.cvtColor(
+++++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++++-+                cv2.COLOR_BGR2RGB
+++++++++++++++++-+            )   
+++++++++++++++++-+            for img_path in img_list
+++++++++++++++++-+        ]
+++++++++++++++++-+    
+++++++++++++++++-+        return video, label_list, fi
+++++++++++++++++- 
+++++++++++++++++-     def read_features(self, index):
+++++++++++++++++-         # load file info
+++++++++++++++++-diff --git a/main.py b/main.py
+++++++++++++++++-index 9e68cee..18ac59b 100644
+++++++++++++++++---- a/main.py
+++++++++++++++++-+++ b/main.py
+++++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++++++++++++-                 batch_size=batch_size,
+++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++++-                 num_workers=self.arg.num_worker,
+++++++++++++++++--                pin_memory=True,
+++++++++++++++++-+                pin_memory=False,
+++++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++++-             )
+++++++++++++++++-             return loader
+++++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++++++++++++-                 drop_last=train_flag,
+++++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++++--                pin_memory=True,
+++++++++++++++++-+                pin_memory=False,
+++++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++++-             )
+++++++++++++++++- 
+++++++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++++++++-index 528856d..d8fcaf9 100644
+++++++++++++++++---- a/seq_scripts.py
+++++++++++++++++-+++ b/seq_scripts.py
+++++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++++++++++++-     return
+++++++++++++++++- 
+++++++++++++++++- 
+++++++++++++++++-+import csv 
+++++++++++++++++-+from jiwer import wer as jiwer_wer
+++++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++++++++++++-     model.eval()
+++++++++++++++++--    results=defaultdict(dict)
+++++++++++++++++-+    results = defaultdict(dict)
+++++++++++++++++- 
+++++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++++++++++++-         recoder.record_timer("device")
+++++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++++++++++++-                 results[inf]['gloss'] = gl
+++++++++++++++++-+
+++++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++++-+
+++++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++++++++++++--        reg_per = wer_results
+++++++++++++++++--    else:
+++++++++++++++++--        reg_per = wer_results_con
+++++++++++++++++-+
+++++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++++++++++-+
+++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++++++++++++-         f"{work_dir}/{mode}.txt")
+++++++++++++++++-+
+++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++++++++++++-+        f"{work_dir}/{mode}.txt")
+++++++++++++++++-+
+++++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++++++++++-+
+++++++++++++++++-+    rows = []
+++++++++++++++++-+    for file_id in results:
+++++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++++-+
+++++++++++++++++-+        rows.append([
+++++++++++++++++-+            file_id,
+++++++++++++++++-+            gt,
+++++++++++++++++-+            conv_pred,
+++++++++++++++++-+            f"{conv_wer:.4f}",
+++++++++++++++++-+            lstm_pred,
+++++++++++++++++-+            f"{lstm_wer:.4f}"
+++++++++++++++++-+        ])
+++++++++++++++++-+
+++++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++++++++++++-+        writer = csv.writer(f)
+++++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++++++++++++-+        writer.writerows(rows)
+++++++++++++++++-+
+++++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++++++++++++-+    sample_wers = []
+++++++++++++++++-+    for file_id in results:
+++++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++++-+    
+++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++++-+    
+++++++++++++++++-+        sample_wers.append({
+++++++++++++++++-+            'file_id': file_id,
+++++++++++++++++-+            'gt': gt,
+++++++++++++++++-+            'conv_pred': conv_pred,
+++++++++++++++++-+            'conv_wer': conv_wer,
+++++++++++++++++-+            'lstm_pred': lstm_pred,
+++++++++++++++++-+            'lstm_wer': lstm_wer,
+++++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++++++++++++-+        })
+++++++++++++++++-+
+++++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++++++++-+    
+++++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++++++++-+    for sample in top5:
+++++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++++++++-+        print(f"GT   : {sample['gt']}")
+++++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++++++++++++-+        print("-" * 60)
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
+++++++++++++++++-+
++++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++++++++++++index 7ac0c3b..c89f220 100644
++++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++++++++++++index 71519fd..b9664e1 100644
++++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++++++++++++index e69de29..0342039 100644
++++++++++++++++++--- a/tmp.ipynb
+++++++++++++++++++++ b/tmp.ipynb
++++++++++++++++++@@ -0,0 +1,272 @@
+++++++++++++++++++{
+++++++++++++++++++ "cells": [
+++++++++++++++++++  {
+++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++   "execution_count": 2,
+++++++++++++++++++   "metadata": {},
+++++++++++++++++++   "outputs": [],
+++++++++++++++++++   "source": [
+++++++++++++++++++    "import torch\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++++++++++++   ]
+++++++++++++++++++  },
+++++++++++++++++++  {
+++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++   "execution_count": 3,
+++++++++++++++++++   "metadata": {},
+++++++++++++++++++   "outputs": [
+++++++++++++++++++    {
+++++++++++++++++++     "name": "stdout",
+++++++++++++++++++     "output_type": "stream",
+++++++++++++++++++     "text": [
+++++++++++++++++++      "Model is a state_dict.\n",
+++++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++++++++++++     ]
+++++++++++++++++++    }
+++++++++++++++++++   ],
+++++++++++++++++++   "source": [
+++++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++++++++++++   ]
+++++++++++++++++++  },
+++++++++++++++++++  {
+++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++   "execution_count": 7,
+++++++++++++++++++   "metadata": {},
+++++++++++++++++++   "outputs": [],
+++++++++++++++++++   "source": [
+++++++++++++++++++    "import torch\n",
+++++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++++++++++++    "import torch.nn.functional as F\n"
+++++++++++++++++++   ]
+++++++++++++++++++  },
+++++++++++++++++++  {
+++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++   "execution_count": 1,
+++++++++++++++++++   "metadata": {},
+++++++++++++++++++   "outputs": [
+++++++++++++++++++    {
+++++++++++++++++++     "name": "stderr",
+++++++++++++++++++     "output_type": "stream",
+++++++++++++++++++     "text": [
+++++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++++++++++++     ]
+++++++++++++++++++    }
+++++++++++++++++++   ],
+++++++++++++++++++   "source": [
+++++++++++++++++++    "import torch\n",
+++++++++++++++++++    "import torch.nn as nn\n",
+++++++++++++++++++    "import torch.nn.functional as F\n",
+++++++++++++++++++    "import torchvision.models as models\n",
+++++++++++++++++++    "import numpy as np\n",
+++++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++++++++++++    "from modules.criterions import SeqKD\n",
+++++++++++++++++++    "import utils\n",
+++++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++++++++++++    "class Identity(nn.Module):\n",
+++++++++++++++++++    "    def __init__(self):\n",
+++++++++++++++++++    "        super(Identity, self).__init__()\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++++    "        return x\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# L2 정규화 선형 레이어\n",
+++++++++++++++++++    "class NormLinear(nn.Module):\n",
+++++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++++++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++++++++++++    "        return outputs\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++++++++++++    "class SLRModel(nn.Module):\n",
+++++++++++++++++++    "    def __init__(\n",
+++++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++++++++++++    "    ):\n",
+++++++++++++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++++++++++++    "        self.decoder = None\n",
+++++++++++++++++++    "        self.loss = dict()\n",
+++++++++++++++++++    "        self.criterion_init()\n",
+++++++++++++++++++    "        self.num_classes = num_classes\n",
+++++++++++++++++++    "        self.loss_weights = loss_weights\n",
+++++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++++++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++++++++++++    "                                   conv_type=conv_type,\n",
+++++++++++++++++++    "                                   use_bn=use_bn,\n",
+++++++++++++++++++    "                                   num_classes=num_classes)\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++++++++++++    "        if weight_norm:\n",
+++++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++++    "        else:\n",
+++++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        # Classifier 공유 여부\n",
+++++++++++++++++++    "        if share_classifier:\n",
+++++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++++++++++++    "        if len(x.shape) == 5:\n",
+++++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++++++++++++    "        else:\n",
+++++++++++++++++++    "            framewise = x\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        # 최종 Classifier 적용\n",
+++++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "        return {\n",
+++++++++++++++++++    "            \"framewise_features\": framewise,\n",
+++++++++++++++++++    "            \"visual_features\": x,\n",
+++++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++++++++++++    "            \"feat_len\": lgt,\n",
+++++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++++++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++++++++++++    "            \"recognized_sents\": pred,\n",
+++++++++++++++++++    "        }\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "    def criterion_init(self):\n",
+++++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++++++++++++    "        return self.loss\n"
+++++++++++++++++++   ]
+++++++++++++++++++  },
+++++++++++++++++++  {
+++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++   "execution_count": 6,
+++++++++++++++++++   "metadata": {},
+++++++++++++++++++   "outputs": [
+++++++++++++++++++    {
+++++++++++++++++++     "ename": "KeyError",
+++++++++++++++++++     "evalue": "'dataset_info'",
+++++++++++++++++++     "output_type": "error",
+++++++++++++++++++     "traceback": [
+++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++++++++++++     ]
+++++++++++++++++++    }
+++++++++++++++++++   ],
+++++++++++++++++++   "source": [
+++++++++++++++++++    "import os\n",
+++++++++++++++++++    "import numpy as np\n",
+++++++++++++++++++    "import yaml\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# 환경 변수 설정\n",
+++++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++++++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# ✅ gloss_dict 로드\n",
+++++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++++++++++++   ]
+++++++++++++++++++  },
+++++++++++++++++++  {
+++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++   "execution_count": 5,
+++++++++++++++++++   "metadata": {},
+++++++++++++++++++   "outputs": [
+++++++++++++++++++    {
+++++++++++++++++++     "ename": "AttributeError",
+++++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++++++++++++     "output_type": "error",
+++++++++++++++++++     "traceback": [
+++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++++++++++++     ]
+++++++++++++++++++    }
+++++++++++++++++++   ],
+++++++++++++++++++   "source": [
+++++++++++++++++++    "import torch\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# 모델 불러오기\n",
+++++++++++++++++++    "model = SLRModel(\n",
+++++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++++++++++++    ")\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# 저장된 가중치 로드\n",
+++++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++++++++++++    "if isinstance(state_dict, dict):\n",
+++++++++++++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++++++++++++    "\n",
+++++++++++++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++++++++++++    "model.eval()\n"
+++++++++++++++++++   ]
+++++++++++++++++++  }
+++++++++++++++++++ ],
+++++++++++++++++++ "metadata": {
+++++++++++++++++++  "kernelspec": {
+++++++++++++++++++   "display_name": "3.9.13",
+++++++++++++++++++   "language": "python",
+++++++++++++++++++   "name": "python3"
+++++++++++++++++++  },
+++++++++++++++++++  "language_info": {
+++++++++++++++++++   "codemirror_mode": {
+++++++++++++++++++    "name": "ipython",
+++++++++++++++++++    "version": 3
+++++++++++++++++++   },
+++++++++++++++++++   "file_extension": ".py",
+++++++++++++++++++   "mimetype": "text/x-python",
+++++++++++++++++++   "name": "python",
+++++++++++++++++++   "nbconvert_exporter": "python",
+++++++++++++++++++   "pygments_lexer": "ipython3",
+++++++++++++++++++   "version": "3.9.13"
+++++++++++++++++++  }
+++++++++++++++++++ },
+++++++++++++++++++ "nbformat": 4,
+++++++++++++++++++ "nbformat_minor": 2
+++++++++++++++++++}
++++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++++++++++++index cb157af..4e1e0f0 100644
++++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++++++++++++index 3877729..9defc30 100644
++++++++++++++++++--- a/utils/decode.py
+++++++++++++++++++++ b/utils/decode.py
++++++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++++++++++++++ import numpy as np
++++++++++++++++++ from itertools import groupby
++++++++++++++++++ import torch.nn.functional as F
+++++++++++++++++++import torch
+++++++++++++++++++import matplotlib.pyplot as plt
+++++++++++++++++++import numpy as np
+++++++++++++++++ +
+++++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
+++++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
+++++++++++++++++ +
+++++++++++++++++++    length = int(vid_lgt[batch_idx].item())
+++++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++++++++++++    x = np.arange(length)
+++++++++++++++++ +
+++++++++++++++++++    plt.figure(figsize=(15, 4))
+++++++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++++++++++++++ +
+++++++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++++++++++++++    if len(blank_indices) > 0:
+++++++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++++++++++++++ +
+++++++++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
+++++++++++++++++++    if sample_id:
+++++++++++++++++++        title_str += f"\nSample: {sample_id}"
+++++++++++++++++++    plt.title(title_str)
+++++++++++++++++++    plt.xlabel("Time Step")
+++++++++++++++++++    plt.ylabel("Gloss ID")
+++++++++++++++++++    plt.yticks(np.unique(pred_seq))
+++++++++++++++++++    plt.grid(True)
+++++++++++++++++++    plt.legend()
+++++++++++++++++++    plt.tight_layout()
+++++++++++++++++++    plt.show()
+++++++++++++++++ +
++++++++++++++++++ 
++++++++++++++++++ 
++++++++++++++++++ class Decode(object):
++++++++++++++++++@@ -16,35 +48,29 @@ class Decode(object):
++++++++++++++++++         self.search_mode = search_mode
++++++++++++++++++         self.blank_id = blank_id
++++++++++++++++++         vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
++++++++++++++++++-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id,
++++++++++++++++++-                                                    num_processes=10)
+++++++++++++++++++        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=beam_width, blank_id=blank_id, num_processes=10)
++++++++++++++++++ 
++++++++++++++++++-    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++++++++++++    def decode(self, nn_output, vid_lgt, batch_first=True, probs=False, sample_ids=None):
++++++++++++++++++         if not batch_first:
++++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++++++++++++ +
+++++++++++++++++++        # sample_id가 존재하면 시각화에 함께 넘김
+++++++++++++++++++        sample_id = sample_ids[0] if sample_ids else None
+++++++++++++++++++        plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0, sample_id=sample_id)
+++++++++++++++++ +
++++++++++++++++++         if self.search_mode == "max":
++++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++++++++++++         else:
++++++++++++++++++             return self.BeamSearch(nn_output, vid_lgt, probs)
+++++++++++++++++  
+++++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++++++++++++++     def BeamSearch(self, nn_output, vid_lgt, probs=False):
++++++++++++++++++-        '''
++++++++++++++++++-        CTCBeamDecoder Shape:
++++++++++++++++++-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
++++++++++++++++++-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
++++++++++++++++++-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
++++++++++++++++++-                          timesteps (B, N_beams)
++++++++++++++++++-                          out_lens (B, N_beams)
++++++++++++++++++-        '''
++++++++++++++++++-
++++++++++++++++++         index_list = torch.argmax(nn_output.cpu(), axis=2)
++++++++++++++++++         batchsize, lgt = index_list.shape
++++++++++++++++++-        blank_rate =[]
+++++++++++++++++++        blank_rate = []
++++++++++++++++++         for batch_idx in range(batchsize):
++++++++++++++++++             group_result = [x.item() for x in index_list[batch_idx][:int(vid_lgt[batch_idx])]]
++++++++++++++++++             blank_rate.append(group_result)
++++++++++++++++++ 
++++++++++++++++++-
++++++++++++++++++         if not probs:
++++++++++++++++++             nn_output = nn_output.softmax(-1).cpu()
++++++++++++++++++         vid_lgt = vid_lgt.cpu()
++++++++++++++++++@@ -54,9 +80,7 @@ class Decode(object):
++++++++++++++++++             first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
++++++++++++++++++             if len(first_result) != 0:
++++++++++++++++++                 first_result = torch.stack([x[0] for x in groupby(first_result)])
++++++++++++++++++-            # ret_list.append([str(int(gloss_id)) for idx, gloss_id in enumerate(first_result)])
++++++++++++++++++-            ret_list.append([self.i2g_dict[int(gloss_id)] for idx, gloss_id in
++++++++++++++++++-                             enumerate(first_result)])
+++++++++++++++++++            ret_list.append([self.i2g_dict[int(gloss_id)] for gloss_id in first_result])
++++++++++++++++++         return ret_list
++++++++++++++++++ 
++++++++++++++++++     def MaxDecode(self, nn_output, vid_lgt):
++++++++++++++++++@@ -65,12 +89,11 @@ class Decode(object):
++++++++++++++++++         ret_list = []
++++++++++++++++++         for batch_idx in range(batchsize):
++++++++++++++++++             group_result = [x[0] for x in groupby(index_list[batch_idx][:vid_lgt[batch_idx]])]
++++++++++++++++++-            filtered = [*filter(lambda x: x != self.blank_id, group_result)]
+++++++++++++++++++            filtered = [x for x in group_result if x != self.blank_id]
++++++++++++++++++             if len(filtered) > 0:
++++++++++++++++++                 max_result = torch.stack(filtered)
++++++++++++++++++                 max_result = [x[0] for x in groupby(max_result)]
++++++++++++++++++             else:
++++++++++++++++++                 max_result = filtered
++++++++++++++++++-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
++++++++++++++++++-                             enumerate(max_result)])
++++++++++++++++++-        return ret_list
+++++++++++++++++++            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in enumerate(max_result)])
+++++++++++++++++++        return ret_list
++++++++++++++++++\ No newline at end of file
++++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++++++++++++index 7d0a2aa..cd66258 100644
++++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++++++++++++++++index c31483a..239691c 100644
++++++++++++++++++--- a/work_dirt/config.yaml
+++++++++++++++++++++ b/work_dirt/config.yaml
++++++++++++++++++@@ -7,7 +7,7 @@ dataset_info:
++++++++++++++++++   evaluation_dir: ./evaluation/slr_eval
++++++++++++++++++   evaluation_prefix: phoenix2014-groundtruth
++++++++++++++++++ decode_mode: beam
++++++++++++++++++-device: your_device
+++++++++++++++++++device: cuda
++++++++++++++++++ dist_url: env://
++++++++++++++++++ eval_interval: 1
++++++++++++++++++ evaluate_tool: python
++++++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++++++++++++++index 8a22ece..7650df5 100644
++++++++++++++++++--- a/work_dirt/dirty.patch
+++++++++++++++++++++ b/work_dirt/dirty.patch
++++++++++++++++++@@ -1,284 +1,3624 @@
++++++++++++++++++-diff --git a/README.md b/README.md
++++++++++++++++++-index bdbc17f..8cb240b 100644
++++++++++++++++++---- a/README.md
++++++++++++++++++-+++ b/README.md
++++++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++++++++++++- 
++++++++++++++++++- 
++++++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++++++++++++++- 
++++++++++++++++++- ### Training
++++++++++++++++++- 
++++++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++++++++++++++-index bfc1da8..25ffa61 100644
++++++++++++++++++---- a/configs/baseline.yaml
++++++++++++++++++-+++ b/configs/baseline.yaml
++++++++++++++++++-@@ -1,14 +1,14 @@
++++++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++++++++++++++- phase: train
++++++++++++++++++--dataset: phoenix2014-T
++++++++++++++++++-+dataset: phoenix2014
++++++++++++++++++- #CSL-Daily
++++++++++++++++++- # dataset: phoenix14-si5
++++++++++++++++++- 
++++++++++++++++++- work_dir: ./work_dirt/
++++++++++++++++++--batch_size: 4
++++++++++++++++++-+batch_size: 1
++++++++++++++++++- random_seed: 0 
++++++++++++++++++--test_batch_size: 4
++++++++++++++++++--num_worker: 20
++++++++++++++++++-+test_batch_size: 1
++++++++++++++++++-+num_worker: 3
++++++++++++++++++- device: 0
++++++++++++++++++- log_interval: 10000
++++++++++++++++++- eval_interval: 1
++++++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++++++++++++++-index 555f4b8..c126e7a 100644
++++++++++++++++++---- a/dataset/dataloader_video.py
++++++++++++++++++-+++ b/dataset/dataloader_video.py
++++++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++++++++++++-+
++++++++++++++++++-         print(mode, len(self))
++++++++++++++++++-         self.data_aug = self.transform()
++++++++++++++++++-         print("")
++++++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
+++++++++++++++++ - 
+++++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++++++++++++-diff --git a/slr_network.py b/slr_network.py
+++++++++++++++++-index 45295cb..ede70cf 100644
+++++++++++++++++---- a/slr_network.py
+++++++++++++++++-+++ b/slr_network.py
+++++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++++++++++++-     def read_video(self, index):
++++++++++++++++++--        # load file info
++++++++++++++++++-         fi = self.inputs_list[index]
++++++++++++++++++-+    
++++++++++++++++++-         if 'phoenix' in self.dataset:
++++++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++++++++++++++-+#            print(img_list)
++++++++++++++++++-+
++++++++++++++++++-+#            print("[LOG] Using phoenix")
++++++++++++++++++-+
++++++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++++++++++++- 
++++++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++++++++++-+#            print(img_folder)
++++++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++++++++++++++-+#            print(len(img_list))
++++++++++++++++++-         elif self.dataset == 'CSL-Daily':
++++++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++++++++++++++-+    
++++++++++++++++++-         img_list = sorted(glob.glob(img_folder))
++++++++++++++++++-+    
++++++++++++++++++-+        if len(img_list) == 0:
++++++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++++++++++++++-+    
++++++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++++++++++++++++-+    
++++++++++++++++++-         label_list = []
++++++++++++++++++--        if self.dataset=='phoenix2014':
++++++++++++++++++-+        if self.dataset == 'phoenix2014':
++++++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++++++++++++++--        if self.dataset=='phoenix2014-T':
++++++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
++++++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++++++++++++++-+    
++++++++++++++++++-         for phase in fi['label'].split(" "):
++++++++++++++++++--            if phase == '':
++++++++++++++++++--                continue
++++++++++++++++++--            if phase in self.dict.keys():
++++++++++++++++++-+            if phase and phase in self.dict:
++++++++++++++++++-                 label_list.append(self.dict[phase][0])
++++++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++++++++++++++-+    
++++++++++++++++++-+        video = [
++++++++++++++++++-+            cv2.cvtColor(
++++++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++++++-+                cv2.COLOR_BGR2RGB
++++++++++++++++++-+            )   
++++++++++++++++++-+            for img_path in img_list
++++++++++++++++++-+        ]
++++++++++++++++++-+    
++++++++++++++++++-+        return video, label_list, fi
++++++++++++++++++- 
++++++++++++++++++-     def read_features(self, index):
++++++++++++++++++-         # load file info
++++++++++++++++++-diff --git a/main.py b/main.py
++++++++++++++++++-index 9e68cee..18ac59b 100644
++++++++++++++++++---- a/main.py
++++++++++++++++++-+++ b/main.py
++++++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
++++++++++++++++++-                 batch_size=batch_size,
++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++++++-                 num_workers=self.arg.num_worker,
++++++++++++++++++--                pin_memory=True,
++++++++++++++++++-+                pin_memory=False,
++++++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++++++-             )
++++++++++++++++++-             return loader
++++++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
++++++++++++++++++-                 drop_last=train_flag,
++++++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++++++--                pin_memory=True,
++++++++++++++++++-+                pin_memory=False,
++++++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++++++-             )
++++++++++++++++++- 
++++++++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++++++++++-index 528856d..d8fcaf9 100644
++++++++++++++++++---- a/seq_scripts.py
++++++++++++++++++-+++ b/seq_scripts.py
++++++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++++++++++++++-     return
++++++++++++++++++- 
++++++++++++++++++- 
++++++++++++++++++-+import csv 
++++++++++++++++++-+from jiwer import wer as jiwer_wer
++++++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++++++++++++++-     model.eval()
++++++++++++++++++--    results=defaultdict(dict)
++++++++++++++++++-+    results = defaultdict(dict)
++++++++++++++++++- 
++++++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++++++++++++++-         recoder.record_timer("device")
++++++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
++++++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++++++++++++++-                 results[inf]['gloss'] = gl
++++++++++++++++++-+
++++++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++++++-+
++++++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++++++++++++++--        reg_per = wer_results
++++++++++++++++++--    else:
++++++++++++++++++--        reg_per = wer_results_con
++++++++++++++++++-+
++++++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++++++++++++-+
++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++++++++++++++-         f"{work_dir}/{mode}.txt")
++++++++++++++++++-+
++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++++++++++++++-+        f"{work_dir}/{mode}.txt")
++++++++++++++++++-+
++++++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
++++++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++++++++++++++++++-+
++++++++++++++++++-+    rows = []
++++++++++++++++++-+    for file_id in results:
++++++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++++++-+
++++++++++++++++++-+        rows.append([
++++++++++++++++++-+            file_id,
++++++++++++++++++-+            gt,
++++++++++++++++++-+            conv_pred,
++++++++++++++++++-+            f"{conv_wer:.4f}",
++++++++++++++++++-+            lstm_pred,
++++++++++++++++++-+            f"{lstm_wer:.4f}"
++++++++++++++++++-+        ])
++++++++++++++++++-+
++++++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++++++++++++++-+        writer = csv.writer(f)
++++++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++++++++++++++-+        writer.writerows(rows)
++++++++++++++++++-+
++++++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
++++++++++++++++++-+    sample_wers = []
++++++++++++++++++-+    for file_id in results:
++++++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++++++-+    
++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++++++-+    
++++++++++++++++++-+        sample_wers.append({
++++++++++++++++++-+            'file_id': file_id,
++++++++++++++++++-+            'gt': gt,
++++++++++++++++++-+            'conv_pred': conv_pred,
++++++++++++++++++-+            'conv_wer': conv_wer,
++++++++++++++++++-+            'lstm_pred': lstm_pred,
++++++++++++++++++-+            'lstm_wer': lstm_wer,
++++++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++++++++++++++-+        })
++++++++++++++++++-+
++++++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++++++++-+    
++++++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++++++++-+    for sample in top5:
++++++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++++++++-+        print(f"GT   : {sample['gt']}")
++++++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++++++++++++++-+        print("-" * 60)
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
++++++++++++++++++-+
+++++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++++++++++++++index 7ac0c3b..c89f220 100644
+++++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++++++++++++++index 71519fd..b9664e1 100644
+++++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++++++++++++++index e69de29..0342039 100644
+++++++++++++++++++--- a/tmp.ipynb
++++++++++++++++++++++ b/tmp.ipynb
+++++++++++++++++++@@ -0,0 +1,272 @@
++++++++++++++++++++{
++++++++++++++++++++ "cells": [
++++++++++++++++++++  {
++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++   "execution_count": 2,
++++++++++++++++++++   "metadata": {},
++++++++++++++++++++   "outputs": [],
++++++++++++++++++++   "source": [
++++++++++++++++++++    "import torch\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++++++++++++++   ]
++++++++++++++++++++  },
++++++++++++++++++++  {
++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++   "execution_count": 3,
++++++++++++++++++++   "metadata": {},
++++++++++++++++++++   "outputs": [
++++++++++++++++++++    {
++++++++++++++++++++     "name": "stdout",
++++++++++++++++++++     "output_type": "stream",
++++++++++++++++++++     "text": [
++++++++++++++++++++      "Model is a state_dict.\n",
++++++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++++++++++++++     ]
++++++++++++++++++++    }
++++++++++++++++++++   ],
++++++++++++++++++++   "source": [
++++++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++++++++++++++   ]
++++++++++++++++++++  },
++++++++++++++++++++  {
++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++   "execution_count": 7,
++++++++++++++++++++   "metadata": {},
++++++++++++++++++++   "outputs": [],
++++++++++++++++++++   "source": [
++++++++++++++++++++    "import torch\n",
++++++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++++++++++++++    "import torch.nn.functional as F\n"
++++++++++++++++++++   ]
++++++++++++++++++++  },
++++++++++++++++++++  {
++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++   "execution_count": 1,
++++++++++++++++++++   "metadata": {},
++++++++++++++++++++   "outputs": [
++++++++++++++++++++    {
++++++++++++++++++++     "name": "stderr",
++++++++++++++++++++     "output_type": "stream",
++++++++++++++++++++     "text": [
++++++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++++++++++++++     ]
++++++++++++++++++++    }
++++++++++++++++++++   ],
++++++++++++++++++++   "source": [
++++++++++++++++++++    "import torch\n",
++++++++++++++++++++    "import torch.nn as nn\n",
++++++++++++++++++++    "import torch.nn.functional as F\n",
++++++++++++++++++++    "import torchvision.models as models\n",
++++++++++++++++++++    "import numpy as np\n",
++++++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++++++++++++++    "from modules.criterions import SeqKD\n",
++++++++++++++++++++    "import utils\n",
++++++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++++++++++++++    "class Identity(nn.Module):\n",
++++++++++++++++++++    "    def __init__(self):\n",
++++++++++++++++++++    "        super(Identity, self).__init__()\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++++++    "        return x\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# L2 정규화 선형 레이어\n",
++++++++++++++++++++    "class NormLinear(nn.Module):\n",
++++++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++++++++++++++    "        super(NormLinear, self).__init__()\n",
++++++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++++++++++++++    "        return outputs\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++++++++++++++    "class SLRModel(nn.Module):\n",
++++++++++++++++++++    "    def __init__(\n",
++++++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++++++++++++++    "    ):\n",
++++++++++++++++++++    "        super(SLRModel, self).__init__()\n",
++++++++++++++++++++    "        self.decoder = None\n",
++++++++++++++++++++    "        self.loss = dict()\n",
++++++++++++++++++++    "        self.criterion_init()\n",
++++++++++++++++++++    "        self.num_classes = num_classes\n",
++++++++++++++++++++    "        self.loss_weights = loss_weights\n",
++++++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++++++++++++++    "                                   hidden_size=hidden_size,\n",
++++++++++++++++++++    "                                   conv_type=conv_type,\n",
++++++++++++++++++++    "                                   use_bn=use_bn,\n",
++++++++++++++++++++    "                                   num_classes=num_classes)\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++++++++++++++    "        if weight_norm:\n",
++++++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++++++    "        else:\n",
++++++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        # Classifier 공유 여부\n",
++++++++++++++++++++    "        if share_classifier:\n",
++++++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++++++++++++++    "        if len(x.shape) == 5:\n",
++++++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++++++++++++++    "        else:\n",
++++++++++++++++++++    "            framewise = x\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        # 최종 Classifier 적용\n",
++++++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        # Inference 모드에서 Decoding\n",
++++++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "        return {\n",
++++++++++++++++++++    "            \"framewise_features\": framewise,\n",
++++++++++++++++++++    "            \"visual_features\": x,\n",
++++++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++++++++++++++    "            \"feat_len\": lgt,\n",
++++++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++++++++++++++    "            \"sequence_logits\": outputs,\n",
++++++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++++++++++++++    "            \"recognized_sents\": pred,\n",
++++++++++++++++++++    "        }\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "    def criterion_init(self):\n",
++++++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++++++++++++++    "        return self.loss\n"
++++++++++++++++++++   ]
++++++++++++++++++++  },
++++++++++++++++++++  {
++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++   "execution_count": 6,
++++++++++++++++++++   "metadata": {},
++++++++++++++++++++   "outputs": [
++++++++++++++++++++    {
++++++++++++++++++++     "ename": "KeyError",
++++++++++++++++++++     "evalue": "'dataset_info'",
++++++++++++++++++++     "output_type": "error",
++++++++++++++++++++     "traceback": [
++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++++++++++++++     ]
++++++++++++++++++++    }
++++++++++++++++++++   ],
++++++++++++++++++++   "source": [
++++++++++++++++++++    "import os\n",
++++++++++++++++++++    "import numpy as np\n",
++++++++++++++++++++    "import yaml\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# 환경 변수 설정\n",
++++++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++++++++++++++    "with open(config_path, \"r\") as f:\n",
++++++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# ✅ gloss_dict 로드\n",
++++++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++++++++++++++   ]
++++++++++++++++++++  },
++++++++++++++++++++  {
++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++   "execution_count": 5,
++++++++++++++++++++   "metadata": {},
++++++++++++++++++++   "outputs": [
++++++++++++++++++++    {
++++++++++++++++++++     "ename": "AttributeError",
++++++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++++++++++++++     "output_type": "error",
++++++++++++++++++++     "traceback": [
++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++++++++++++++     ]
++++++++++++++++++++    }
++++++++++++++++++++   ],
++++++++++++++++++++   "source": [
++++++++++++++++++++    "import torch\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# 모델 불러오기\n",
++++++++++++++++++++    "model = SLRModel(\n",
++++++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++++++++++++++    ")\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# 저장된 가중치 로드\n",
++++++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++++++++++++++    "if isinstance(state_dict, dict):\n",
++++++++++++++++++++    "    model.load_state_dict(state_dict)\n",
++++++++++++++++++++    "\n",
++++++++++++++++++++    "# 모델을 평가 모드로 설정\n",
++++++++++++++++++++    "model.eval()\n"
++++++++++++++++++++   ]
++++++++++++++++++++  }
++++++++++++++++++++ ],
++++++++++++++++++++ "metadata": {
++++++++++++++++++++  "kernelspec": {
++++++++++++++++++++   "display_name": "3.9.13",
++++++++++++++++++++   "language": "python",
++++++++++++++++++++   "name": "python3"
++++++++++++++++++++  },
++++++++++++++++++++  "language_info": {
++++++++++++++++++++   "codemirror_mode": {
++++++++++++++++++++    "name": "ipython",
++++++++++++++++++++    "version": 3
++++++++++++++++++++   },
++++++++++++++++++++   "file_extension": ".py",
++++++++++++++++++++   "mimetype": "text/x-python",
++++++++++++++++++++   "name": "python",
++++++++++++++++++++   "nbconvert_exporter": "python",
++++++++++++++++++++   "pygments_lexer": "ipython3",
++++++++++++++++++++   "version": "3.9.13"
++++++++++++++++++++  }
++++++++++++++++++++ },
++++++++++++++++++++ "nbformat": 4,
++++++++++++++++++++ "nbformat_minor": 2
++++++++++++++++++++}
+++++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++++++++++++++index cb157af..1207f99 100644
+++++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++++++++++++++index 3877729..773fe51 100644
+++++++++++++++++++--- a/utils/decode.py
++++++++++++++++++++++ b/utils/decode.py
+++++++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
+++++++++++++++++++ import numpy as np
+++++++++++++++++++ from itertools import groupby
+++++++++++++++++++ import torch.nn.functional as F
++++++++++++++++++++import torch
++++++++++++++++++++import matplotlib.pyplot as plt
++++++++++++++++++++import numpy as np
++++++++++++++++++ +
++++++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0, sample_id=None):
++++++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)
++++++++++++++++++ +
++++++++++++++++++++    length = int(vid_lgt[batch_idx].item())
++++++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++++++++++++++    x = np.arange(length)
++++++++++++++++++ +
++++++++++++++++++++    plt.figure(figsize=(15, 4))
++++++++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++++++++++++++++ +
++++++++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++++++++++++++++    if len(blank_indices) > 0:
++++++++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++++++++++++++++ +
++++++++++++++++++++    title_str = "Predicted Gloss ID over Time (CTC Blank Highlighted)"
++++++++++++++++++++    if sample_id:
++++++++++++++++++++        title_str += f"\nSample: {sample_id}"
++++++++++++++++++++    plt.title(title_str)
++++++++++++++++++++    plt.xlabel("Time Step")
++++++++++++++++++++    plt.ylabel("Gloss ID")
++++++++++++++++++++    plt.yticks(np.unique(pred_seq))
++++++++++++++++++++    plt.grid(True)
++++++++++++++++++++    plt.legend()
++++++++++++++++++++    plt.tight_layout()
++++++++++++++++++++    plt.show()
++++++++++++++++++ +
++++++++++++++++++  
++++++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++++++++++++++ 
+++++++++++++++++++ class Decode(object):
+++++++++++++++++++@@ -22,6 +54,10 @@ class Decode(object):
+++++++++++++++++++     def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++++++++++++         if not batch_first:
+++++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
++++++++++++++++++++        
++++++++++++++++++++        # time 별 class 시각화
++++++++++++++++++++        plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0)
++++++++++++++++++++
+++++++++++++++++++         if self.search_mode == "max":
+++++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++++++++++++++         else:
+++++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++++++++++++++index 7d0a2aa..cd66258 100644
+++++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
+++++++++++++++++++index c31483a..239691c 100644
+++++++++++++++++++--- a/work_dirt/config.yaml
++++++++++++++++++++++ b/work_dirt/config.yaml
+++++++++++++++++++@@ -7,7 +7,7 @@ dataset_info:
+++++++++++++++++++   evaluation_dir: ./evaluation/slr_eval
+++++++++++++++++++   evaluation_prefix: phoenix2014-groundtruth
+++++++++++++++++++ decode_mode: beam
+++++++++++++++++++-device: your_device
++++++++++++++++++++device: cuda
+++++++++++++++++++ dist_url: env://
+++++++++++++++++++ eval_interval: 1
+++++++++++++++++++ evaluate_tool: python
+++++++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++++++++++++++index 8a22ece..8746262 100644
+++++++++++++++++++--- a/work_dirt/dirty.patch
++++++++++++++++++++++ b/work_dirt/dirty.patch
+++++++++++++++++++@@ -1,284 +1,2958 @@
+++++++++++++++++++-diff --git a/README.md b/README.md
+++++++++++++++++++-index bdbc17f..8cb240b 100644
+++++++++++++++++++---- a/README.md
+++++++++++++++++++-+++ b/README.md
+++++++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++++++++++++++++- 
+++++++++++++++++++- 
+++++++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++++++++++++++- 
+++++++++++++++++++- ### Training
+++++++++++++++++++- 
+++++++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++++++++++++++-index bfc1da8..25ffa61 100644
+++++++++++++++++++---- a/configs/baseline.yaml
+++++++++++++++++++-+++ b/configs/baseline.yaml
+++++++++++++++++++-@@ -1,14 +1,14 @@
+++++++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++++++++++++++- phase: train
+++++++++++++++++++--dataset: phoenix2014-T
+++++++++++++++++++-+dataset: phoenix2014
+++++++++++++++++++- #CSL-Daily
+++++++++++++++++++- # dataset: phoenix14-si5
+++++++++++++++++++- 
+++++++++++++++++++- work_dir: ./work_dirt/
+++++++++++++++++++--batch_size: 4
+++++++++++++++++++-+batch_size: 1
+++++++++++++++++++- random_seed: 0 
+++++++++++++++++++--test_batch_size: 4
+++++++++++++++++++--num_worker: 20
+++++++++++++++++++-+test_batch_size: 1
+++++++++++++++++++-+num_worker: 3
+++++++++++++++++++- device: 0
+++++++++++++++++++- log_interval: 10000
+++++++++++++++++++- eval_interval: 1
+++++++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++++++++++++++-index 555f4b8..c126e7a 100644
+++++++++++++++++++---- a/dataset/dataloader_video.py
+++++++++++++++++++-+++ b/dataset/dataloader_video.py
+++++++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++++++++++++++index 7ac0c3b..c89f220 100644
++++++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++++++++++++++index 71519fd..b9664e1 100644
++++++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++++++++++++++index e69de29..0342039 100644
++++++++++++++++++++--- a/tmp.ipynb
+++++++++++++++++++++++ b/tmp.ipynb
++++++++++++++++++++@@ -0,0 +1,272 @@
+++++++++++++++++++++{
+++++++++++++++++++++ "cells": [
+++++++++++++++++++++  {
+++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++   "execution_count": 2,
+++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++   "outputs": [],
+++++++++++++++++++++   "source": [
+++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++++++++++++++   ]
+++++++++++++++++++++  },
+++++++++++++++++++++  {
+++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++   "execution_count": 3,
+++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++   "outputs": [
+++++++++++++++++++++    {
+++++++++++++++++++++     "name": "stdout",
+++++++++++++++++++++     "output_type": "stream",
+++++++++++++++++++++     "text": [
+++++++++++++++++++++      "Model is a state_dict.\n",
+++++++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++++++++++++++     ]
+++++++++++++++++++++    }
+++++++++++++++++++++   ],
+++++++++++++++++++++   "source": [
+++++++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++++++++++++++   ]
+++++++++++++++++++++  },
+++++++++++++++++++++  {
+++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++   "execution_count": 7,
+++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++   "outputs": [],
+++++++++++++++++++++   "source": [
+++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++++++++++++++    "import torch.nn.functional as F\n"
+++++++++++++++++++++   ]
+++++++++++++++++++++  },
+++++++++++++++++++++  {
+++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++   "execution_count": 1,
+++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++   "outputs": [
+++++++++++++++++++++    {
+++++++++++++++++++++     "name": "stderr",
+++++++++++++++++++++     "output_type": "stream",
+++++++++++++++++++++     "text": [
+++++++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++++++++++++++     ]
+++++++++++++++++++++    }
+++++++++++++++++++++   ],
+++++++++++++++++++++   "source": [
+++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++    "import torch.nn as nn\n",
+++++++++++++++++++++    "import torch.nn.functional as F\n",
+++++++++++++++++++++    "import torchvision.models as models\n",
+++++++++++++++++++++    "import numpy as np\n",
+++++++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++++++++++++++    "from modules.criterions import SeqKD\n",
+++++++++++++++++++++    "import utils\n",
+++++++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++++++++++++++    "class Identity(nn.Module):\n",
+++++++++++++++++++++    "    def __init__(self):\n",
+++++++++++++++++++++    "        super(Identity, self).__init__()\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++++++    "        return x\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# L2 정규화 선형 레이어\n",
+++++++++++++++++++++    "class NormLinear(nn.Module):\n",
+++++++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++++++++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++++++++++++++    "        return outputs\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++++++++++++++    "class SLRModel(nn.Module):\n",
+++++++++++++++++++++    "    def __init__(\n",
+++++++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++++++++++++++    "    ):\n",
+++++++++++++++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++++++++++++++    "        self.decoder = None\n",
+++++++++++++++++++++    "        self.loss = dict()\n",
+++++++++++++++++++++    "        self.criterion_init()\n",
+++++++++++++++++++++    "        self.num_classes = num_classes\n",
+++++++++++++++++++++    "        self.loss_weights = loss_weights\n",
+++++++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++++++++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++++++++++++++    "                                   conv_type=conv_type,\n",
+++++++++++++++++++++    "                                   use_bn=use_bn,\n",
+++++++++++++++++++++    "                                   num_classes=num_classes)\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++++++++++++++    "        if weight_norm:\n",
+++++++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++    "        else:\n",
+++++++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        # Classifier 공유 여부\n",
+++++++++++++++++++++    "        if share_classifier:\n",
+++++++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++++++++++++++    "        if len(x.shape) == 5:\n",
+++++++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++++++++++++++    "        else:\n",
+++++++++++++++++++++    "            framewise = x\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        # 최종 Classifier 적용\n",
+++++++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "        return {\n",
+++++++++++++++++++++    "            \"framewise_features\": framewise,\n",
+++++++++++++++++++++    "            \"visual_features\": x,\n",
+++++++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++++++++++++++    "            \"feat_len\": lgt,\n",
+++++++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++++++++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++++++++++++++    "            \"recognized_sents\": pred,\n",
+++++++++++++++++++++    "        }\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "    def criterion_init(self):\n",
+++++++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++++++++++++++    "        return self.loss\n"
+++++++++++++++++++++   ]
+++++++++++++++++++++  },
+++++++++++++++++++++  {
+++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++   "execution_count": 6,
+++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++   "outputs": [
+++++++++++++++++++++    {
+++++++++++++++++++++     "ename": "KeyError",
+++++++++++++++++++++     "evalue": "'dataset_info'",
+++++++++++++++++++++     "output_type": "error",
+++++++++++++++++++++     "traceback": [
+++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++++++++++++++     ]
+++++++++++++++++++++    }
+++++++++++++++++++++   ],
+++++++++++++++++++++   "source": [
+++++++++++++++++++++    "import os\n",
+++++++++++++++++++++    "import numpy as np\n",
+++++++++++++++++++++    "import yaml\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# 환경 변수 설정\n",
+++++++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++++++++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# ✅ gloss_dict 로드\n",
+++++++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++++++++++++++   ]
+++++++++++++++++++++  },
+++++++++++++++++++++  {
+++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++   "execution_count": 5,
+++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++   "outputs": [
+++++++++++++++++++++    {
+++++++++++++++++++++     "ename": "AttributeError",
+++++++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++++++++++++++     "output_type": "error",
+++++++++++++++++++++     "traceback": [
+++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++++++++++++++     ]
+++++++++++++++++++++    }
+++++++++++++++++++++   ],
+++++++++++++++++++++   "source": [
+++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# 모델 불러오기\n",
+++++++++++++++++++++    "model = SLRModel(\n",
+++++++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++++++++++++++    ")\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# 저장된 가중치 로드\n",
+++++++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++++++++++++++    "if isinstance(state_dict, dict):\n",
+++++++++++++++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++++++++++++++    "\n",
+++++++++++++++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++++++++++++++    "model.eval()\n"
+++++++++++++++++++++   ]
+++++++++++++++++++++  }
+++++++++++++++++++++ ],
+++++++++++++++++++++ "metadata": {
+++++++++++++++++++++  "kernelspec": {
+++++++++++++++++++++   "display_name": "3.9.13",
+++++++++++++++++++++   "language": "python",
+++++++++++++++++++++   "name": "python3"
+++++++++++++++++++++  },
+++++++++++++++++++++  "language_info": {
+++++++++++++++++++++   "codemirror_mode": {
+++++++++++++++++++++    "name": "ipython",
+++++++++++++++++++++    "version": 3
+++++++++++++++++++++   },
+++++++++++++++++++++   "file_extension": ".py",
+++++++++++++++++++++   "mimetype": "text/x-python",
+++++++++++++++++++++   "name": "python",
+++++++++++++++++++++   "nbconvert_exporter": "python",
+++++++++++++++++++++   "pygments_lexer": "ipython3",
+++++++++++++++++++++   "version": "3.9.13"
+++++++++++++++++++++  }
+++++++++++++++++++++ },
+++++++++++++++++++++ "nbformat": 4,
+++++++++++++++++++++ "nbformat_minor": 2
+++++++++++++++++++++}
++++++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++++++++++++++index cb157af..8e90f4b 100644
++++++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++++++++++++++index 3877729..a90eb35 100644
++++++++++++++++++++--- a/utils/decode.py
+++++++++++++++++++++++ b/utils/decode.py
++++++++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++++++++++++++++ import numpy as np
++++++++++++++++++++ from itertools import groupby
++++++++++++++++++++ import torch.nn.functional as F
+++++++++++++++++++++import torch
+++++++++++++++++++++import matplotlib.pyplot as plt
+++++++++++++++++++++import numpy as np
+++++++++++++++++++ +
+++++++++++++++++++-         print(mode, len(self))
+++++++++++++++++++-         self.data_aug = self.transform()
+++++++++++++++++++-         print("")
+++++++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++++++++ - 
++++++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++++++++++++++-diff --git a/slr_network.py b/slr_network.py
++++++++++++++++++-index 45295cb..ede70cf 100644
++++++++++++++++++---- a/slr_network.py
++++++++++++++++++-+++ b/slr_network.py
++++++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++++++++++++++-     def read_video(self, index):
+++++++++++++++++++--        # load file info
+++++++++++++++++++-         fi = self.inputs_list[index]
+++++++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0):
+++++++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)  # (B, T)
+++++++++++++++++++ +    
+++++++++++++++++++-         if 'phoenix' in self.dataset:
+++++++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++++++++++++++-+#            print(img_list)
+++++++++++++++++++-+
+++++++++++++++++++-+#            print("[LOG] Using phoenix")
+++++++++++++++++++-+
+++++++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++++++++++- 
+++++++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++++++++++-+#            print(img_folder)
+++++++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++++++++++++++-+#            print(len(img_list))
+++++++++++++++++++-         elif self.dataset == 'CSL-Daily':
+++++++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++++++++++++++-+    
+++++++++++++++++++-         img_list = sorted(glob.glob(img_folder))
+++++++++++++++++++-+    
+++++++++++++++++++-+        if len(img_list) == 0:
+++++++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++++++++++++++-+    
+++++++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++++++++++++-+    
+++++++++++++++++++-         label_list = []
+++++++++++++++++++--        if self.dataset=='phoenix2014':
+++++++++++++++++++-+        if self.dataset == 'phoenix2014':
+++++++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++++++++++++++--        if self.dataset=='phoenix2014-T':
+++++++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++++++++++++++-+    
+++++++++++++++++++-         for phase in fi['label'].split(" "):
+++++++++++++++++++--            if phase == '':
+++++++++++++++++++--                continue
+++++++++++++++++++--            if phase in self.dict.keys():
+++++++++++++++++++-+            if phase and phase in self.dict:
+++++++++++++++++++-                 label_list.append(self.dict[phase][0])
+++++++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++++++++++++++-+    
+++++++++++++++++++-+        video = [
+++++++++++++++++++-+            cv2.cvtColor(
+++++++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++++++-+                cv2.COLOR_BGR2RGB
+++++++++++++++++++-+            )   
+++++++++++++++++++-+            for img_path in img_list
+++++++++++++++++++-+        ]
+++++++++++++++++++-+    
+++++++++++++++++++-+        return video, label_list, fi
+++++++++++++++++++- 
+++++++++++++++++++-     def read_features(self, index):
+++++++++++++++++++-         # load file info
+++++++++++++++++++-diff --git a/main.py b/main.py
+++++++++++++++++++-index 9e68cee..18ac59b 100644
+++++++++++++++++++---- a/main.py
+++++++++++++++++++-+++ b/main.py
+++++++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++++++++++++++-                 batch_size=batch_size,
+++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++++++-                 num_workers=self.arg.num_worker,
+++++++++++++++++++--                pin_memory=True,
+++++++++++++++++++-+                pin_memory=False,
+++++++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++++++-             )
+++++++++++++++++++-             return loader
+++++++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++++++++++++++-                 drop_last=train_flag,
+++++++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++++++--                pin_memory=True,
+++++++++++++++++++-+                pin_memory=False,
+++++++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++++++-             )
+++++++++++++++++++- 
+++++++++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++++++++++-index 528856d..d8fcaf9 100644
+++++++++++++++++++---- a/seq_scripts.py
+++++++++++++++++++-+++ b/seq_scripts.py
+++++++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++++++++++++++-     return
+++++++++++++++++++- 
+++++++++++++++++++- 
+++++++++++++++++++-+import csv 
+++++++++++++++++++-+from jiwer import wer as jiwer_wer
+++++++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++++++++++++++-     model.eval()
+++++++++++++++++++--    results=defaultdict(dict)
+++++++++++++++++++-+    results = defaultdict(dict)
+++++++++++++++++++- 
+++++++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++++++++++++++-         recoder.record_timer("device")
+++++++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++++++++++++++-                 results[inf]['gloss'] = gl
+++++++++++++++++++-+
+++++++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++++++-+
+++++++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++++++++++++++--        reg_per = wer_results
+++++++++++++++++++--    else:
+++++++++++++++++++--        reg_per = wer_results_con
+++++++++++++++++++-+
+++++++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++++++++++++-+
+++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++++++++++++++-         f"{work_dir}/{mode}.txt")
+++++++++++++++++++-+
+++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++++++++++++++-+        f"{work_dir}/{mode}.txt")
+++++++++++++++++++-+
+++++++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++++++++++++-+
+++++++++++++++++++-+    rows = []
+++++++++++++++++++-+    for file_id in results:
+++++++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++++++-+
+++++++++++++++++++-+        rows.append([
+++++++++++++++++++-+            file_id,
+++++++++++++++++++-+            gt,
+++++++++++++++++++-+            conv_pred,
+++++++++++++++++++-+            f"{conv_wer:.4f}",
+++++++++++++++++++-+            lstm_pred,
+++++++++++++++++++-+            f"{lstm_wer:.4f}"
+++++++++++++++++++-+        ])
+++++++++++++++++++-+
+++++++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++++++++++++++-+        writer = csv.writer(f)
+++++++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++++++++++++++-+        writer.writerows(rows)
+++++++++++++++++++-+
+++++++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++++++++++++++-+    sample_wers = []
+++++++++++++++++++-+    for file_id in results:
+++++++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++++++-+    
+++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++++++-+    
+++++++++++++++++++-+        sample_wers.append({
+++++++++++++++++++-+            'file_id': file_id,
+++++++++++++++++++-+            'gt': gt,
+++++++++++++++++++-+            'conv_pred': conv_pred,
+++++++++++++++++++-+            'conv_wer': conv_wer,
+++++++++++++++++++-+            'lstm_pred': lstm_pred,
+++++++++++++++++++-+            'lstm_wer': lstm_wer,
+++++++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++++++++++++++-+        })
+++++++++++++++++++-+
+++++++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++++++++++-+    
+++++++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++++++++++-+    for sample in top5:
+++++++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++++++++++-+        print(f"GT   : {sample['gt']}")
+++++++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++++++++++++++-+        print("-" * 60)
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++-+
+++++++++++++++++++++    length = int(vid_lgt[batch_idx].item())
+++++++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++++++++++++ +
+++++++++++++++++++++    # X축: timestep
+++++++++++++++++++++    x = np.arange(length)
+++++++++++++++++++ +
+++++++++++++++++++++    # 선 그래프 전체 (steps-post 스타일)
+++++++++++++++++++++    plt.figure(figsize=(15, 4))
+++++++++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++++++++++++++++ +
+++++++++++++++++++++    # Blank(0) 위치 마커 강조
+++++++++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++++++++++++++++    if len(blank_indices) > 0:
+++++++++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++++++++++++++++ +
+++++++++++++++++++++    plt.title("Predicted Gloss ID over Time (CTC Blank Highlighted)")
+++++++++++++++++++++    plt.xlabel("Time Step")
+++++++++++++++++++++    plt.ylabel("Gloss ID")
+++++++++++++++++++++    plt.yticks(np.unique(pred_seq))
+++++++++++++++++++++    plt.grid(True)
+++++++++++++++++++++    plt.legend()
+++++++++++++++++++++    plt.tight_layout()
+++++++++++++++++++++    plt.show()
+++++++++++++++++++  
+++++++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++++++++++++++++ 
++++++++++++++++++++ class Decode(object):
++++++++++++++++++++@@ -22,6 +54,10 @@ class Decode(object):
++++++++++++++++++++     def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++++++++++++++         if not batch_first:
++++++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++++++++++++++++        
+++++++++++++++++++++        # time 별 class 시각화
+++++++++++++++++++++        plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0)
+++++++++++++++++++++
++++++++++++++++++++         if self.search_mode == "max":
++++++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++++++++++++++         else:
++++++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++++++++++++++index 7d0a2aa..cd66258 100644
++++++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++++++++++++++diff --git a/work_dirt/config.yaml b/work_dirt/config.yaml
++++++++++++++++++++index c31483a..239691c 100644
++++++++++++++++++++--- a/work_dirt/config.yaml
+++++++++++++++++++++++ b/work_dirt/config.yaml
++++++++++++++++++++@@ -7,7 +7,7 @@ dataset_info:
++++++++++++++++++++   evaluation_dir: ./evaluation/slr_eval
++++++++++++++++++++   evaluation_prefix: phoenix2014-groundtruth
++++++++++++++++++++ decode_mode: beam
++++++++++++++++++++-device: your_device
+++++++++++++++++++++device: cuda
++++++++++++++++++++ dist_url: env://
++++++++++++++++++++ eval_interval: 1
++++++++++++++++++++ evaluate_tool: python
++++++++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++++++++++++++++index 8a22ece..aacc03f 100644
++++++++++++++++++++--- a/work_dirt/dirty.patch
+++++++++++++++++++++++ b/work_dirt/dirty.patch
++++++++++++++++++++@@ -1,284 +1,2294 @@
++++++++++++++++++++-diff --git a/README.md b/README.md
++++++++++++++++++++-index bdbc17f..8cb240b 100644
++++++++++++++++++++---- a/README.md
++++++++++++++++++++-+++ b/README.md
++++++++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++++++++++++++- 
++++++++++++++++++++- 
++++++++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++++++++++++++++- 
++++++++++++++++++++- ### Training
+++++++++++++++++++ - 
+++++++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++++++++++++++-diff --git a/slr_network.py b/slr_network.py
+++++++++++++++++++-index 45295cb..ede70cf 100644
+++++++++++++++++++---- a/slr_network.py
+++++++++++++++++++-+++ b/slr_network.py
+++++++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++++++++++++++++-index bfc1da8..25ffa61 100644
++++++++++++++++++++---- a/configs/baseline.yaml
++++++++++++++++++++-+++ b/configs/baseline.yaml
++++++++++++++++++++-@@ -1,14 +1,14 @@
++++++++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++++++++++++++++- phase: train
++++++++++++++++++++--dataset: phoenix2014-T
++++++++++++++++++++-+dataset: phoenix2014
++++++++++++++++++++- #CSL-Daily
++++++++++++++++++++- # dataset: phoenix14-si5
++++++++++++++++++++- 
++++++++++++++++++++- work_dir: ./work_dirt/
++++++++++++++++++++--batch_size: 4
++++++++++++++++++++-+batch_size: 1
++++++++++++++++++++- random_seed: 0 
++++++++++++++++++++--test_batch_size: 4
++++++++++++++++++++--num_worker: 20
++++++++++++++++++++-+test_batch_size: 1
++++++++++++++++++++-+num_worker: 3
++++++++++++++++++++- device: 0
++++++++++++++++++++- log_interval: 10000
++++++++++++++++++++- eval_interval: 1
++++++++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++++++++++++++++-index 555f4b8..c126e7a 100644
++++++++++++++++++++---- a/dataset/dataloader_video.py
++++++++++++++++++++-+++ b/dataset/dataloader_video.py
++++++++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++++++++++++++-+
++++++++++++++++++++-         print(mode, len(self))
++++++++++++++++++++-         self.data_aug = self.transform()
++++++++++++++++++++-         print("")
++++++++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++++++++++- 
++++++++++++++++++++-     def read_video(self, index):
++++++++++++++++++++--        # load file info
++++++++++++++++++++-         fi = self.inputs_list[index]
++++++++++++++++++++-+    
++++++++++++++++++++-         if 'phoenix' in self.dataset:
++++++++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++++++++++++++++-+#            print(img_list)
++++++++++++++++++++-+
++++++++++++++++++++-+#            print("[LOG] Using phoenix")
+++++++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++++++++++++++++index 7ac0c3b..c89f220 100644
+++++++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++++++++++++++++index 71519fd..b9664e1 100644
+++++++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++++++++++++++++index e69de29..0342039 100644
+++++++++++++++++++++--- a/tmp.ipynb
++++++++++++++++++++++++ b/tmp.ipynb
+++++++++++++++++++++@@ -0,0 +1,272 @@
++++++++++++++++++++++{
++++++++++++++++++++++ "cells": [
++++++++++++++++++++++  {
++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++   "execution_count": 2,
++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++   "outputs": [],
++++++++++++++++++++++   "source": [
++++++++++++++++++++++    "import torch\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++++++++++++++++   ]
++++++++++++++++++++++  },
++++++++++++++++++++++  {
++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++   "execution_count": 3,
++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++   "outputs": [
++++++++++++++++++++++    {
++++++++++++++++++++++     "name": "stdout",
++++++++++++++++++++++     "output_type": "stream",
++++++++++++++++++++++     "text": [
++++++++++++++++++++++      "Model is a state_dict.\n",
++++++++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++++++++++++++++     ]
++++++++++++++++++++++    }
++++++++++++++++++++++   ],
++++++++++++++++++++++   "source": [
++++++++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++++++++++++++++   ]
++++++++++++++++++++++  },
++++++++++++++++++++++  {
++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++   "execution_count": 7,
++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++   "outputs": [],
++++++++++++++++++++++   "source": [
++++++++++++++++++++++    "import torch\n",
++++++++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++++++++++++++++    "import torch.nn.functional as F\n"
++++++++++++++++++++++   ]
++++++++++++++++++++++  },
++++++++++++++++++++++  {
++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++   "execution_count": 1,
++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++   "outputs": [
++++++++++++++++++++++    {
++++++++++++++++++++++     "name": "stderr",
++++++++++++++++++++++     "output_type": "stream",
++++++++++++++++++++++     "text": [
++++++++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++++++++++++++++     ]
++++++++++++++++++++++    }
++++++++++++++++++++++   ],
++++++++++++++++++++++   "source": [
++++++++++++++++++++++    "import torch\n",
++++++++++++++++++++++    "import torch.nn as nn\n",
++++++++++++++++++++++    "import torch.nn.functional as F\n",
++++++++++++++++++++++    "import torchvision.models as models\n",
++++++++++++++++++++++    "import numpy as np\n",
++++++++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++++++++++++++++    "from modules.criterions import SeqKD\n",
++++++++++++++++++++++    "import utils\n",
++++++++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++++++++++++++++    "class Identity(nn.Module):\n",
++++++++++++++++++++++    "    def __init__(self):\n",
++++++++++++++++++++++    "        super(Identity, self).__init__()\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++++++++    "        return x\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# L2 정규화 선형 레이어\n",
++++++++++++++++++++++    "class NormLinear(nn.Module):\n",
++++++++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++++++++++++++++    "        super(NormLinear, self).__init__()\n",
++++++++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++++++++++++++++    "        return outputs\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++++++++++++++++    "class SLRModel(nn.Module):\n",
++++++++++++++++++++++    "    def __init__(\n",
++++++++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++++++++++++++++    "    ):\n",
++++++++++++++++++++++    "        super(SLRModel, self).__init__()\n",
++++++++++++++++++++++    "        self.decoder = None\n",
++++++++++++++++++++++    "        self.loss = dict()\n",
++++++++++++++++++++++    "        self.criterion_init()\n",
++++++++++++++++++++++    "        self.num_classes = num_classes\n",
++++++++++++++++++++++    "        self.loss_weights = loss_weights\n",
++++++++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++++++++++++++++    "                                   hidden_size=hidden_size,\n",
++++++++++++++++++++++    "                                   conv_type=conv_type,\n",
++++++++++++++++++++++    "                                   use_bn=use_bn,\n",
++++++++++++++++++++++    "                                   num_classes=num_classes)\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++++++++++++++++    "        if weight_norm:\n",
++++++++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++++++++    "        else:\n",
++++++++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        # Classifier 공유 여부\n",
++++++++++++++++++++++    "        if share_classifier:\n",
++++++++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++++++++++++++++    "        if len(x.shape) == 5:\n",
++++++++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++++++++++++++++    "        else:\n",
++++++++++++++++++++++    "            framewise = x\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        # 최종 Classifier 적용\n",
++++++++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        # Inference 모드에서 Decoding\n",
++++++++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "        return {\n",
++++++++++++++++++++++    "            \"framewise_features\": framewise,\n",
++++++++++++++++++++++    "            \"visual_features\": x,\n",
++++++++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++++++++++++++++    "            \"feat_len\": lgt,\n",
++++++++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++++++++++++++++    "            \"sequence_logits\": outputs,\n",
++++++++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++++++++++++++++    "            \"recognized_sents\": pred,\n",
++++++++++++++++++++++    "        }\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "    def criterion_init(self):\n",
++++++++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++++++++++++++++    "        return self.loss\n"
++++++++++++++++++++++   ]
++++++++++++++++++++++  },
++++++++++++++++++++++  {
++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++   "execution_count": 6,
++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++   "outputs": [
++++++++++++++++++++++    {
++++++++++++++++++++++     "ename": "KeyError",
++++++++++++++++++++++     "evalue": "'dataset_info'",
++++++++++++++++++++++     "output_type": "error",
++++++++++++++++++++++     "traceback": [
++++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++++++++++++++++     ]
++++++++++++++++++++++    }
++++++++++++++++++++++   ],
++++++++++++++++++++++   "source": [
++++++++++++++++++++++    "import os\n",
++++++++++++++++++++++    "import numpy as np\n",
++++++++++++++++++++++    "import yaml\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# 환경 변수 설정\n",
++++++++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++++++++++++++++    "with open(config_path, \"r\") as f:\n",
++++++++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# ✅ gloss_dict 로드\n",
++++++++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++++++++++++++++   ]
++++++++++++++++++++++  },
++++++++++++++++++++++  {
++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++   "execution_count": 5,
++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++   "outputs": [
++++++++++++++++++++++    {
++++++++++++++++++++++     "ename": "AttributeError",
++++++++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++++++++++++++++     "output_type": "error",
++++++++++++++++++++++     "traceback": [
++++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++++++++++++++++     ]
++++++++++++++++++++++    }
++++++++++++++++++++++   ],
++++++++++++++++++++++   "source": [
++++++++++++++++++++++    "import torch\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# 모델 불러오기\n",
++++++++++++++++++++++    "model = SLRModel(\n",
++++++++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++++++++++++++++    ")\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# 저장된 가중치 로드\n",
++++++++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++++++++++++++++    "if isinstance(state_dict, dict):\n",
++++++++++++++++++++++    "    model.load_state_dict(state_dict)\n",
++++++++++++++++++++++    "\n",
++++++++++++++++++++++    "# 모델을 평가 모드로 설정\n",
++++++++++++++++++++++    "model.eval()\n"
++++++++++++++++++++++   ]
++++++++++++++++++++++  }
++++++++++++++++++++++ ],
++++++++++++++++++++++ "metadata": {
++++++++++++++++++++++  "kernelspec": {
++++++++++++++++++++++   "display_name": "3.9.13",
++++++++++++++++++++++   "language": "python",
++++++++++++++++++++++   "name": "python3"
++++++++++++++++++++++  },
++++++++++++++++++++++  "language_info": {
++++++++++++++++++++++   "codemirror_mode": {
++++++++++++++++++++++    "name": "ipython",
++++++++++++++++++++++    "version": 3
++++++++++++++++++++++   },
++++++++++++++++++++++   "file_extension": ".py",
++++++++++++++++++++++   "mimetype": "text/x-python",
++++++++++++++++++++++   "name": "python",
++++++++++++++++++++++   "nbconvert_exporter": "python",
++++++++++++++++++++++   "pygments_lexer": "ipython3",
++++++++++++++++++++++   "version": "3.9.13"
++++++++++++++++++++++  }
++++++++++++++++++++++ },
++++++++++++++++++++++ "nbformat": 4,
++++++++++++++++++++++ "nbformat_minor": 2
++++++++++++++++++++++}
+++++++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++++++++++++++++index cb157af..8e90f4b 100644
+++++++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++++++++++++++++index 3877729..a90eb35 100644
+++++++++++++++++++++--- a/utils/decode.py
++++++++++++++++++++++++ b/utils/decode.py
+++++++++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
+++++++++++++++++++++ import numpy as np
+++++++++++++++++++++ from itertools import groupby
+++++++++++++++++++++ import torch.nn.functional as F
++++++++++++++++++++++import torch
++++++++++++++++++++++import matplotlib.pyplot as plt
++++++++++++++++++++++import numpy as np
++++++++++++++++++++ +
++++++++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++++++++++++++- 
++++++++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++++++++++++-+#            print(img_folder)
++++++++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++++++++++++++++-+#            print(len(img_list))
++++++++++++++++++++-         elif self.dataset == 'CSL-Daily':
++++++++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++++++++++++++++-+    
++++++++++++++++++++-         img_list = sorted(glob.glob(img_folder))
++++++++++++++++++++-+    
++++++++++++++++++++-+        if len(img_list) == 0:
++++++++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++++++++++++++++-+    
++++++++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
++++++++++++++++++++-+    
++++++++++++++++++++-         label_list = []
++++++++++++++++++++--        if self.dataset=='phoenix2014':
++++++++++++++++++++-+        if self.dataset == 'phoenix2014':
++++++++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++++++++++++++++--        if self.dataset=='phoenix2014-T':
++++++++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
++++++++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0):
++++++++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)  # (B, T)
++++++++++++++++++++ +    
++++++++++++++++++++-         for phase in fi['label'].split(" "):
++++++++++++++++++++--            if phase == '':
++++++++++++++++++++--                continue
++++++++++++++++++++--            if phase in self.dict.keys():
++++++++++++++++++++-+            if phase and phase in self.dict:
++++++++++++++++++++-                 label_list.append(self.dict[phase][0])
++++++++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++++++++++++++++-+    
++++++++++++++++++++-+        video = [
++++++++++++++++++++-+            cv2.cvtColor(
++++++++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++++++++-+                cv2.COLOR_BGR2RGB
++++++++++++++++++++-+            )   
++++++++++++++++++++-+            for img_path in img_list
++++++++++++++++++++-+        ]
++++++++++++++++++++-+    
++++++++++++++++++++-+        return video, label_list, fi
++++++++++++++++++++- 
++++++++++++++++++++-     def read_features(self, index):
++++++++++++++++++++-         # load file info
++++++++++++++++++++-diff --git a/main.py b/main.py
++++++++++++++++++++-index 9e68cee..18ac59b 100644
++++++++++++++++++++---- a/main.py
++++++++++++++++++++-+++ b/main.py
++++++++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
++++++++++++++++++++-                 batch_size=batch_size,
++++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++++++++-                 num_workers=self.arg.num_worker,
++++++++++++++++++++--                pin_memory=True,
++++++++++++++++++++-+                pin_memory=False,
++++++++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++++++++-             )
++++++++++++++++++++-             return loader
++++++++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
++++++++++++++++++++-                 drop_last=train_flag,
++++++++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++++++++--                pin_memory=True,
++++++++++++++++++++-+                pin_memory=False,
++++++++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++++++++-             )
++++++++++++++++++++- 
++++++++++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++++++++++++-index 528856d..d8fcaf9 100644
++++++++++++++++++++---- a/seq_scripts.py
++++++++++++++++++++-+++ b/seq_scripts.py
++++++++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++++++++++++++++-     return
++++++++++++++++++++- 
++++++++++++++++++++- 
++++++++++++++++++++-+import csv 
++++++++++++++++++++-+from jiwer import wer as jiwer_wer
++++++++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++++++++++++++++-     model.eval()
++++++++++++++++++++--    results=defaultdict(dict)
++++++++++++++++++++-+    results = defaultdict(dict)
++++++++++++++++++++- 
++++++++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++++++++++++++++-         recoder.record_timer("device")
++++++++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
++++++++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++++++++++++++++-                 results[inf]['gloss'] = gl
++++++++++++++++++++-+
++++++++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++++++++-+
++++++++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++++++++++++++++--        reg_per = wer_results
++++++++++++++++++++--    else:
++++++++++++++++++++--        reg_per = wer_results_con
++++++++++++++++++++-+
++++++++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++++++++++++++-+
++++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++++++++++++++++-         f"{work_dir}/{mode}.txt")
++++++++++++++++++++-+
++++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++++++++++++++++-+        f"{work_dir}/{mode}.txt")
++++++++++++++++++++-+
++++++++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
++++++++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++++++++++++++++++++-+
++++++++++++++++++++-+    rows = []
++++++++++++++++++++-+    for file_id in results:
++++++++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++++++++-+
++++++++++++++++++++-+        rows.append([
++++++++++++++++++++-+            file_id,
++++++++++++++++++++-+            gt,
++++++++++++++++++++-+            conv_pred,
++++++++++++++++++++-+            f"{conv_wer:.4f}",
++++++++++++++++++++-+            lstm_pred,
++++++++++++++++++++-+            f"{lstm_wer:.4f}"
++++++++++++++++++++-+        ])
++++++++++++++++++++-+
++++++++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++++++++++++++++-+        writer = csv.writer(f)
++++++++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++++++++++++++++-+        writer.writerows(rows)
++++++++++++++++++++-+
++++++++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
++++++++++++++++++++-+    sample_wers = []
++++++++++++++++++++-+    for file_id in results:
++++++++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++++++++-+    
++++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++++++++-+    
++++++++++++++++++++-+        sample_wers.append({
++++++++++++++++++++-+            'file_id': file_id,
++++++++++++++++++++-+            'gt': gt,
++++++++++++++++++++-+            'conv_pred': conv_pred,
++++++++++++++++++++-+            'conv_wer': conv_wer,
++++++++++++++++++++-+            'lstm_pred': lstm_pred,
++++++++++++++++++++-+            'lstm_wer': lstm_wer,
++++++++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++++++++++++++++-+        })
++++++++++++++++++++-+
++++++++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++++++++++-+    
++++++++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++++++++++-+    for sample in top5:
++++++++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++++++++++-+        print(f"GT   : {sample['gt']}")
++++++++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++++++++++++++++-+        print("-" * 60)
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++-+
++++++++++++++++++++++    length = int(vid_lgt[batch_idx].item())
++++++++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++++++++++++++ +
++++++++++++++++++++++    # X축: timestep
++++++++++++++++++++++    x = np.arange(length)
++++++++++++++++++++ +
++++++++++++++++++++++    # 선 그래프 전체 (steps-post 스타일)
++++++++++++++++++++++    plt.figure(figsize=(15, 4))
++++++++++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
++++++++++++++++++++ +
++++++++++++++++++++++    # Blank(0) 위치 마커 강조
++++++++++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
++++++++++++++++++++++    if len(blank_indices) > 0:
++++++++++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
++++++++++++++++++++ +
++++++++++++++++++++++    plt.title("Predicted Gloss ID over Time (CTC Blank Highlighted)")
++++++++++++++++++++++    plt.xlabel("Time Step")
++++++++++++++++++++++    plt.ylabel("Gloss ID")
++++++++++++++++++++++    plt.yticks(np.unique(pred_seq))
++++++++++++++++++++++    plt.grid(True)
++++++++++++++++++++++    plt.legend()
++++++++++++++++++++++    plt.tight_layout()
++++++++++++++++++++++    plt.show()
++++++++++++++++++++  
++++++++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++++++++++++++++ 
+++++++++++++++++++++ class Decode(object):
+++++++++++++++++++++@@ -22,6 +54,10 @@ class Decode(object):
+++++++++++++++++++++     def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++++++++++++++         if not batch_first:
+++++++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
++++++++++++++++++++++        
++++++++++++++++++++++        # time 별 class 시각화
++++++++++++++++++++++        plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0)
++++++++++++++++++++++
+++++++++++++++++++++         if self.search_mode == "max":
+++++++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++++++++++++++++         else:
+++++++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++++++++++++++++index 7d0a2aa..cd66258 100644
+++++++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++++++++++++++++index 8a22ece..541c633 100644
+++++++++++++++++++++--- a/work_dirt/dirty.patch
++++++++++++++++++++++++ b/work_dirt/dirty.patch
+++++++++++++++++++++@@ -1,284 +1,1645 @@
+++++++++++++++++++++-diff --git a/README.md b/README.md
+++++++++++++++++++++-index bdbc17f..8cb240b 100644
+++++++++++++++++++++---- a/README.md
+++++++++++++++++++++-+++ b/README.md
+++++++++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
+++++++++++++++++++++- 
+++++++++++++++++++++- 
+++++++++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++++++++++++++++- 
+++++++++++++++++++++- ### Training
+++++++++++++++++++++- 
+++++++++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++++++++++++++++-index bfc1da8..25ffa61 100644
+++++++++++++++++++++---- a/configs/baseline.yaml
+++++++++++++++++++++-+++ b/configs/baseline.yaml
+++++++++++++++++++++-@@ -1,14 +1,14 @@
+++++++++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++++++++++++++++- phase: train
+++++++++++++++++++++--dataset: phoenix2014-T
+++++++++++++++++++++-+dataset: phoenix2014
+++++++++++++++++++++- #CSL-Daily
+++++++++++++++++++++- # dataset: phoenix14-si5
++++++++++++++++++++ - 
++++++++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++++++++++++++++-diff --git a/slr_network.py b/slr_network.py
++++++++++++++++++++-index 45295cb..ede70cf 100644
++++++++++++++++++++---- a/slr_network.py
++++++++++++++++++++-+++ b/slr_network.py
++++++++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++++++++++++++++- work_dir: ./work_dirt/
+++++++++++++++++++++--batch_size: 4
+++++++++++++++++++++-+batch_size: 1
+++++++++++++++++++++- random_seed: 0 
+++++++++++++++++++++--test_batch_size: 4
+++++++++++++++++++++--num_worker: 20
+++++++++++++++++++++-+test_batch_size: 1
+++++++++++++++++++++-+num_worker: 3
+++++++++++++++++++++- device: 0
+++++++++++++++++++++- log_interval: 10000
+++++++++++++++++++++- eval_interval: 1
+++++++++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++++++++++++++++-index 555f4b8..c126e7a 100644
+++++++++++++++++++++---- a/dataset/dataloader_video.py
+++++++++++++++++++++-+++ b/dataset/dataloader_video.py
+++++++++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++++++++++++++++++-+
+++++++++++++++++++++-         print(mode, len(self))
+++++++++++++++++++++-         self.data_aug = self.transform()
+++++++++++++++++++++-         print("")
+++++++++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
+++++++++++++++++++++- 
+++++++++++++++++++++-     def read_video(self, index):
+++++++++++++++++++++--        # load file info
+++++++++++++++++++++-         fi = self.inputs_list[index]
+++++++++++++++++++++-+    
+++++++++++++++++++++-         if 'phoenix' in self.dataset:
+++++++++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++++++++++++++++-+#            print(img_list)
+++++++++++++++++++++-+
+++++++++++++++++++++-+#            print("[LOG] Using phoenix")
+++++++++++++++++++++-+
+++++++++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++++++++++++- 
+++++++++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++++++++++++-+#            print(img_folder)
+++++++++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++++++++++++++++-+#            print(len(img_list))
+++++++++++++++++++++-         elif self.dataset == 'CSL-Daily':
+++++++++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++++++++++++++++-+    
+++++++++++++++++++++-         img_list = sorted(glob.glob(img_folder))
+++++++++++++++++++++-+    
+++++++++++++++++++++-+        if len(img_list) == 0:
+++++++++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++++++++++++++++-+    
+++++++++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++++++++++++++-+    
+++++++++++++++++++++-         label_list = []
+++++++++++++++++++++--        if self.dataset=='phoenix2014':
+++++++++++++++++++++-+        if self.dataset == 'phoenix2014':
+++++++++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++++++++++++++++--        if self.dataset=='phoenix2014-T':
+++++++++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++++++++++++++++-+    
+++++++++++++++++++++-         for phase in fi['label'].split(" "):
+++++++++++++++++++++--            if phase == '':
+++++++++++++++++++++--                continue
+++++++++++++++++++++--            if phase in self.dict.keys():
+++++++++++++++++++++-+            if phase and phase in self.dict:
+++++++++++++++++++++-                 label_list.append(self.dict[phase][0])
+++++++++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++++++++++++++++-+    
+++++++++++++++++++++-+        video = [
+++++++++++++++++++++-+            cv2.cvtColor(
+++++++++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++++++++-+                cv2.COLOR_BGR2RGB
+++++++++++++++++++++-+            )   
+++++++++++++++++++++-+            for img_path in img_list
+++++++++++++++++++++-+        ]
+++++++++++++++++++++-+    
+++++++++++++++++++++-+        return video, label_list, fi
+++++++++++++++++++++- 
+++++++++++++++++++++-     def read_features(self, index):
+++++++++++++++++++++-         # load file info
+++++++++++++++++++++-diff --git a/main.py b/main.py
+++++++++++++++++++++-index 9e68cee..18ac59b 100644
+++++++++++++++++++++---- a/main.py
+++++++++++++++++++++-+++ b/main.py
+++++++++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++++++++++++++++-                 batch_size=batch_size,
+++++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++++++++-                 num_workers=self.arg.num_worker,
+++++++++++++++++++++--                pin_memory=True,
+++++++++++++++++++++-+                pin_memory=False,
+++++++++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++++++++-             )
+++++++++++++++++++++-             return loader
+++++++++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++++++++++++++++-                 drop_last=train_flag,
+++++++++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++++++++--                pin_memory=True,
+++++++++++++++++++++-+                pin_memory=False,
+++++++++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++++++++-             )
+++++++++++++++++++++- 
+++++++++++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++++++++++++-index 528856d..d8fcaf9 100644
+++++++++++++++++++++---- a/seq_scripts.py
+++++++++++++++++++++-+++ b/seq_scripts.py
+++++++++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++++++++++++++++-     return
+++++++++++++++++++++- 
+++++++++++++++++++++- 
+++++++++++++++++++++-+import csv 
+++++++++++++++++++++-+from jiwer import wer as jiwer_wer
+++++++++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++++++++++++++++-     model.eval()
+++++++++++++++++++++--    results=defaultdict(dict)
+++++++++++++++++++++-+    results = defaultdict(dict)
+++++++++++++++++++++- 
+++++++++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++++++++++++++++-         recoder.record_timer("device")
+++++++++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++++++++++++++++-                 results[inf]['gloss'] = gl
+++++++++++++++++++++-+
+++++++++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++++++++-+
+++++++++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++++++++++++++++--        reg_per = wer_results
+++++++++++++++++++++--    else:
+++++++++++++++++++++--        reg_per = wer_results_con
+++++++++++++++++++++-+
+++++++++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++++++++++++++-+
+++++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++++++++++++++++-         f"{work_dir}/{mode}.txt")
+++++++++++++++++++++-+
+++++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++++++++++++++++-+        f"{work_dir}/{mode}.txt")
+++++++++++++++++++++-+
+++++++++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++++++++++++++-+
+++++++++++++++++++++-+    rows = []
+++++++++++++++++++++-+    for file_id in results:
+++++++++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++++++++-+
+++++++++++++++++++++-+        rows.append([
+++++++++++++++++++++-+            file_id,
+++++++++++++++++++++-+            gt,
+++++++++++++++++++++-+            conv_pred,
+++++++++++++++++++++-+            f"{conv_wer:.4f}",
+++++++++++++++++++++-+            lstm_pred,
+++++++++++++++++++++-+            f"{lstm_wer:.4f}"
+++++++++++++++++++++-+        ])
+++++++++++++++++++++-+
+++++++++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++++++++++++++++-+        writer = csv.writer(f)
+++++++++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++++++++++++++++-+        writer.writerows(rows)
+++++++++++++++++++++-+
+++++++++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++++++++++++-+
+++++++++++++++++++++-+
++++++++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++++++++++++++++index 7ac0c3b..c89f220 100644
++++++++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++++++++++++++++index 71519fd..b9664e1 100644
++++++++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++++++++++++++++index e69de29..0342039 100644
++++++++++++++++++++++--- a/tmp.ipynb
+++++++++++++++++++++++++ b/tmp.ipynb
++++++++++++++++++++++@@ -0,0 +1,272 @@
+++++++++++++++++++++++{
+++++++++++++++++++++++ "cells": [
+++++++++++++++++++++++  {
+++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++   "execution_count": 2,
+++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++   "outputs": [],
+++++++++++++++++++++++   "source": [
+++++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++++++++++++++++   ]
+++++++++++++++++++++++  },
+++++++++++++++++++++++  {
+++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++   "execution_count": 3,
+++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++   "outputs": [
+++++++++++++++++++++++    {
+++++++++++++++++++++++     "name": "stdout",
+++++++++++++++++++++++     "output_type": "stream",
+++++++++++++++++++++++     "text": [
+++++++++++++++++++++++      "Model is a state_dict.\n",
+++++++++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++++++++++++++++     ]
+++++++++++++++++++++++    }
+++++++++++++++++++++++   ],
+++++++++++++++++++++++   "source": [
+++++++++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++++++++++++++++   ]
+++++++++++++++++++++++  },
+++++++++++++++++++++++  {
+++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++   "execution_count": 7,
+++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++   "outputs": [],
+++++++++++++++++++++++   "source": [
+++++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++++++++++++++++    "import torch.nn.functional as F\n"
+++++++++++++++++++++++   ]
+++++++++++++++++++++++  },
+++++++++++++++++++++++  {
+++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++   "execution_count": 1,
+++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++   "outputs": [
+++++++++++++++++++++++    {
+++++++++++++++++++++++     "name": "stderr",
+++++++++++++++++++++++     "output_type": "stream",
+++++++++++++++++++++++     "text": [
+++++++++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++++++++++++++++     ]
+++++++++++++++++++++++    }
+++++++++++++++++++++++   ],
+++++++++++++++++++++++   "source": [
+++++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++++    "import torch.nn as nn\n",
+++++++++++++++++++++++    "import torch.nn.functional as F\n",
+++++++++++++++++++++++    "import torchvision.models as models\n",
+++++++++++++++++++++++    "import numpy as np\n",
+++++++++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++++++++++++++++    "from modules.criterions import SeqKD\n",
+++++++++++++++++++++++    "import utils\n",
+++++++++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++++++++++++++++    "class Identity(nn.Module):\n",
+++++++++++++++++++++++    "    def __init__(self):\n",
+++++++++++++++++++++++    "        super(Identity, self).__init__()\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++++++++    "        return x\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# L2 정규화 선형 레이어\n",
+++++++++++++++++++++++    "class NormLinear(nn.Module):\n",
+++++++++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++++++++++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++++++++++++++++    "        return outputs\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++++++++++++++++    "class SLRModel(nn.Module):\n",
+++++++++++++++++++++++    "    def __init__(\n",
+++++++++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++++++++++++++++    "    ):\n",
+++++++++++++++++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++++++++++++++++    "        self.decoder = None\n",
+++++++++++++++++++++++    "        self.loss = dict()\n",
+++++++++++++++++++++++    "        self.criterion_init()\n",
+++++++++++++++++++++++    "        self.num_classes = num_classes\n",
+++++++++++++++++++++++    "        self.loss_weights = loss_weights\n",
+++++++++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++++++++++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++++++++++++++++    "                                   conv_type=conv_type,\n",
+++++++++++++++++++++++    "                                   use_bn=use_bn,\n",
+++++++++++++++++++++++    "                                   num_classes=num_classes)\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++++++++++++++++    "        if weight_norm:\n",
+++++++++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++++    "        else:\n",
+++++++++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        # Classifier 공유 여부\n",
+++++++++++++++++++++++    "        if share_classifier:\n",
+++++++++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++++++++++++++++    "        if len(x.shape) == 5:\n",
+++++++++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++++++++++++++++    "        else:\n",
+++++++++++++++++++++++    "            framewise = x\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        # 최종 Classifier 적용\n",
+++++++++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "        return {\n",
+++++++++++++++++++++++    "            \"framewise_features\": framewise,\n",
+++++++++++++++++++++++    "            \"visual_features\": x,\n",
+++++++++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++++++++++++++++    "            \"feat_len\": lgt,\n",
+++++++++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++++++++++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++++++++++++++++    "            \"recognized_sents\": pred,\n",
+++++++++++++++++++++++    "        }\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "    def criterion_init(self):\n",
+++++++++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++++++++++++++++    "        return self.loss\n"
+++++++++++++++++++++++   ]
+++++++++++++++++++++++  },
+++++++++++++++++++++++  {
+++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++   "execution_count": 6,
+++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++   "outputs": [
+++++++++++++++++++++++    {
+++++++++++++++++++++++     "ename": "KeyError",
+++++++++++++++++++++++     "evalue": "'dataset_info'",
+++++++++++++++++++++++     "output_type": "error",
+++++++++++++++++++++++     "traceback": [
+++++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++++++++++++++++     ]
+++++++++++++++++++++++    }
+++++++++++++++++++++++   ],
+++++++++++++++++++++++   "source": [
+++++++++++++++++++++++    "import os\n",
+++++++++++++++++++++++    "import numpy as np\n",
+++++++++++++++++++++++    "import yaml\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# 환경 변수 설정\n",
+++++++++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++++++++++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# ✅ gloss_dict 로드\n",
+++++++++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++++++++++++++++   ]
+++++++++++++++++++++++  },
+++++++++++++++++++++++  {
+++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++   "execution_count": 5,
+++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++   "outputs": [
+++++++++++++++++++++++    {
+++++++++++++++++++++++     "ename": "AttributeError",
+++++++++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++++++++++++++++     "output_type": "error",
+++++++++++++++++++++++     "traceback": [
+++++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++++++++++++++++     ]
+++++++++++++++++++++++    }
+++++++++++++++++++++++   ],
+++++++++++++++++++++++   "source": [
+++++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# 모델 불러오기\n",
+++++++++++++++++++++++    "model = SLRModel(\n",
+++++++++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++++++++++++++++    ")\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# 저장된 가중치 로드\n",
+++++++++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++++++++++++++++    "if isinstance(state_dict, dict):\n",
+++++++++++++++++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++++++++++++++++    "\n",
+++++++++++++++++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++++++++++++++++    "model.eval()\n"
+++++++++++++++++++++++   ]
+++++++++++++++++++++++  }
+++++++++++++++++++++++ ],
+++++++++++++++++++++++ "metadata": {
+++++++++++++++++++++++  "kernelspec": {
+++++++++++++++++++++++   "display_name": "3.9.13",
+++++++++++++++++++++++   "language": "python",
+++++++++++++++++++++++   "name": "python3"
+++++++++++++++++++++++  },
+++++++++++++++++++++++  "language_info": {
+++++++++++++++++++++++   "codemirror_mode": {
+++++++++++++++++++++++    "name": "ipython",
+++++++++++++++++++++++    "version": 3
+++++++++++++++++++++++   },
+++++++++++++++++++++++   "file_extension": ".py",
+++++++++++++++++++++++   "mimetype": "text/x-python",
+++++++++++++++++++++++   "name": "python",
+++++++++++++++++++++++   "nbconvert_exporter": "python",
+++++++++++++++++++++++   "pygments_lexer": "ipython3",
+++++++++++++++++++++++   "version": "3.9.13"
+++++++++++++++++++++++  }
+++++++++++++++++++++++ },
+++++++++++++++++++++++ "nbformat": 4,
+++++++++++++++++++++++ "nbformat_minor": 2
+++++++++++++++++++++++}
++++++++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++++++++++++++++index cb157af..8e90f4b 100644
++++++++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++++++++++++++++index 3877729..a90eb35 100644
++++++++++++++++++++++--- a/utils/decode.py
+++++++++++++++++++++++++ b/utils/decode.py
++++++++++++++++++++++@@ -6,6 +6,38 @@ import ctcdecode
++++++++++++++++++++++ import numpy as np
++++++++++++++++++++++ from itertools import groupby
++++++++++++++++++++++ import torch.nn.functional as F
+++++++++++++++++++++++import torch
+++++++++++++++++++++++import matplotlib.pyplot as plt
+++++++++++++++++++++++import numpy as np
+++++++++++++++++++++ +
+++++++++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++++++++++++++++-+    sample_wers = []
+++++++++++++++++++++-+    for file_id in results:
+++++++++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0, blank_id=0):
+++++++++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)  # (B, T)
+++++++++++++++++++++ +    
+++++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++++++++-+    
+++++++++++++++++++++-+        sample_wers.append({
+++++++++++++++++++++-+            'file_id': file_id,
+++++++++++++++++++++-+            'gt': gt,
+++++++++++++++++++++-+            'conv_pred': conv_pred,
+++++++++++++++++++++-+            'conv_wer': conv_wer,
+++++++++++++++++++++-+            'lstm_pred': lstm_pred,
+++++++++++++++++++++-+            'lstm_wer': lstm_wer,
+++++++++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++++++++++++++++-+        })
+++++++++++++++++++++-+
+++++++++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++++++++++++-+    
+++++++++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++++++++++++-+    for sample in top5:
+++++++++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++++++++++++-+        print(f"GT   : {sample['gt']}")
+++++++++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++++++++++++++++-+        print("-" * 60)
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++-+
+++++++++++++++++++++++    length = int(vid_lgt[batch_idx].item())
+++++++++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++++++++++++++ +
+++++++++++++++++++++++    # X축: timestep
+++++++++++++++++++++++    x = np.arange(length)
+++++++++++++++++++++ +
+++++++++++++++++++++++    # 선 그래프 전체 (steps-post 스타일)
+++++++++++++++++++++++    plt.figure(figsize=(15, 4))
+++++++++++++++++++++++    plt.plot(x, pred_seq, drawstyle='steps-post', label='Predicted Gloss ID', linewidth=1.5)
+++++++++++++++++++++ +
+++++++++++++++++++++++    # Blank(0) 위치 마커 강조
+++++++++++++++++++++++    blank_indices = np.where(pred_seq == blank_id)[0]
+++++++++++++++++++++++    if len(blank_indices) > 0:
+++++++++++++++++++++++        plt.scatter(blank_indices, pred_seq[blank_indices], color='red', marker='x', label='CTC Blank (0)', zorder=5)
+++++++++++++++++++++ +
+++++++++++++++++++++++    plt.title("Predicted Gloss ID over Time (CTC Blank Highlighted)")
+++++++++++++++++++++++    plt.xlabel("Time Step")
+++++++++++++++++++++++    plt.ylabel("Gloss ID")
+++++++++++++++++++++++    plt.yticks(np.unique(pred_seq))
+++++++++++++++++++++++    plt.grid(True)
+++++++++++++++++++++++    plt.legend()
+++++++++++++++++++++++    plt.tight_layout()
+++++++++++++++++++++++    plt.show()
++++++++++++++++++++++ 
+++++++++++++++++++++  
+++++++++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++++++++++++++++++ class Decode(object):
++++++++++++++++++++++@@ -22,6 +54,10 @@ class Decode(object):
++++++++++++++++++++++     def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++++++++++++++++         if not batch_first:
++++++++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++++++++++++++++++        
+++++++++++++++++++++++        # time 별 class 시각화
+++++++++++++++++++++++        plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0)
+++++++++++++++++++++++
++++++++++++++++++++++         if self.search_mode == "max":
++++++++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++++++++++++++++         else:
++++++++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++++++++++++++++index 7d0a2aa..cd66258 100644
++++++++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
++++++++++++++++++++++index 8a22ece..6197828 100644
++++++++++++++++++++++--- a/work_dirt/dirty.patch
+++++++++++++++++++++++++ b/work_dirt/dirty.patch
++++++++++++++++++++++@@ -1,284 +1,995 @@
++++++++++++++++++++++-diff --git a/README.md b/README.md
++++++++++++++++++++++-index bdbc17f..8cb240b 100644
++++++++++++++++++++++---- a/README.md
++++++++++++++++++++++-+++ b/README.md
++++++++++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++++++++++++++++- 
++++++++++++++++++++++- 
++++++++++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
++++++++++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
++++++++++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
++++++++++++++++++++++- 
++++++++++++++++++++++- ### Training
+++++++++++++++++++++ - 
+++++++++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++++++++++++++++-diff --git a/slr_network.py b/slr_network.py
+++++++++++++++++++++-index 45295cb..ede70cf 100644
+++++++++++++++++++++---- a/slr_network.py
+++++++++++++++++++++-+++ b/slr_network.py
+++++++++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
++++++++++++++++++++++-index bfc1da8..25ffa61 100644
++++++++++++++++++++++---- a/configs/baseline.yaml
++++++++++++++++++++++-+++ b/configs/baseline.yaml
++++++++++++++++++++++-@@ -1,14 +1,14 @@
++++++++++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
++++++++++++++++++++++- phase: train
++++++++++++++++++++++--dataset: phoenix2014-T
++++++++++++++++++++++-+dataset: phoenix2014
++++++++++++++++++++++- #CSL-Daily
++++++++++++++++++++++- # dataset: phoenix14-si5
++++++++++++++++++++++- 
++++++++++++++++++++++- work_dir: ./work_dirt/
++++++++++++++++++++++--batch_size: 4
++++++++++++++++++++++-+batch_size: 1
++++++++++++++++++++++- random_seed: 0 
++++++++++++++++++++++--test_batch_size: 4
++++++++++++++++++++++--num_worker: 20
++++++++++++++++++++++-+test_batch_size: 1
++++++++++++++++++++++-+num_worker: 3
++++++++++++++++++++++- device: 0
++++++++++++++++++++++- log_interval: 10000
++++++++++++++++++++++- eval_interval: 1
++++++++++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
++++++++++++++++++++++-index 555f4b8..c126e7a 100644
++++++++++++++++++++++---- a/dataset/dataloader_video.py
++++++++++++++++++++++-+++ b/dataset/dataloader_video.py
++++++++++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
++++++++++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
++++++++++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
++++++++++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
++++++++++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
++++++++++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
++++++++++++++++++++++-+
++++++++++++++++++++++-         print(mode, len(self))
++++++++++++++++++++++-         self.data_aug = self.transform()
++++++++++++++++++++++-         print("")
++++++++++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
++++++++++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++++++++++++- 
++++++++++++++++++++++-     def read_video(self, index):
++++++++++++++++++++++--        # load file info
++++++++++++++++++++++-         fi = self.inputs_list[index]
++++++++++++++++++++++-+    
++++++++++++++++++++++-         if 'phoenix' in self.dataset:
++++++++++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
++++++++++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
++++++++++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
++++++++++++++++++++++-+#            print(img_list)
++++++++++++++++++++++-+
++++++++++++++++++++++-+#            print("[LOG] Using phoenix")
++++++++++++++++++++++-+
++++++++++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
++++++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
++++++++++++++++++++++- 
++++++++++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
++++++++++++++++++++++-+#            print(img_folder)
++++++++++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
++++++++++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
++++++++++++++++++++++-+#            print(len(img_list))
++++++++++++++++++++++-         elif self.dataset == 'CSL-Daily':
++++++++++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
++++++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
++++++++++++++++++++++-+    
++++++++++++++++++++++-         img_list = sorted(glob.glob(img_folder))
++++++++++++++++++++++-+    
++++++++++++++++++++++-+        if len(img_list) == 0:
++++++++++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
++++++++++++++++++++++-+    
++++++++++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
+++++++++++++++++++++++index 7ac0c3b..c89f220 100644
+++++++++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
+++++++++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
+++++++++++++++++++++++index 71519fd..b9664e1 100644
+++++++++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
+++++++++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
+++++++++++++++++++++++index e69de29..0342039 100644
+++++++++++++++++++++++--- a/tmp.ipynb
++++++++++++++++++++++++++ b/tmp.ipynb
+++++++++++++++++++++++@@ -0,0 +1,272 @@
++++++++++++++++++++++++{
++++++++++++++++++++++++ "cells": [
++++++++++++++++++++++++  {
++++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++++   "execution_count": 2,
++++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++++   "outputs": [],
++++++++++++++++++++++++   "source": [
++++++++++++++++++++++++    "import torch\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
++++++++++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
++++++++++++++++++++++++   ]
++++++++++++++++++++++++  },
++++++++++++++++++++++++  {
++++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++++   "execution_count": 3,
++++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++++   "outputs": [
++++++++++++++++++++++++    {
++++++++++++++++++++++++     "name": "stdout",
++++++++++++++++++++++++     "output_type": "stream",
++++++++++++++++++++++++     "text": [
++++++++++++++++++++++++      "Model is a state_dict.\n",
++++++++++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
++++++++++++++++++++++++     ]
++++++++++++++++++++++++    }
++++++++++++++++++++++++   ],
++++++++++++++++++++++++   "source": [
++++++++++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
++++++++++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
++++++++++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
++++++++++++++++++++++++   ]
++++++++++++++++++++++++  },
++++++++++++++++++++++++  {
++++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++++   "execution_count": 7,
++++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++++   "outputs": [],
++++++++++++++++++++++++   "source": [
++++++++++++++++++++++++    "import torch\n",
++++++++++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
++++++++++++++++++++++++    "import torch.nn.functional as F\n"
++++++++++++++++++++++++   ]
++++++++++++++++++++++++  },
++++++++++++++++++++++++  {
++++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++++   "execution_count": 1,
++++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++++   "outputs": [
++++++++++++++++++++++++    {
++++++++++++++++++++++++     "name": "stderr",
++++++++++++++++++++++++     "output_type": "stream",
++++++++++++++++++++++++     "text": [
++++++++++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
++++++++++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
++++++++++++++++++++++++     ]
++++++++++++++++++++++++    }
++++++++++++++++++++++++   ],
++++++++++++++++++++++++   "source": [
++++++++++++++++++++++++    "import torch\n",
++++++++++++++++++++++++    "import torch.nn as nn\n",
++++++++++++++++++++++++    "import torch.nn.functional as F\n",
++++++++++++++++++++++++    "import torchvision.models as models\n",
++++++++++++++++++++++++    "import numpy as np\n",
++++++++++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
++++++++++++++++++++++++    "from modules.criterions import SeqKD\n",
++++++++++++++++++++++++    "import utils\n",
++++++++++++++++++++++++    "import modules.resnet as resnet\n",
++++++++++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
++++++++++++++++++++++++    "class Identity(nn.Module):\n",
++++++++++++++++++++++++    "    def __init__(self):\n",
++++++++++++++++++++++++    "        super(Identity, self).__init__()\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++++++++++    "        return x\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# L2 정규화 선형 레이어\n",
++++++++++++++++++++++++    "class NormLinear(nn.Module):\n",
++++++++++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
++++++++++++++++++++++++    "        super(NormLinear, self).__init__()\n",
++++++++++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
++++++++++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "    def forward(self, x):\n",
++++++++++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
++++++++++++++++++++++++    "        return outputs\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
++++++++++++++++++++++++    "class SLRModel(nn.Module):\n",
++++++++++++++++++++++++    "    def __init__(\n",
++++++++++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
++++++++++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
++++++++++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
++++++++++++++++++++++++    "    ):\n",
++++++++++++++++++++++++    "        super(SLRModel, self).__init__()\n",
++++++++++++++++++++++++    "        self.decoder = None\n",
++++++++++++++++++++++++    "        self.loss = dict()\n",
++++++++++++++++++++++++    "        self.criterion_init()\n",
++++++++++++++++++++++++    "        self.num_classes = num_classes\n",
++++++++++++++++++++++++    "        self.loss_weights = loss_weights\n",
++++++++++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
++++++++++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
++++++++++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
++++++++++++++++++++++++    "                                   hidden_size=hidden_size,\n",
++++++++++++++++++++++++    "                                   conv_type=conv_type,\n",
++++++++++++++++++++++++    "                                   use_bn=use_bn,\n",
++++++++++++++++++++++++    "                                   num_classes=num_classes)\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
++++++++++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
++++++++++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
++++++++++++++++++++++++    "        if weight_norm:\n",
++++++++++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
++++++++++++++++++++++++    "        else:\n",
++++++++++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        # Classifier 공유 여부\n",
++++++++++++++++++++++++    "        if share_classifier:\n",
++++++++++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
++++++++++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
++++++++++++++++++++++++    "        if len(x.shape) == 5:\n",
++++++++++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
++++++++++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
++++++++++++++++++++++++    "        else:\n",
++++++++++++++++++++++++    "            framewise = x\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
++++++++++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
++++++++++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
++++++++++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
++++++++++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        # 최종 Classifier 적용\n",
++++++++++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        # Inference 모드에서 Decoding\n",
++++++++++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
++++++++++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "        return {\n",
++++++++++++++++++++++++    "            \"framewise_features\": framewise,\n",
++++++++++++++++++++++++    "            \"visual_features\": x,\n",
++++++++++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
++++++++++++++++++++++++    "            \"feat_len\": lgt,\n",
++++++++++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
++++++++++++++++++++++++    "            \"sequence_logits\": outputs,\n",
++++++++++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
++++++++++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
++++++++++++++++++++++++    "            \"recognized_sents\": pred,\n",
++++++++++++++++++++++++    "        }\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "    def criterion_init(self):\n",
++++++++++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
++++++++++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
++++++++++++++++++++++++    "        return self.loss\n"
++++++++++++++++++++++++   ]
++++++++++++++++++++++++  },
++++++++++++++++++++++++  {
++++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++++   "execution_count": 6,
++++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++++   "outputs": [
++++++++++++++++++++++++    {
++++++++++++++++++++++++     "ename": "KeyError",
++++++++++++++++++++++++     "evalue": "'dataset_info'",
++++++++++++++++++++++++     "output_type": "error",
++++++++++++++++++++++++     "traceback": [
++++++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
++++++++++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
++++++++++++++++++++++++     ]
++++++++++++++++++++++++    }
++++++++++++++++++++++++   ],
++++++++++++++++++++++++   "source": [
++++++++++++++++++++++++    "import os\n",
++++++++++++++++++++++++    "import numpy as np\n",
++++++++++++++++++++++++    "import yaml\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# 환경 변수 설정\n",
++++++++++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
++++++++++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
++++++++++++++++++++++++    "with open(config_path, \"r\") as f:\n",
++++++++++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# ✅ gloss_dict 로드\n",
++++++++++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
++++++++++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
++++++++++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
++++++++++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
++++++++++++++++++++++++   ]
++++++++++++++++++++++++  },
++++++++++++++++++++++++  {
++++++++++++++++++++++++   "cell_type": "code",
++++++++++++++++++++++++   "execution_count": 5,
++++++++++++++++++++++++   "metadata": {},
++++++++++++++++++++++++   "outputs": [
++++++++++++++++++++++++    {
++++++++++++++++++++++++     "ename": "AttributeError",
++++++++++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
++++++++++++++++++++++++     "output_type": "error",
++++++++++++++++++++++++     "traceback": [
++++++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
++++++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
++++++++++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
++++++++++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
++++++++++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
++++++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
++++++++++++++++++++++++     ]
++++++++++++++++++++++++    }
++++++++++++++++++++++++   ],
++++++++++++++++++++++++   "source": [
++++++++++++++++++++++++    "import torch\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# 모델 불러오기\n",
++++++++++++++++++++++++    "model = SLRModel(\n",
++++++++++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
++++++++++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
++++++++++++++++++++++++    ")\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# 저장된 가중치 로드\n",
++++++++++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
++++++++++++++++++++++++    "if isinstance(state_dict, dict):\n",
++++++++++++++++++++++++    "    model.load_state_dict(state_dict)\n",
++++++++++++++++++++++++    "\n",
++++++++++++++++++++++++    "# 모델을 평가 모드로 설정\n",
++++++++++++++++++++++++    "model.eval()\n"
++++++++++++++++++++++++   ]
++++++++++++++++++++++++  }
++++++++++++++++++++++++ ],
++++++++++++++++++++++++ "metadata": {
++++++++++++++++++++++++  "kernelspec": {
++++++++++++++++++++++++   "display_name": "3.9.13",
++++++++++++++++++++++++   "language": "python",
++++++++++++++++++++++++   "name": "python3"
++++++++++++++++++++++++  },
++++++++++++++++++++++++  "language_info": {
++++++++++++++++++++++++   "codemirror_mode": {
++++++++++++++++++++++++    "name": "ipython",
++++++++++++++++++++++++    "version": 3
++++++++++++++++++++++++   },
++++++++++++++++++++++++   "file_extension": ".py",
++++++++++++++++++++++++   "mimetype": "text/x-python",
++++++++++++++++++++++++   "name": "python",
++++++++++++++++++++++++   "nbconvert_exporter": "python",
++++++++++++++++++++++++   "pygments_lexer": "ipython3",
++++++++++++++++++++++++   "version": "3.9.13"
++++++++++++++++++++++++  }
++++++++++++++++++++++++ },
++++++++++++++++++++++++ "nbformat": 4,
++++++++++++++++++++++++ "nbformat_minor": 2
++++++++++++++++++++++++}
+++++++++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
+++++++++++++++++++++++index cb157af..5af45d8 100644
+++++++++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
+++++++++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
+++++++++++++++++++++++index 3877729..abeb9db 100644
+++++++++++++++++++++++--- a/utils/decode.py
++++++++++++++++++++++++++ b/utils/decode.py
+++++++++++++++++++++++@@ -6,6 +6,33 @@ import ctcdecode
+++++++++++++++++++++++ import numpy as np
+++++++++++++++++++++++ from itertools import groupby
+++++++++++++++++++++++ import torch.nn.functional as F
++++++++++++++++++++++++import torch
++++++++++++++++++++++++import matplotlib.pyplot as plt
++++++++++++++++++++++++import numpy as np
++++++++++++++++++++++++
++++++++++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0):
++++++++++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
++++++++++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
++++++++++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)  # (B, T)
++++++++++++++++++++++ +    
++++++++++++++++++++++-         label_list = []
++++++++++++++++++++++--        if self.dataset=='phoenix2014':
++++++++++++++++++++++-+        if self.dataset == 'phoenix2014':
++++++++++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
++++++++++++++++++++++--        if self.dataset=='phoenix2014-T':
++++++++++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
++++++++++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
++++++++++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
++++++++++++++++++++++++    # 🔧 수정된 부분: 명시적으로 int 변환
++++++++++++++++++++++++    length = int(vid_lgt[batch_idx].item())
++++++++++++++++++++++ +    
++++++++++++++++++++++-         for phase in fi['label'].split(" "):
++++++++++++++++++++++--            if phase == '':
++++++++++++++++++++++--                continue
++++++++++++++++++++++--            if phase in self.dict.keys():
++++++++++++++++++++++-+            if phase and phase in self.dict:
++++++++++++++++++++++-                 label_list.append(self.dict[phase][0])
++++++++++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
++++++++++++++++++++++-+    
++++++++++++++++++++++-+        video = [
++++++++++++++++++++++-+            cv2.cvtColor(
++++++++++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
++++++++++++++++++++++-+                cv2.COLOR_BGR2RGB
++++++++++++++++++++++-+            )   
++++++++++++++++++++++-+            for img_path in img_list
++++++++++++++++++++++-+        ]
++++++++++++++++++++++-+    
++++++++++++++++++++++-+        return video, label_list, fi
++++++++++++++++++++++- 
++++++++++++++++++++++-     def read_features(self, index):
++++++++++++++++++++++-         # load file info
++++++++++++++++++++++-diff --git a/main.py b/main.py
++++++++++++++++++++++-index 9e68cee..18ac59b 100644
++++++++++++++++++++++---- a/main.py
++++++++++++++++++++++-+++ b/main.py
++++++++++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
++++++++++++++++++++++-                 batch_size=batch_size,
++++++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++++++++++-                 num_workers=self.arg.num_worker,
++++++++++++++++++++++--                pin_memory=True,
++++++++++++++++++++++-+                pin_memory=False,
++++++++++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++++++++++-             )
++++++++++++++++++++++-             return loader
++++++++++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
++++++++++++++++++++++-                 drop_last=train_flag,
++++++++++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
++++++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
++++++++++++++++++++++--                pin_memory=True,
++++++++++++++++++++++-+                pin_memory=False,
++++++++++++++++++++++-                 worker_init_fn=self.init_fn,
++++++++++++++++++++++-             )
++++++++++++++++++++++- 
++++++++++++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
++++++++++++++++++++++-index 528856d..d8fcaf9 100644
++++++++++++++++++++++---- a/seq_scripts.py
++++++++++++++++++++++-+++ b/seq_scripts.py
++++++++++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
++++++++++++++++++++++-     return
++++++++++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
++++++++++++++++++++++++    gloss_names = [i2g_dict.get(int(idx), "UNK") for idx in pred_seq]
++++++++++++++++++++++++
++++++++++++++++++++++++    import matplotlib.pyplot as plt
++++++++++++++++++++++++    import numpy as np
++++++++++++++++++++++++
++++++++++++++++++++++++    plt.figure(figsize=(15, 3))
++++++++++++++++++++++++    plt.plot(pred_seq, drawstyle='steps-post', linewidth=1.5)
++++++++++++++++++++++++    plt.title("Predicted Gloss ID over Time")
++++++++++++++++++++++++    plt.xlabel("Time Step")
++++++++++++++++++++++++    plt.ylabel("Gloss ID")
++++++++++++++++++++++++    plt.yticks(np.unique(pred_seq))
++++++++++++++++++++++++    plt.grid(True)
++++++++++++++++++++++++    plt.tight_layout()
++++++++++++++++++++++++    plt.show()
++++++++++++++++++++++  
++++++++++++++++++++++  
++++++++++++++++++++++-+import csv 
++++++++++++++++++++++-+from jiwer import wer as jiwer_wer
++++++++++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
++++++++++++++++++++++-     model.eval()
++++++++++++++++++++++--    results=defaultdict(dict)
++++++++++++++++++++++-+    results = defaultdict(dict)
++++++++++++++++++++++- 
++++++++++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
++++++++++++++++++++++-         recoder.record_timer("device")
++++++++++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
++++++++++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
++++++++++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
++++++++++++++++++++++-                 results[inf]['gloss'] = gl
++++++++++++++++++++++-+
++++++++++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
++++++++++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
++++++++++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++++++++++-+
++++++++++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
++++++++++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
++++++++++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
++++++++++++++++++++++--        reg_per = wer_results
++++++++++++++++++++++--    else:
++++++++++++++++++++++--        reg_per = wer_results_con
++++++++++++++++++++++-+
++++++++++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
++++++++++++++++++++++-+
++++++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
++++++++++++++++++++++-         f"{work_dir}/{mode}.txt")
++++++++++++++++++++++-+
++++++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
++++++++++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
++++++++++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
++++++++++++++++++++++-+        f"{work_dir}/{mode}.txt")
++++++++++++++++++++++-+
++++++++++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
++++++++++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
++++++++++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
++++++++++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
++++++++++++++++++++++-+
++++++++++++++++++++++-+    rows = []
++++++++++++++++++++++-+    for file_id in results:
++++++++++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++++++++++-+
++++++++++++++++++++++-+        rows.append([
++++++++++++++++++++++-+            file_id,
++++++++++++++++++++++-+            gt,
++++++++++++++++++++++-+            conv_pred,
++++++++++++++++++++++-+            f"{conv_wer:.4f}",
++++++++++++++++++++++-+            lstm_pred,
++++++++++++++++++++++-+            f"{lstm_wer:.4f}"
++++++++++++++++++++++-+        ])
++++++++++++++++++++++-+
++++++++++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
++++++++++++++++++++++-+        writer = csv.writer(f)
++++++++++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
++++++++++++++++++++++-+        writer.writerows(rows)
++++++++++++++++++++++-+
++++++++++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
++++++++++++++++++++++-+    sample_wers = []
++++++++++++++++++++++-+    for file_id in results:
++++++++++++++++++++++-+        gt = results[file_id]['gloss']
++++++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
++++++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
++++++++++++++++++++++-+    
++++++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
++++++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
++++++++++++++++++++++-+    
++++++++++++++++++++++-+        sample_wers.append({
++++++++++++++++++++++-+            'file_id': file_id,
++++++++++++++++++++++-+            'gt': gt,
++++++++++++++++++++++-+            'conv_pred': conv_pred,
++++++++++++++++++++++-+            'conv_wer': conv_wer,
++++++++++++++++++++++-+            'lstm_pred': lstm_pred,
++++++++++++++++++++++-+            'lstm_wer': lstm_wer,
++++++++++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
++++++++++++++++++++++-+        })
++++++++++++++++++++++-+
++++++++++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
++++++++++++++++++++++-+    
++++++++++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
++++++++++++++++++++++-+    for sample in top5:
++++++++++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
++++++++++++++++++++++-+        print(f"GT   : {sample['gt']}")
++++++++++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
++++++++++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
++++++++++++++++++++++-+        print("-" * 60)
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++-+
++++++++++++++++++++++- 
++++++++++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++++++++++++++++++ class Decode(object):
+++++++++++++++++++++++@@ -22,6 +49,10 @@ class Decode(object):
+++++++++++++++++++++++     def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
+++++++++++++++++++++++         if not batch_first:
+++++++++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
++++++++++++++++++++++++        
++++++++++++++++++++++++        # time 별 class 시각화
++++++++++++++++++++++++        plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0)
++++++++++++++++++++++++
+++++++++++++++++++++++         if self.search_mode == "max":
+++++++++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
+++++++++++++++++++++++         else:
+++++++++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
+++++++++++++++++++++++index 7d0a2aa..cd66258 100644
+++++++++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
+++++++++++++++++++++++diff --git a/work_dirt/dirty.patch b/work_dirt/dirty.patch
+++++++++++++++++++++++index 8a22ece..424b96f 100644
+++++++++++++++++++++++--- a/work_dirt/dirty.patch
++++++++++++++++++++++++++ b/work_dirt/dirty.patch
+++++++++++++++++++++++@@ -1,284 +1,349 @@
+++++++++++++++++++++++-diff --git a/README.md b/README.md
+++++++++++++++++++++++-index bdbc17f..8cb240b 100644
+++++++++++++++++++++++---- a/README.md
+++++++++++++++++++++++-+++ b/README.md
+++++++++++++++++++++++-@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
++++++++++++++++++++++++diff --git a/__pycache__/slr_network.cpython-39.pyc b/__pycache__/slr_network.cpython-39.pyc
++++++++++++++++++++++++index 7ac0c3b..c89f220 100644
++++++++++++++++++++++++Binary files a/__pycache__/slr_network.cpython-39.pyc and b/__pycache__/slr_network.cpython-39.pyc differ
++++++++++++++++++++++++diff --git a/modules/__pycache__/criterions.cpython-39.pyc b/modules/__pycache__/criterions.cpython-39.pyc
++++++++++++++++++++++++index 71519fd..b9664e1 100644
++++++++++++++++++++++++Binary files a/modules/__pycache__/criterions.cpython-39.pyc and b/modules/__pycache__/criterions.cpython-39.pyc differ
++++++++++++++++++++++++diff --git a/tmp.ipynb b/tmp.ipynb
++++++++++++++++++++++++index e69de29..0342039 100644
++++++++++++++++++++++++--- a/tmp.ipynb
+++++++++++++++++++++++++++ b/tmp.ipynb
++++++++++++++++++++++++@@ -0,0 +1,272 @@
+++++++++++++++++++++++++{
+++++++++++++++++++++++++ "cells": [
+++++++++++++++++++++++++  {
+++++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++++   "execution_count": 2,
+++++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++++   "outputs": [],
+++++++++++++++++++++++++   "source": [
+++++++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "model_path = \"/home/jhy/SignGraph/_best_model.pt\"  # 모델 파일 경로\n",
+++++++++++++++++++++++++    "model = torch.load(model_path, map_location=torch.device(\"cpu\"))  # CPU에서 로드\n"
+++++++++++++++++++++++++   ]
+++++++++++++++++++++++++  },
+++++++++++++++++++++++++  {
+++++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++++   "execution_count": 3,
+++++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++++   "outputs": [
+++++++++++++++++++++++++    {
+++++++++++++++++++++++++     "name": "stdout",
+++++++++++++++++++++++++     "output_type": "stream",
+++++++++++++++++++++++++     "text": [
+++++++++++++++++++++++++      "Model is a state_dict.\n",
+++++++++++++++++++++++++      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'rng_state'])\n"
+++++++++++++++++++++++++     ]
+++++++++++++++++++++++++    }
+++++++++++++++++++++++++   ],
+++++++++++++++++++++++++   "source": [
+++++++++++++++++++++++++    "if isinstance(model, dict):  # state_dict 형태인지 확인\n",
+++++++++++++++++++++++++    "    print(\"Model is a state_dict.\")\n",
+++++++++++++++++++++++++    "    print(model.keys())  # 저장된 파라미터 키 확인\n"
+++++++++++++++++++++++++   ]
+++++++++++++++++++++++++  },
+++++++++++++++++++++++++  {
+++++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++++   "execution_count": 7,
+++++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++++   "outputs": [],
+++++++++++++++++++++++++   "source": [
+++++++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++++++    "import torch.nn as nn  # <== 여기가 중요\n",
+++++++++++++++++++++++++    "import torch.nn.functional as F\n"
+++++++++++++++++++++++++   ]
+++++++++++++++++++++++++  },
+++++++++++++++++++++++++  {
+++++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++++   "execution_count": 1,
+++++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++++   "outputs": [
+++++++++++++++++++++++++    {
+++++++++++++++++++++++++     "name": "stderr",
+++++++++++++++++++++++++     "output_type": "stream",
+++++++++++++++++++++++++     "text": [
+++++++++++++++++++++++++      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
+++++++++++++++++++++++++      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
+++++++++++++++++++++++++     ]
+++++++++++++++++++++++++    }
+++++++++++++++++++++++++   ],
+++++++++++++++++++++++++   "source": [
+++++++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++++++    "import torch.nn as nn\n",
+++++++++++++++++++++++++    "import torch.nn.functional as F\n",
+++++++++++++++++++++++++    "import torchvision.models as models\n",
+++++++++++++++++++++++++    "import numpy as np\n",
+++++++++++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++++++++++    "from modules import BiLSTMLayer, TemporalConv\n",
+++++++++++++++++++++++++    "from modules.criterions import SeqKD\n",
+++++++++++++++++++++++++    "import utils\n",
+++++++++++++++++++++++++    "import modules.resnet as resnet\n",
+++++++++++++++++++++++++    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
+++++++++++++++++++++++++    "class Identity(nn.Module):\n",
+++++++++++++++++++++++++    "    def __init__(self):\n",
+++++++++++++++++++++++++    "        super(Identity, self).__init__()\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++++++++++    "        return x\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# L2 정규화 선형 레이어\n",
+++++++++++++++++++++++++    "class NormLinear(nn.Module):\n",
+++++++++++++++++++++++++    "    def __init__(self, in_dim, out_dim):\n",
+++++++++++++++++++++++++    "        super(NormLinear, self).__init__()\n",
+++++++++++++++++++++++++    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
+++++++++++++++++++++++++    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "    def forward(self, x):\n",
+++++++++++++++++++++++++    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
+++++++++++++++++++++++++    "        return outputs\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# SLRModel (수어 인식 모델)\n",
+++++++++++++++++++++++++    "class SLRModel(nn.Module):\n",
+++++++++++++++++++++++++    "    def __init__(\n",
+++++++++++++++++++++++++    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
+++++++++++++++++++++++++    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
+++++++++++++++++++++++++    "            weight_norm=True, share_classifier=True\n",
+++++++++++++++++++++++++    "    ):\n",
+++++++++++++++++++++++++    "        super(SLRModel, self).__init__()\n",
+++++++++++++++++++++++++    "        self.decoder = None\n",
+++++++++++++++++++++++++    "        self.loss = dict()\n",
+++++++++++++++++++++++++    "        self.criterion_init()\n",
+++++++++++++++++++++++++    "        self.num_classes = num_classes\n",
+++++++++++++++++++++++++    "        self.loss_weights = loss_weights\n",
+++++++++++++++++++++++++    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
+++++++++++++++++++++++++    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        # 1D CNN을 활용한 Temporal Encoding\n",
+++++++++++++++++++++++++    "        self.conv1d = TemporalConv(input_size=512,\n",
+++++++++++++++++++++++++    "                                   hidden_size=hidden_size,\n",
+++++++++++++++++++++++++    "                                   conv_type=conv_type,\n",
+++++++++++++++++++++++++    "                                   use_bn=use_bn,\n",
+++++++++++++++++++++++++    "                                   num_classes=num_classes)\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        # BiLSTM 기반 Temporal Model\n",
+++++++++++++++++++++++++    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
+++++++++++++++++++++++++    "                                          num_layers=2, bidirectional=True)\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        # Classifier (NormLinear 사용 여부 결정)\n",
+++++++++++++++++++++++++    "        if weight_norm:\n",
+++++++++++++++++++++++++    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++++++    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++++++    "        else:\n",
+++++++++++++++++++++++++    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++++++    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        # Classifier 공유 여부\n",
+++++++++++++++++++++++++    "        if share_classifier:\n",
+++++++++++++++++++++++++    "            self.conv1d.fc = self.classifier\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
+++++++++++++++++++++++++    "        # CNN으로 Frame-wise Feature 추출\n",
+++++++++++++++++++++++++    "        if len(x.shape) == 5:\n",
+++++++++++++++++++++++++    "            batch, temp, channel, height, width = x.shape\n",
+++++++++++++++++++++++++    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
+++++++++++++++++++++++++    "        else:\n",
+++++++++++++++++++++++++    "            framewise = x\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
+++++++++++++++++++++++++    "        x = conv1d_outputs['visual_feat']\n",
+++++++++++++++++++++++++    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        # BiLSTM을 활용한 Temporal Modeling\n",
+++++++++++++++++++++++++    "        tm_outputs = self.temporal_model(x, lgt)\n",
+++++++++++++++++++++++++    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        # 최종 Classifier 적용\n",
+++++++++++++++++++++++++    "        outputs = self.classifier(features_before_classifier)\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        # Inference 모드에서 Decoding\n",
+++++++++++++++++++++++++    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
+++++++++++++++++++++++++    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "        return {\n",
+++++++++++++++++++++++++    "            \"framewise_features\": framewise,\n",
+++++++++++++++++++++++++    "            \"visual_features\": x,\n",
+++++++++++++++++++++++++    "            \"temproal_features\": tm_outputs['predictions'],\n",
+++++++++++++++++++++++++    "            \"feat_len\": lgt,\n",
+++++++++++++++++++++++++    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
+++++++++++++++++++++++++    "            \"sequence_logits\": outputs,\n",
+++++++++++++++++++++++++    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
+++++++++++++++++++++++++    "            \"conv_sents\": conv_pred,\n",
+++++++++++++++++++++++++    "            \"recognized_sents\": pred,\n",
+++++++++++++++++++++++++    "        }\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "    def criterion_init(self):\n",
+++++++++++++++++++++++++    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
+++++++++++++++++++++++++    "        self.loss['distillation'] = SeqKD(T=8)\n",
+++++++++++++++++++++++++    "        return self.loss\n"
+++++++++++++++++++++++++   ]
+++++++++++++++++++++++++  },
+++++++++++++++++++++++++  {
+++++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++++   "execution_count": 6,
+++++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++++   "outputs": [
+++++++++++++++++++++++++    {
+++++++++++++++++++++++++     "ename": "KeyError",
+++++++++++++++++++++++++     "evalue": "'dataset_info'",
+++++++++++++++++++++++++     "output_type": "error",
+++++++++++++++++++++++++     "traceback": [
+++++++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
+++++++++++++++++++++++++      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++++++++++      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
+++++++++++++++++++++++++     ]
+++++++++++++++++++++++++    }
+++++++++++++++++++++++++   ],
+++++++++++++++++++++++++   "source": [
+++++++++++++++++++++++++    "import os\n",
+++++++++++++++++++++++++    "import numpy as np\n",
+++++++++++++++++++++++++    "import yaml\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# 환경 변수 설정\n",
+++++++++++++++++++++++++    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
+++++++++++++++++++++++++    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
+++++++++++++++++++++++++    "with open(config_path, \"r\") as f:\n",
+++++++++++++++++++++++++    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# ✅ gloss_dict 로드\n",
+++++++++++++++++++++++++    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
+++++++++++++++++++++++++    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "print(\"📌 Gloss Dictionary Loaded!\")\n",
+++++++++++++++++++++++++    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
+++++++++++++++++++++++++    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
+++++++++++++++++++++++++   ]
+++++++++++++++++++++++++  },
+++++++++++++++++++++++++  {
+++++++++++++++++++++++++   "cell_type": "code",
+++++++++++++++++++++++++   "execution_count": 5,
+++++++++++++++++++++++++   "metadata": {},
+++++++++++++++++++++++++   "outputs": [
+++++++++++++++++++++++++    {
+++++++++++++++++++++++++     "ename": "AttributeError",
+++++++++++++++++++++++++     "evalue": "'NoneType' object has no attribute 'items'",
+++++++++++++++++++++++++     "output_type": "error",
+++++++++++++++++++++++++     "traceback": [
+++++++++++++++++++++++++      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+++++++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+++++++++++++++++++++++++      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+++++++++++++++++++++++++      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
+++++++++++++++++++++++++      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
+++++++++++++++++++++++++      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
+++++++++++++++++++++++++     ]
+++++++++++++++++++++++++    }
+++++++++++++++++++++++++   ],
+++++++++++++++++++++++++   "source": [
+++++++++++++++++++++++++    "import torch\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# 모델 불러오기\n",
+++++++++++++++++++++++++    "model = SLRModel(\n",
+++++++++++++++++++++++++    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
+++++++++++++++++++++++++    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
+++++++++++++++++++++++++    ")\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# 저장된 가중치 로드\n",
+++++++++++++++++++++++++    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
+++++++++++++++++++++++++    "if isinstance(state_dict, dict):\n",
+++++++++++++++++++++++++    "    model.load_state_dict(state_dict)\n",
+++++++++++++++++++++++++    "\n",
+++++++++++++++++++++++++    "# 모델을 평가 모드로 설정\n",
+++++++++++++++++++++++++    "model.eval()\n"
+++++++++++++++++++++++++   ]
+++++++++++++++++++++++++  }
+++++++++++++++++++++++++ ],
+++++++++++++++++++++++++ "metadata": {
+++++++++++++++++++++++++  "kernelspec": {
+++++++++++++++++++++++++   "display_name": "3.9.13",
+++++++++++++++++++++++++   "language": "python",
+++++++++++++++++++++++++   "name": "python3"
+++++++++++++++++++++++++  },
+++++++++++++++++++++++++  "language_info": {
+++++++++++++++++++++++++   "codemirror_mode": {
+++++++++++++++++++++++++    "name": "ipython",
+++++++++++++++++++++++++    "version": 3
+++++++++++++++++++++++++   },
+++++++++++++++++++++++++   "file_extension": ".py",
+++++++++++++++++++++++++   "mimetype": "text/x-python",
+++++++++++++++++++++++++   "name": "python",
+++++++++++++++++++++++++   "nbconvert_exporter": "python",
+++++++++++++++++++++++++   "pygments_lexer": "ipython3",
+++++++++++++++++++++++++   "version": "3.9.13"
+++++++++++++++++++++++++  }
+++++++++++++++++++++++++ },
+++++++++++++++++++++++++ "nbformat": 4,
+++++++++++++++++++++++++ "nbformat_minor": 2
+++++++++++++++++++++++++}
++++++++++++++++++++++++diff --git a/utils/__pycache__/decode.cpython-39.pyc b/utils/__pycache__/decode.cpython-39.pyc
++++++++++++++++++++++++index cb157af..1703b1b 100644
++++++++++++++++++++++++Binary files a/utils/__pycache__/decode.cpython-39.pyc and b/utils/__pycache__/decode.cpython-39.pyc differ
++++++++++++++++++++++++diff --git a/utils/decode.py b/utils/decode.py
++++++++++++++++++++++++index 3877729..2bb571a 100644
++++++++++++++++++++++++--- a/utils/decode.py
+++++++++++++++++++++++++++ b/utils/decode.py
++++++++++++++++++++++++@@ -6,7 +6,27 @@ import ctcdecode
++++++++++++++++++++++++ import numpy as np
++++++++++++++++++++++++ from itertools import groupby
++++++++++++++++++++++++ import torch.nn.functional as F
+++++++++++++++++++++++++import torch
+++++++++++++++++++++++++import matplotlib.pyplot as plt
+++++++++++++++++++++++++import numpy as np
+++++++++++++++++++++++  
+++++++++++++++++++++++++def plot_timewise_predictions(nn_output, gloss_dict, vid_lgt, batch_idx=0):
+++++++++++++++++++++++++    i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
+++++++++++++++++++++++++    probs = torch.softmax(nn_output, dim=-1)
+++++++++++++++++++++++++    pred_ids = torch.argmax(probs, dim=-1)  # (B, T)
+++++++++++++++++++++++++    length = vid_lgt[batch_idx].item()
+++++++++++++++++++++++++    pred_seq = pred_ids[batch_idx, :length].cpu().numpy()
+++++++++++++++++++++++++    gloss_names = [i2g_dict.get(int(idx), "UNK") for idx in pred_seq]
+++++++++++++++++++++++++
+++++++++++++++++++++++++    plt.figure(figsize=(15, 3))
+++++++++++++++++++++++++    plt.plot(pred_seq, drawstyle='steps-post', linewidth=1.5)
+++++++++++++++++++++++++    plt.title("Predicted Gloss ID over Time")
+++++++++++++++++++++++++    plt.xlabel("Time Step")
+++++++++++++++++++++++++    plt.ylabel("Gloss ID")
+++++++++++++++++++++++++    plt.yticks(np.unique(pred_seq))
+++++++++++++++++++++++++    plt.grid(True)
+++++++++++++++++++++++++    plt.tight_layout()
+++++++++++++++++++++++++    plt.show()
+++++++++++++++++++++++  
+++++++++++++++++++++++- ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
+++++++++++++++++++++++--`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+++++++++++++++++++++++-+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
+++++++++++++++++++++++- 
+++++++++++++++++++++++- ### Training
+++++++++++++++++++++++- 
+++++++++++++++++++++++-diff --git a/configs/baseline.yaml b/configs/baseline.yaml
+++++++++++++++++++++++-index bfc1da8..25ffa61 100644
+++++++++++++++++++++++---- a/configs/baseline.yaml
+++++++++++++++++++++++-+++ b/configs/baseline.yaml
+++++++++++++++++++++++-@@ -1,14 +1,14 @@
+++++++++++++++++++++++- feeder: dataset.dataloader_video.BaseFeeder
+++++++++++++++++++++++- phase: train
+++++++++++++++++++++++--dataset: phoenix2014-T
+++++++++++++++++++++++-+dataset: phoenix2014
+++++++++++++++++++++++- #CSL-Daily
+++++++++++++++++++++++- # dataset: phoenix14-si5
+++++++++++++++++++++++- 
+++++++++++++++++++++++- work_dir: ./work_dirt/
+++++++++++++++++++++++--batch_size: 4
+++++++++++++++++++++++-+batch_size: 1
+++++++++++++++++++++++- random_seed: 0 
+++++++++++++++++++++++--test_batch_size: 4
+++++++++++++++++++++++--num_worker: 20
+++++++++++++++++++++++-+test_batch_size: 1
+++++++++++++++++++++++-+num_worker: 3
+++++++++++++++++++++++- device: 0
+++++++++++++++++++++++- log_interval: 10000
+++++++++++++++++++++++- eval_interval: 1
+++++++++++++++++++++++-diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
+++++++++++++++++++++++-index 555f4b8..c126e7a 100644
+++++++++++++++++++++++---- a/dataset/dataloader_video.py
+++++++++++++++++++++++-+++ b/dataset/dataloader_video.py
+++++++++++++++++++++++-@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
+++++++++++++++++++++++-         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
+++++++++++++++++++++++-         #/data1/gsw/CSL-Daily/sentence/frames_512x512
+++++++++++++++++++++++-         self.transform_mode = "train" if transform_mode else "test"
+++++++++++++++++++++++--        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++++++-+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++++++-+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+++++++++++++++++++++++-+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+++++++++++++++++++++++-+
+++++++++++++++++++++++-         print(mode, len(self))
+++++++++++++++++++++++-         self.data_aug = self.transform()
+++++++++++++++++++++++-         print("")
+++++++++++++++++++++++-@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
+++++++++++++++++++++++-             return input_data, label, self.inputs_list[idx]['original_info']
++++++++++++++++++++++ - 
++++++++++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++++++++++++++++++-diff --git a/slr_network.py b/slr_network.py
++++++++++++++++++++++-index 45295cb..ede70cf 100644
++++++++++++++++++++++---- a/slr_network.py
++++++++++++++++++++++-+++ b/slr_network.py
++++++++++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++++++++++++++++++-     def read_video(self, index):
+++++++++++++++++++++++--        # load file info
+++++++++++++++++++++++-         fi = self.inputs_list[index]
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-         if 'phoenix' in self.dataset:
+++++++++++++++++++++++--            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+++++++++++++++++++++++-+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+++++++++++++++++++++++-+#            img_list = sorted(glob.glob(frame_pattern))
+++++++++++++++++++++++-+#            print(img_list)
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+#            print("[LOG] Using phoenix")
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+++++++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
+++++++++++++++++++++++- 
+++++++++++++++++++++++-+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+++++++++++++++++++++++-+#            print(img_folder)
+++++++++++++++++++++++-+#            img_list = sorted(glob.glob(img_folder))
+++++++++++++++++++++++-+#            print(f"[DEBUG] Found {len(img_list)} frames")
+++++++++++++++++++++++-+#            print(len(img_list))
+++++++++++++++++++++++-         elif self.dataset == 'CSL-Daily':
+++++++++++++++++++++++--            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+++++++++++++++++++++++-+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-         img_list = sorted(glob.glob(img_folder))
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-+        if len(img_list) == 0:
+++++++++++++++++++++++-+            print(f"[WARNING] No frames found in: {img_list}")
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-         label_list = []
+++++++++++++++++++++++--        if self.dataset=='phoenix2014':
+++++++++++++++++++++++-+        if self.dataset == 'phoenix2014':
+++++++++++++++++++++++-             fi['label'] = clean_phoenix_2014(fi['label'])
+++++++++++++++++++++++--        if self.dataset=='phoenix2014-T':
+++++++++++++++++++++++--            fi['label']=clean_phoenix_2014_trans(fi['label'])
+++++++++++++++++++++++-+        elif self.dataset == 'phoenix2014-T':
+++++++++++++++++++++++-+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-         for phase in fi['label'].split(" "):
+++++++++++++++++++++++--            if phase == '':
+++++++++++++++++++++++--                continue
+++++++++++++++++++++++--            if phase in self.dict.keys():
+++++++++++++++++++++++-+            if phase and phase in self.dict:
+++++++++++++++++++++++-                 label_list.append(self.dict[phase][0])
+++++++++++++++++++++++--        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++++++++++--                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-+        video = [
+++++++++++++++++++++++-+            cv2.cvtColor(
+++++++++++++++++++++++-+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+++++++++++++++++++++++-+                cv2.COLOR_BGR2RGB
+++++++++++++++++++++++-+            )   
+++++++++++++++++++++++-+            for img_path in img_list
+++++++++++++++++++++++-+        ]
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-+        return video, label_list, fi
+++++++++++++++++++++++- 
+++++++++++++++++++++++-     def read_features(self, index):
+++++++++++++++++++++++-         # load file info
+++++++++++++++++++++++-diff --git a/main.py b/main.py
+++++++++++++++++++++++-index 9e68cee..18ac59b 100644
+++++++++++++++++++++++---- a/main.py
+++++++++++++++++++++++-+++ b/main.py
+++++++++++++++++++++++-@@ -256,7 +256,7 @@ class Processor():
+++++++++++++++++++++++-                 batch_size=batch_size,
+++++++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++++++++++-                 num_workers=self.arg.num_worker,
+++++++++++++++++++++++--                pin_memory=True,
+++++++++++++++++++++++-+                pin_memory=False,
+++++++++++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++++++++++-             )
+++++++++++++++++++++++-             return loader
+++++++++++++++++++++++-@@ -268,7 +268,7 @@ class Processor():
+++++++++++++++++++++++-                 drop_last=train_flag,
+++++++++++++++++++++++-                 num_workers=self.arg.num_worker,  # if train_flag else 0
+++++++++++++++++++++++-                 collate_fn=self.feeder.collate_fn,
+++++++++++++++++++++++--                pin_memory=True,
+++++++++++++++++++++++-+                pin_memory=False,
+++++++++++++++++++++++-                 worker_init_fn=self.init_fn,
+++++++++++++++++++++++-             )
+++++++++++++++++++++++- 
+++++++++++++++++++++++-diff --git a/seq_scripts.py b/seq_scripts.py
+++++++++++++++++++++++-index 528856d..d8fcaf9 100644
+++++++++++++++++++++++---- a/seq_scripts.py
+++++++++++++++++++++++-+++ b/seq_scripts.py
+++++++++++++++++++++++-@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
+++++++++++++++++++++++-     return
+++++++++++++++++++++++- 
+++++++++++++++++++++++- 
+++++++++++++++++++++++-+import csv 
+++++++++++++++++++++++-+from jiwer import wer as jiwer_wer
+++++++++++++++++++++++- def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
+++++++++++++++++++++++-     model.eval()
+++++++++++++++++++++++--    results=defaultdict(dict)
+++++++++++++++++++++++-+    results = defaultdict(dict)
+++++++++++++++++++++++- 
+++++++++++++++++++++++-     for batch_idx, data in enumerate(tqdm(loader)):
+++++++++++++++++++++++-         recoder.record_timer("device")
+++++++++++++++++++++++-@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
+++++++++++++++++++++++-                 results[inf]['conv_sents'] = conv_sents
+++++++++++++++++++++++-                 results[inf]['recognized_sents'] = recognized_sents
+++++++++++++++++++++++-                 results[inf]['gloss'] = gl
+++++++++++++++++++++++-+
+++++++++++++++++++++++-     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
+++++++++++++++++++++++-     gls_ref = [results[n]['gloss'] for n in results]
+++++++++++++++++++++++-     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++++++++++-+
+++++++++++++++++++++++-     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
+++++++++++++++++++++++-     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
+++++++++++++++++++++++--    if wer_results['wer'] < wer_results_con['wer']:
+++++++++++++++++++++++--        reg_per = wer_results
+++++++++++++++++++++++--    else:
+++++++++++++++++++++++--        reg_per = wer_results_con
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+++++++++++++++++++++++-+
+++++++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++++++++++-         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
+++++++++++++++++++++++-         f"{work_dir}/{mode}.txt")
+++++++++++++++++++++++-+
+++++++++++++++++++++++-     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
+++++++++++++++++++++++--        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+++++++++++++++++++++++-+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+++++++++++++++++++++++-+        f"{work_dir}/{mode}.txt")
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+    # ✅ 전체 결과 CSV로 저장
+++++++++++++++++++++++-+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+++++++++++++++++++++++-+    os.makedirs(save_folder, exist_ok=True)
+++++++++++++++++++++++-+    csv_path = os.path.join(save_folder, "wer_results.csv")
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+    rows = []
+++++++++++++++++++++++-+    for file_id in results:
+++++++++++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+        rows.append([
+++++++++++++++++++++++-+            file_id,
+++++++++++++++++++++++-+            gt,
+++++++++++++++++++++++-+            conv_pred,
+++++++++++++++++++++++-+            f"{conv_wer:.4f}",
+++++++++++++++++++++++-+            lstm_pred,
+++++++++++++++++++++++-+            f"{lstm_wer:.4f}"
+++++++++++++++++++++++-+        ])
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+++++++++++++++++++++++-+        writer = csv.writer(f)
+++++++++++++++++++++++-+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+++++++++++++++++++++++-+        writer.writerows(rows)
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+    # WER 기준 상위 5개 샘플 출력
+++++++++++++++++++++++-+    sample_wers = []
+++++++++++++++++++++++-+    for file_id in results:
+++++++++++++++++++++++-+        gt = results[file_id]['gloss']
+++++++++++++++++++++++-+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+++++++++++++++++++++++-+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-+        conv_wer = jiwer_wer(gt, conv_pred)
+++++++++++++++++++++++-+        lstm_wer = jiwer_wer(gt, lstm_pred)
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-+        sample_wers.append({
+++++++++++++++++++++++-+            'file_id': file_id,
+++++++++++++++++++++++-+            'gt': gt,
+++++++++++++++++++++++-+            'conv_pred': conv_pred,
+++++++++++++++++++++++-+            'conv_wer': conv_wer,
+++++++++++++++++++++++-+            'lstm_pred': lstm_pred,
+++++++++++++++++++++++-+            'lstm_wer': lstm_wer,
+++++++++++++++++++++++-+            'max_wer': max(conv_wer, lstm_wer)
+++++++++++++++++++++++-+        })
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+++++++++++++++++++++++-+    
+++++++++++++++++++++++-+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+++++++++++++++++++++++-+    for sample in top5:
+++++++++++++++++++++++-+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+++++++++++++++++++++++-+        print(f"GT   : {sample['gt']}")
+++++++++++++++++++++++-+        print(f"Conv : {sample['conv_pred']}")
+++++++++++++++++++++++-+        print(f"LSTM : {sample['lstm_pred']}")
+++++++++++++++++++++++-+        print("-" * 60)
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+
+++++++++++++++++++++++- 
+++++++++++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++++++++++++++++++-- 
+++++++++++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++++++++++++++++++-diff --git a/slr_network.py b/slr_network.py
+++++++++++++++++++++++-index 45295cb..ede70cf 100644
+++++++++++++++++++++++---- a/slr_network.py
+++++++++++++++++++++++-+++ b/slr_network.py
+++++++++++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++++++++++++++++++ class Decode(object):
++++++++++++++++++++++++     def __init__(self, gloss_dict, num_classes, search_mode, blank_id=0, beam_width=10):
++++++++++++++++++++++++@@ -22,6 +42,10 @@ class Decode(object):
++++++++++++++++++++++++     def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
++++++++++++++++++++++++         if not batch_first:
++++++++++++++++++++++++             nn_output = nn_output.permute(1, 0, 2)
+++++++++++++++++++++++++        
+++++++++++++++++++++++++        # time 별 class 시각화
+++++++++++++++++++++++++        plot_timewise_predictions(nn_output, self.i2g_dict, vid_lgt, batch_idx=0)
+++++++++++++++++++++++++
++++++++++++++++++++++++         if self.search_mode == "max":
++++++++++++++++++++++++             return self.MaxDecode(nn_output, vid_lgt)
++++++++++++++++++++++++         else:
++++++++++++++++++++++++diff --git a/work_dirt/code.tar.gz b/work_dirt/code.tar.gz
++++++++++++++++++++++++index 7d0a2aa..cd66258 100644
++++++++++++++++++++++++Binary files a/work_dirt/code.tar.gz and b/work_dirt/code.tar.gz differ
++++++++++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++++++++++++++++++index ede70cf..4ed3e3a 100644
++++++++++++++++++++++++--- a/work_dirt/slr_network.py
+++++++++++++++++++++++++++ b/work_dirt/slr_network.py
++++++++++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++++++++++++++++++          x = conv1d_outputs['visual_feat']
+++++++++++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++++++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++++++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++++++++++-+
+++++++++++++++++++++++-+        print('#######################################################')
++++++++++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++++++++++-
++++++++++++++++++++++++-        print('#######################################################')
+++++++++++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++++++++        # print('#######################################################')
+++++++++++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++++++++++++++++++          pred = None if self.training \
+++++++++++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++++++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++++++++++++++++++index acadd49..e35ae13 100644
+++++++++++++++++++++++--- a/work_dirt/log.txt
++++++++++++++++++++++++++ b/work_dirt/log.txt
+++++++++++++++++++++++@@ -116,3 +116,5 @@
+++++++++++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++++++++++++++++++index ede70cf..4ed3e3a 100644
+++++++++++++++++++++++--- a/work_dirt/slr_network.py
++++++++++++++++++++++++++ b/work_dirt/slr_network.py
+++++++++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++++++++++++++++++          x = conv1d_outputs['visual_feat']
++++++++++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++++++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
++++++++++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++++++++-+
++++++++++++++++++++++-+        print('#######################################################')
+++++++++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++++++++++-
+++++++++++++++++++++++-        print('#######################################################')
++++++++++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++++++++        # print('#######################################################')
++++++++++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++++++++++++++++++          pred = None if self.training \
++++++++++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++++++++++++++++++index acadd49..5e3b961 100644
++++++++++++++++++++++--- a/work_dirt/log.txt
+++++++++++++++++++++++++ b/work_dirt/log.txt
++++++++++++++++++++++@@ -116,3 +116,7 @@
++++++++++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++++++++++++++++index ede70cf..4ed3e3a 100644
++++++++++++++++++++++--- a/work_dirt/slr_network.py
+++++++++++++++++++++++++ b/work_dirt/slr_network.py
++++++++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++++++++++++++++          x = conv1d_outputs['visual_feat']
+++++++++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++++++++-+
+++++++++++++++++++++-+        print('#######################################################')
++++++++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++++++++-
++++++++++++++++++++++-        print('#######################################################')
+++++++++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++++++        # print('#######################################################')
+++++++++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++++++++++++++++          pred = None if self.training \
+++++++++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++++++++++++++++index acadd49..5ca45db 100644
+++++++++++++++++++++--- a/work_dirt/log.txt
++++++++++++++++++++++++ b/work_dirt/log.txt
+++++++++++++++++++++@@ -116,3 +116,9 @@
+++++++++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++++++++++++++++index ede70cf..4ed3e3a 100644
+++++++++++++++++++++--- a/work_dirt/slr_network.py
++++++++++++++++++++++++ b/work_dirt/slr_network.py
+++++++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++++++++++++++++          x = conv1d_outputs['visual_feat']
++++++++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
++++++++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++++++-+
++++++++++++++++++++-+        print('#######################################################')
+++++++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++++++++-
+++++++++++++++++++++-        print('#######################################################')
++++++++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++++++        # print('#######################################################')
++++++++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++++++++++++++++          pred = None if self.training \
++++++++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++++++++++++++++index acadd49..338d991 100644
++++++++++++++++++++--- a/work_dirt/log.txt
+++++++++++++++++++++++ b/work_dirt/log.txt
++++++++++++++++++++@@ -116,3 +116,11 @@
++++++++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++++++++++++++index ede70cf..4ed3e3a 100644
++++++++++++++++++++--- a/work_dirt/slr_network.py
+++++++++++++++++++++++ b/work_dirt/slr_network.py
++++++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++++++++++++++          x = conv1d_outputs['visual_feat']
+++++++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++++++-+
+++++++++++++++++++-+        print('#######################################################')
++++++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++++++-
++++++++++++++++++++-        print('#######################################################')
+++++++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++++        # print('#######################################################')
+++++++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++++++++++++++          pred = None if self.training \
+++++++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++++++++++++++index acadd49..dadaaa1 100644
+++++++++++++++++++--- a/work_dirt/log.txt
++++++++++++++++++++++ b/work_dirt/log.txt
+++++++++++++++++++@@ -116,3 +116,13 @@
+++++++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++++++++++++++index ede70cf..4ed3e3a 100644
+++++++++++++++++++--- a/work_dirt/slr_network.py
++++++++++++++++++++++ b/work_dirt/slr_network.py
+++++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++++++++++++++          x = conv1d_outputs['visual_feat']
++++++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
++++++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++++-+
++++++++++++++++++-+        print('#######################################################')
+++++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++++++-
+++++++++++++++++++-        print('#######################################################')
++++++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++++        # print('#######################################################')
++++++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++++++++++++++          pred = None if self.training \
++++++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++++++++++++++index acadd49..3d5e162 100644
++++++++++++++++++--- a/work_dirt/log.txt
+++++++++++++++++++++ b/work_dirt/log.txt
++++++++++++++++++@@ -116,3 +116,15 @@
++++++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++++++++++++index ede70cf..4ed3e3a 100644
++++++++++++++++++--- a/work_dirt/slr_network.py
+++++++++++++++++++++ b/work_dirt/slr_network.py
++++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++++++++++++          x = conv1d_outputs['visual_feat']
+++++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++++-+
+++++++++++++++++-+        print('#######################################################')
++++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++++-
++++++++++++++++++-        print('#######################################################')
+++++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++++        # print('#######################################################')
+++++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++++++++++++          pred = None if self.training \
+++++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++++++++++++index acadd49..e0ea7c5 100644
+++++++++++++++++--- a/work_dirt/log.txt
++++++++++++++++++++ b/work_dirt/log.txt
+++++++++++++++++@@ -116,3 +116,17 @@
+++++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++++++++++++index ede70cf..4ed3e3a 100644
+++++++++++++++++--- a/work_dirt/slr_network.py
++++++++++++++++++++ b/work_dirt/slr_network.py
+++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++++++++++++          x = conv1d_outputs['visual_feat']
++++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
++++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++-+
++++++++++++++++-+        print('#######################################################')
+++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++++-
+++++++++++++++++-        print('#######################################################')
++++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++++        # print('#######################################################')
++++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++++++++++++          pred = None if self.training \
++++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++++++++++++index acadd49..8de3f1f 100644
++++++++++++++++--- a/work_dirt/log.txt
+++++++++++++++++++ b/work_dirt/log.txt
++++++++++++++++@@ -116,3 +116,21 @@
++++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+++++++++++++++++
++++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++++++++++index ede70cf..4ed3e3a 100644
++++++++++++++++--- a/work_dirt/slr_network.py
+++++++++++++++++++ b/work_dirt/slr_network.py
++++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++++++++++          x = conv1d_outputs['visual_feat']
+++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++-+
+++++++++++++++-+        print('#######################################################')
++++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++++-
++++++++++++++++-        print('#######################################################')
+++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++++        # print('#######################################################')
+++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++++++++++          pred = None if self.training \
+++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
++++++++++++++++index fc2e926..a60f925 100644
++++++++++++++++--- a/work_dirt/test.txt
+++++++++++++++++++ b/work_dirt/test.txt
++++++++++++++++@@ -8,3 +8,5 @@
++++++++++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
++++++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
++++++++++++++++index a2087e5..6bb098b 100644
++++++++++++++++--- a/work_dirt/train.txt
+++++++++++++++++++ b/work_dirt/train.txt
++++++++++++++++@@ -14,3 +14,5 @@
++++++++++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++++++++++index acadd49..ef8bfc8 100644
+++++++++++++++--- a/work_dirt/log.txt
++++++++++++++++++ b/work_dirt/log.txt
+++++++++++++++@@ -116,3 +116,25 @@
+++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
++++++++++++++ +
++++++++++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
++++++++++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
++++++++++++++ +
++++++++++++++- 
++++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++++++++++-- 
++++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++++++++++-diff --git a/slr_network.py b/slr_network.py
++++++++++++++-index 45295cb..ede70cf 100644
++++++++++++++---- a/slr_network.py
++++++++++++++-+++ b/slr_network.py
++++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++++++++++index ede70cf..4ed3e3a 100644
+++++++++++++++--- a/work_dirt/slr_network.py
++++++++++++++++++ b/work_dirt/slr_network.py
+++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++++++++++          x = conv1d_outputs['visual_feat']
++++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++++++++++          tm_outputs = self.temporal_model(x, lgt)
++++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++-+
++++++++++++++-+        print('#######################################################')
+++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++++-
+++++++++++++++-        print('#######################################################')
++++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++++        # print('#######################################################')
++++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++++++++++          pred = None if self.training \
++++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
+++++++++++++++index fc2e926..b83de83 100644
+++++++++++++++--- a/work_dirt/test.txt
++++++++++++++++++ b/work_dirt/test.txt
+++++++++++++++@@ -8,3 +8,7 @@
+++++++++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
+++++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
+++++++++++++++index a2087e5..c83bd9c 100644
+++++++++++++++--- a/work_dirt/train.txt
++++++++++++++++++ b/work_dirt/train.txt
+++++++++++++++@@ -14,3 +14,7 @@
+++++++++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++++++++++index acadd49..f846f9d 100644
++++++++++++++--- a/work_dirt/log.txt
+++++++++++++++++ b/work_dirt/log.txt
++++++++++++++@@ -116,3 +116,29 @@
++++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+++++++++++++ +
+++++++++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
+++++++++++++ +
+++++++++++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
+++++++++++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
+++++++++++++ +
++++++++++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
++++++++++++++index 18ac59b..8f75cf5 100644
++++++++++++++--- a/work_dirt/main.py
+++++++++++++++++ b/work_dirt/main.py
++++++++++++++@@ -21,6 +21,7 @@ import utils
++++++++++++++ from seq_scripts import seq_train, seq_eval
++++++++++++++ from torch.cuda.amp import autocast as autocast
++++++++++++++ from utils.misc import *
+++++++++++++++from utils.decode import analyze_frame_lengths
++++++++++++++ class Processor():
++++++++++++++     def __init__(self, arg):
++++++++++++++         self.arg = arg
++++++++++++++@@ -105,13 +106,25 @@ class Processor():
++++++++++++++                 print('Please appoint --weights.')
++++++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++++ +
+++++++++++++++            train_wer = seq_eval(
+++++++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++++            )
+++++++++++++++            dev_wer = seq_eval(
+++++++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++++            )
+++++++++++++++            test_wer = seq_eval(
+++++++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++++            )
+++++++++++++ +
++++++++++++++             self.recoder.print_log('Evaluation Done.\n')
+++++++++++++ +
+++++++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++++++++++            analyze_frame_lengths()
+++++++++++++ +
++++++++++++++         elif self.arg.phase == "features":
++++++++++++++             for mode in ["train", "dev", "test"]:
++++++++++++++                 seq_feature_generation(
++++++++++++++@@ -119,6 +132,8 @@ class Processor():
++++++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++++++++++                 )
++++++++++++++ 
+++++++++++++ +
+++++++++++++ +
+++++++++++++- 
+++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++++++++-- 
+++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++++++++-diff --git a/slr_network.py b/slr_network.py
+++++++++++++-index 45295cb..ede70cf 100644
+++++++++++++---- a/slr_network.py
+++++++++++++-+++ b/slr_network.py
+++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++++++++     def save_arg(self):
++++++++++++++         arg_dict = vars(self.arg)
++++++++++++++         if not os.path.exists(self.arg.work_dir):
++++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++++++++index ede70cf..4ed3e3a 100644
++++++++++++++--- a/work_dirt/slr_network.py
+++++++++++++++++ b/work_dirt/slr_network.py
++++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++++++++          x = conv1d_outputs['visual_feat']
+++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++-+
+++++++++++++-+        print('#######################################################')
++++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++++-
++++++++++++++-        print('#######################################################')
+++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++++        # print('#######################################################')
+++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++++++++          pred = None if self.training \
+++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
++++++++++++++index fc2e926..901c9ba 100644
++++++++++++++--- a/work_dirt/test.txt
+++++++++++++++++ b/work_dirt/test.txt
++++++++++++++@@ -8,3 +8,9 @@
++++++++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
++++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
++++++++++++++index a2087e5..eef4826 100644
++++++++++++++--- a/work_dirt/train.txt
+++++++++++++++++ b/work_dirt/train.txt
++++++++++++++@@ -14,3 +14,9 @@
++++++++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++++++++index acadd49..8646428 100644
+++++++++++++--- a/work_dirt/log.txt
++++++++++++++++ b/work_dirt/log.txt
+++++++++++++@@ -116,3 +116,33 @@
+++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
++++++++++++ +
++++++++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
++++++++++++ +
++++++++++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
++++++++++++ +
++++++++++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
++++++++++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
++++++++++++ +
+++++++++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
+++++++++++++index 18ac59b..8f75cf5 100644
+++++++++++++--- a/work_dirt/main.py
++++++++++++++++ b/work_dirt/main.py
+++++++++++++@@ -21,6 +21,7 @@ import utils
+++++++++++++ from seq_scripts import seq_train, seq_eval
+++++++++++++ from torch.cuda.amp import autocast as autocast
+++++++++++++ from utils.misc import *
++++++++++++++from utils.decode import analyze_frame_lengths
+++++++++++++ class Processor():
+++++++++++++     def __init__(self, arg):
+++++++++++++         self.arg = arg
+++++++++++++@@ -105,13 +106,25 @@ class Processor():
+++++++++++++                 print('Please appoint --weights.')
+++++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++ +
++++++++++++++            train_wer = seq_eval(
++++++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++++            )
++++++++++++++            dev_wer = seq_eval(
++++++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++++            )
++++++++++++++            test_wer = seq_eval(
++++++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++++            )
++++++++++++ +
+++++++++++++             self.recoder.print_log('Evaluation Done.\n')
++++++++++++ +
++++++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++++++++++            analyze_frame_lengths()
++++++++++++ +
+++++++++++++         elif self.arg.phase == "features":
+++++++++++++             for mode in ["train", "dev", "test"]:
+++++++++++++                 seq_feature_generation(
+++++++++++++@@ -119,6 +132,8 @@ class Processor():
+++++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++++++++++                 )
+++++++++++++ 
++++++++++++ +
++++++++++++ +
++++++++++++- 
++++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++++++++-- 
++++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++++++++-diff --git a/slr_network.py b/slr_network.py
++++++++++++-index 45295cb..ede70cf 100644
++++++++++++---- a/slr_network.py
++++++++++++-+++ b/slr_network.py
++++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++++++++     def save_arg(self):
+++++++++++++         arg_dict = vars(self.arg)
+++++++++++++         if not os.path.exists(self.arg.work_dir):
+++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++++++++index ede70cf..4ed3e3a 100644
+++++++++++++--- a/work_dirt/slr_network.py
++++++++++++++++ b/work_dirt/slr_network.py
+++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++++++++          x = conv1d_outputs['visual_feat']
++++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++++++++          tm_outputs = self.temporal_model(x, lgt)
++++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++-+
++++++++++++-+        print('#######################################################')
+++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++++-
+++++++++++++-        print('#######################################################')
++++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++++        # print('#######################################################')
++++++++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++++++++          pred = None if self.training \
++++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
+++++++++++++index fc2e926..ca4ef79 100644
+++++++++++++--- a/work_dirt/test.txt
++++++++++++++++ b/work_dirt/test.txt
+++++++++++++@@ -8,3 +8,11 @@
+++++++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
+++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
+++++++++++++index a2087e5..f58c36a 100644
+++++++++++++--- a/work_dirt/train.txt
++++++++++++++++ b/work_dirt/train.txt
+++++++++++++@@ -14,3 +14,11 @@
+++++++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++++++++index acadd49..a4f8a9f 100644
++++++++++++--- a/work_dirt/log.txt
+++++++++++++++ b/work_dirt/log.txt
++++++++++++@@ -116,3 +116,37 @@
++++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+++++++++++ +
+++++++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
+++++++++++ +
+++++++++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
+++++++++++ +
+++++++++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
+++++++++++ +
+++++++++++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
+++++++++++ +
++++++++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
++++++++++++index 18ac59b..8f75cf5 100644
++++++++++++--- a/work_dirt/main.py
+++++++++++++++ b/work_dirt/main.py
++++++++++++@@ -21,6 +21,7 @@ import utils
++++++++++++ from seq_scripts import seq_train, seq_eval
++++++++++++ from torch.cuda.amp import autocast as autocast
++++++++++++ from utils.misc import *
+++++++++++++from utils.decode import analyze_frame_lengths
++++++++++++ class Processor():
++++++++++++     def __init__(self, arg):
++++++++++++         self.arg = arg
++++++++++++@@ -105,13 +106,25 @@ class Processor():
++++++++++++                 print('Please appoint --weights.')
++++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++ +
+++++++++++++            train_wer = seq_eval(
+++++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++            )
+++++++++++++            dev_wer = seq_eval(
+++++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++            )
+++++++++++++            test_wer = seq_eval(
+++++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++++            )
+++++++++++ +
++++++++++++             self.recoder.print_log('Evaluation Done.\n')
+++++++++++ +
+++++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++++++++            analyze_frame_lengths()
+++++++++++ +
++++++++++++         elif self.arg.phase == "features":
++++++++++++             for mode in ["train", "dev", "test"]:
++++++++++++                 seq_feature_generation(
++++++++++++@@ -119,6 +132,8 @@ class Processor():
++++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++++++++                 )
++++++++++++ 
+++++++++++ +
+++++++++++ +
+++++++++++- 
+++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++++++-- 
+++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++++++-diff --git a/slr_network.py b/slr_network.py
+++++++++++-index 45295cb..ede70cf 100644
+++++++++++---- a/slr_network.py
+++++++++++-+++ b/slr_network.py
+++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++++++     def save_arg(self):
++++++++++++         arg_dict = vars(self.arg)
++++++++++++         if not os.path.exists(self.arg.work_dir):
++++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++++++index ede70cf..4ed3e3a 100644
++++++++++++--- a/work_dirt/slr_network.py
+++++++++++++++ b/work_dirt/slr_network.py
++++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++++++          x = conv1d_outputs['visual_feat']
+++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++-+
+++++++++++-+        print('#######################################################')
++++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++++-
++++++++++++-        print('#######################################################')
+++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++++        # print('#######################################################')
+++++++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++++++          pred = None if self.training \
+++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
++++++++++++index fc2e926..d016fae 100644
++++++++++++--- a/work_dirt/test.txt
+++++++++++++++ b/work_dirt/test.txt
++++++++++++@@ -8,3 +8,13 @@
++++++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
++++++++++++index a2087e5..a6d1318 100644
++++++++++++--- a/work_dirt/train.txt
+++++++++++++++ b/work_dirt/train.txt
++++++++++++@@ -14,3 +14,13 @@
++++++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++++++index acadd49..4e4323f 100644
+++++++++++--- a/work_dirt/log.txt
++++++++++++++ b/work_dirt/log.txt
+++++++++++@@ -116,3 +116,41 @@
+++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
++++++++++ +
++++++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
++++++++++ +
++++++++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
++++++++++ +
++++++++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
++++++++++ +
++++++++++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
++++++++++ +
++++++++++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
++++++++++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
++++++++++ +
+++++++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
+++++++++++index 18ac59b..8f75cf5 100644
+++++++++++--- a/work_dirt/main.py
++++++++++++++ b/work_dirt/main.py
+++++++++++@@ -21,6 +21,7 @@ import utils
+++++++++++ from seq_scripts import seq_train, seq_eval
+++++++++++ from torch.cuda.amp import autocast as autocast
+++++++++++ from utils.misc import *
++++++++++++from utils.decode import analyze_frame_lengths
+++++++++++ class Processor():
+++++++++++     def __init__(self, arg):
+++++++++++         self.arg = arg
+++++++++++@@ -105,13 +106,25 @@ class Processor():
+++++++++++                 print('Please appoint --weights.')
+++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++ +
++++++++++++            train_wer = seq_eval(
++++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++            )
++++++++++++            dev_wer = seq_eval(
++++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++            )
++++++++++++            test_wer = seq_eval(
++++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++++            )
++++++++++ +
+++++++++++             self.recoder.print_log('Evaluation Done.\n')
++++++++++ +
++++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++++++++            analyze_frame_lengths()
++++++++++ +
+++++++++++         elif self.arg.phase == "features":
+++++++++++             for mode in ["train", "dev", "test"]:
+++++++++++                 seq_feature_generation(
+++++++++++@@ -119,6 +132,8 @@ class Processor():
+++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++++++++                 )
+++++++++++ 
++++++++++ +
++++++++++ +
++++++++++- 
++++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++++++-- 
++++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++++++-diff --git a/slr_network.py b/slr_network.py
++++++++++-index 45295cb..ede70cf 100644
++++++++++---- a/slr_network.py
++++++++++-+++ b/slr_network.py
++++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++++++     def save_arg(self):
+++++++++++         arg_dict = vars(self.arg)
+++++++++++         if not os.path.exists(self.arg.work_dir):
+++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++++++index ede70cf..4ed3e3a 100644
+++++++++++--- a/work_dirt/slr_network.py
++++++++++++++ b/work_dirt/slr_network.py
+++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++++++          x = conv1d_outputs['visual_feat']
++++++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++++++          tm_outputs = self.temporal_model(x, lgt)
++++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++-+
++++++++++-+        print('#######################################################')
+++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++++-
+++++++++++-        print('#######################################################')
++++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++++        # print('#######################################################')
++++++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++++++          pred = None if self.training \
++++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
+++++++++++index fc2e926..6c5779b 100644
+++++++++++--- a/work_dirt/test.txt
++++++++++++++ b/work_dirt/test.txt
+++++++++++@@ -8,3 +8,15 @@
+++++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
+++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
+++++++++++index a2087e5..73fdb64 100644
+++++++++++--- a/work_dirt/train.txt
++++++++++++++ b/work_dirt/train.txt
+++++++++++@@ -14,3 +14,15 @@
+++++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++++++index acadd49..45c4be2 100644
++++++++++--- a/work_dirt/log.txt
+++++++++++++ b/work_dirt/log.txt
++++++++++@@ -116,3 +116,43 @@
++++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+++++++++ +
+++++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
+++++++++ +
+++++++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
+++++++++ +
+++++++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
+++++++++ +
+++++++++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
+++++++++ +
+++++++++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
+++++++++ +
+++++++++++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
+++++++++++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
++++++++++index 18ac59b..8f75cf5 100644
++++++++++--- a/work_dirt/main.py
+++++++++++++ b/work_dirt/main.py
++++++++++@@ -21,6 +21,7 @@ import utils
++++++++++ from seq_scripts import seq_train, seq_eval
++++++++++ from torch.cuda.amp import autocast as autocast
++++++++++ from utils.misc import *
+++++++++++from utils.decode import analyze_frame_lengths
++++++++++ class Processor():
++++++++++     def __init__(self, arg):
++++++++++         self.arg = arg
++++++++++@@ -105,13 +106,25 @@ class Processor():
++++++++++                 print('Please appoint --weights.')
++++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++ +
+++++++++++            train_wer = seq_eval(
+++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++            )
+++++++++++            dev_wer = seq_eval(
+++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++            )
+++++++++++            test_wer = seq_eval(
+++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++++            )
+++++++++ +
++++++++++             self.recoder.print_log('Evaluation Done.\n')
+++++++++ +
+++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++++++            analyze_frame_lengths()
+++++++++ +
++++++++++         elif self.arg.phase == "features":
++++++++++             for mode in ["train", "dev", "test"]:
++++++++++                 seq_feature_generation(
++++++++++@@ -119,6 +132,8 @@ class Processor():
++++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++++++                 )
++++++++++ 
+++++++++ +
+++++++++ +
+++++++++- 
+++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++++-- 
+++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++++-diff --git a/slr_network.py b/slr_network.py
+++++++++-index 45295cb..ede70cf 100644
+++++++++---- a/slr_network.py
+++++++++-+++ b/slr_network.py
+++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++++     def save_arg(self):
++++++++++         arg_dict = vars(self.arg)
++++++++++         if not os.path.exists(self.arg.work_dir):
++++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++++index ede70cf..4ed3e3a 100644
++++++++++--- a/work_dirt/slr_network.py
+++++++++++++ b/work_dirt/slr_network.py
++++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++++          x = conv1d_outputs['visual_feat']
+++++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++-+
+++++++++-+        print('#######################################################')
++++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++++-
++++++++++-        print('#######################################################')
+++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++++        # print('#######################################################')
+++++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++++          pred = None if self.training \
+++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
++++++++++index fc2e926..6c5779b 100644
++++++++++--- a/work_dirt/test.txt
+++++++++++++ b/work_dirt/test.txt
++++++++++@@ -8,3 +8,15 @@
++++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
++++++++++index a2087e5..73fdb64 100644
++++++++++--- a/work_dirt/train.txt
+++++++++++++ b/work_dirt/train.txt
++++++++++@@ -14,3 +14,15 @@
++++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++++index acadd49..c5385fd 100644
+++++++++--- a/work_dirt/log.txt
++++++++++++ b/work_dirt/log.txt
+++++++++@@ -116,3 +116,45 @@
+++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
++++++++ +
++++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
++++++++ +
++++++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
++++++++ +
++++++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
++++++++ +
++++++++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
++++++++ +
++++++++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
++++++++ +
++++++++++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
++++++++++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
+++++++++index 18ac59b..8f75cf5 100644
+++++++++--- a/work_dirt/main.py
++++++++++++ b/work_dirt/main.py
+++++++++@@ -21,6 +21,7 @@ import utils
+++++++++ from seq_scripts import seq_train, seq_eval
+++++++++ from torch.cuda.amp import autocast as autocast
+++++++++ from utils.misc import *
++++++++++from utils.decode import analyze_frame_lengths
+++++++++ class Processor():
+++++++++     def __init__(self, arg):
+++++++++         self.arg = arg
+++++++++@@ -105,13 +106,25 @@ class Processor():
+++++++++                 print('Please appoint --weights.')
+++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++ +
++++++++++            train_wer = seq_eval(
++++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++            )
++++++++++            dev_wer = seq_eval(
++++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++            )
++++++++++            test_wer = seq_eval(
++++++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++++            )
++++++++ +
+++++++++             self.recoder.print_log('Evaluation Done.\n')
++++++++ +
++++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++++++            analyze_frame_lengths()
++++++++ +
+++++++++         elif self.arg.phase == "features":
+++++++++             for mode in ["train", "dev", "test"]:
+++++++++                 seq_feature_generation(
+++++++++@@ -119,6 +132,8 @@ class Processor():
+++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++++++                 )
+++++++++ 
++++++++ +
++++++++ +
++++++++- 
++++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++++-- 
++++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++++-diff --git a/slr_network.py b/slr_network.py
++++++++-index 45295cb..ede70cf 100644
++++++++---- a/slr_network.py
++++++++-+++ b/slr_network.py
++++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++++     def save_arg(self):
+++++++++         arg_dict = vars(self.arg)
+++++++++         if not os.path.exists(self.arg.work_dir):
+++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++++index ede70cf..4ed3e3a 100644
+++++++++--- a/work_dirt/slr_network.py
++++++++++++ b/work_dirt/slr_network.py
+++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++++          x = conv1d_outputs['visual_feat']
++++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++++          tm_outputs = self.temporal_model(x, lgt)
++++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++-+
++++++++-+        print('#######################################################')
+++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++++-
+++++++++-        print('#######################################################')
++++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++++        # print('#######################################################')
++++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++++          pred = None if self.training \
++++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
+++++++++index fc2e926..6c5779b 100644
+++++++++--- a/work_dirt/test.txt
++++++++++++ b/work_dirt/test.txt
+++++++++@@ -8,3 +8,15 @@
+++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
+++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
+++++++++index a2087e5..73fdb64 100644
+++++++++--- a/work_dirt/train.txt
++++++++++++ b/work_dirt/train.txt
+++++++++@@ -14,3 +14,15 @@
+++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++++index acadd49..5933c01 100644
++++++++--- a/work_dirt/log.txt
+++++++++++ b/work_dirt/log.txt
++++++++@@ -116,3 +116,49 @@
++++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+++++++ +
+++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
+++++++ +
+++++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
+++++++ +
+++++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
+++++++ +
+++++++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
+++++++ +
+++++++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
+++++++ +
+++++++++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
+++++++++[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++++[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
+++++++ +
++++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
++++++++index 18ac59b..8f75cf5 100644
++++++++--- a/work_dirt/main.py
+++++++++++ b/work_dirt/main.py
++++++++@@ -21,6 +21,7 @@ import utils
++++++++ from seq_scripts import seq_train, seq_eval
++++++++ from torch.cuda.amp import autocast as autocast
++++++++ from utils.misc import *
+++++++++from utils.decode import analyze_frame_lengths
++++++++ class Processor():
++++++++     def __init__(self, arg):
++++++++         self.arg = arg
++++++++@@ -105,13 +106,25 @@ class Processor():
++++++++                 print('Please appoint --weights.')
++++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++ +
+++++++++            train_wer = seq_eval(
+++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++            )
+++++++++            dev_wer = seq_eval(
+++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++            )
+++++++++            test_wer = seq_eval(
+++++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++++            )
+++++++ +
++++++++             self.recoder.print_log('Evaluation Done.\n')
+++++++ +
+++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++++            analyze_frame_lengths()
+++++++ +
++++++++         elif self.arg.phase == "features":
++++++++             for mode in ["train", "dev", "test"]:
++++++++                 seq_feature_generation(
++++++++@@ -119,6 +132,8 @@ class Processor():
++++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++++                 )
++++++++ 
+++++++ +
+++++++ +
+++++++- 
+++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++++-- 
+++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++++-diff --git a/slr_network.py b/slr_network.py
+++++++-index 45295cb..ede70cf 100644
+++++++---- a/slr_network.py
+++++++-+++ b/slr_network.py
+++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++++     def save_arg(self):
++++++++         arg_dict = vars(self.arg)
++++++++         if not os.path.exists(self.arg.work_dir):
++++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++++index ede70cf..4ed3e3a 100644
++++++++--- a/work_dirt/slr_network.py
+++++++++++ b/work_dirt/slr_network.py
++++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++++          x = conv1d_outputs['visual_feat']
+++++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++++          tm_outputs = self.temporal_model(x, lgt)
+++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++-+
+++++++-+        print('#######################################################')
++++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++++-
++++++++-        print('#######################################################')
+++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++++        # print('#######################################################')
+++++++          outputs = self.classifier(tm_outputs['predictions'])
+++++++          pred = None if self.training \
+++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
++++++++index fc2e926..3202ce4 100644
++++++++--- a/work_dirt/test.txt
+++++++++++ b/work_dirt/test.txt
++++++++@@ -8,3 +8,17 @@
++++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
++++++++index a2087e5..53b5f40 100644
++++++++--- a/work_dirt/train.txt
+++++++++++ b/work_dirt/train.txt
++++++++@@ -14,3 +14,17 @@
++++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++++index acadd49..1b234aa 100644
+++++++--- a/work_dirt/log.txt
++++++++++ b/work_dirt/log.txt
+++++++@@ -116,3 +116,53 @@
+++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
++++++ +
++++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
++++++ +
++++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
++++++ +
++++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
++++++ +
++++++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
++++++ +
++++++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
++++++ +
++++++++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
++++++++[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
++++++ +
++++++++[ Thu Apr  3 11:58:08 2025 ] Model:   slr_network.SLRModel.
++++++++[ Thu Apr  3 11:58:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++++[ Thu Apr  3 11:59:51 2025 ] Evaluation Done.
++++++ +
+++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
+++++++index 18ac59b..7f82626 100644
+++++++--- a/work_dirt/main.py
++++++++++ b/work_dirt/main.py
+++++++@@ -21,6 +21,7 @@ import utils
+++++++ from seq_scripts import seq_train, seq_eval
+++++++ from torch.cuda.amp import autocast as autocast
+++++++ from utils.misc import *
++++++++from utils.decode import analyze_frame_lengths
+++++++ class Processor():
+++++++     def __init__(self, arg):
+++++++         self.arg = arg
+++++++@@ -105,13 +106,25 @@ class Processor():
+++++++                 print('Please appoint --weights.')
+++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++ +
++++++++            train_wer = seq_eval(
++++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++            )
++++++++            dev_wer = seq_eval(
++++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++            )
++++++++            test_wer = seq_eval(
++++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++++            )
++++++ +
+++++++             self.recoder.print_log('Evaluation Done.\n')
++++++ +
++++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++++            analyze_frame_lengths()
++++++ +
+++++++         elif self.arg.phase == "features":
+++++++             for mode in ["train", "dev", "test"]:
+++++++                 seq_feature_generation(
+++++++@@ -119,6 +132,8 @@ class Processor():
+++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++++                 )
+++++++ 
++++++ +
++++++ +
+++++++     def save_arg(self):
+++++++         arg_dict = vars(self.arg)
+++++++         if not os.path.exists(self.arg.work_dir):
+++++++@@ -239,6 +254,7 @@ class Processor():
+++++++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
+++++++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
+++++++         print("Loading Dataprocessing finished.")
++++++++        time.sleep(10)
+++++++     def init_fn(self, worker_id):
+++++++         np.random.seed(int(self.arg.random_seed)+worker_id)
++++++  
++++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++++-- 
++++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++++-diff --git a/slr_network.py b/slr_network.py
++++++-index 45295cb..ede70cf 100644
++++++---- a/slr_network.py
++++++-+++ b/slr_network.py
++++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++++index ede70cf..4ed3e3a 100644
+++++++--- a/work_dirt/slr_network.py
++++++++++ b/work_dirt/slr_network.py
+++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++++          x = conv1d_outputs['visual_feat']
++++++          lgt = conv1d_outputs['feat_len'].cpu()
++++++          tm_outputs = self.temporal_model(x, lgt)
++++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++-+
++++++-+        print('#######################################################')
+++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++++-
+++++++-        print('#######################################################')
++++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++++        # print('#######################################################')
++++++          outputs = self.classifier(tm_outputs['predictions'])
++++++          pred = None if self.training \
++++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
+++++++index fc2e926..f117841 100644
+++++++--- a/work_dirt/test.txt
++++++++++ b/work_dirt/test.txt
+++++++@@ -8,3 +8,19 @@
+++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
+++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
+++++++index a2087e5..dd03f30 100644
+++++++--- a/work_dirt/train.txt
++++++++++ b/work_dirt/train.txt
+++++++@@ -14,3 +14,19 @@
+++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++++index acadd49..272c332 100644
++++++--- a/work_dirt/log.txt
+++++++++ b/work_dirt/log.txt
++++++@@ -116,3 +116,57 @@
++++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+++++ +
+++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
+++++ +
+++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
+++++ +
+++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
+++++ +
+++++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
+++++ +
+++++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
+++++ +
+++++++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
+++++++[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
+++++ +
+++++++[ Thu Apr  3 11:58:08 2025 ] Model:   slr_network.SLRModel.
+++++++[ Thu Apr  3 11:58:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Thu Apr  3 11:59:51 2025 ] Evaluation Done.
+++++ +
+++++++[ Thu Apr  3 13:19:21 2025 ] Model:   slr_network.SLRModel.
+++++++[ Thu Apr  3 13:19:21 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++++[ Thu Apr  3 13:21:06 2025 ] Evaluation Done.
+++++ +
++++++diff --git a/work_dirt/main.py b/work_dirt/main.py
++++++index 18ac59b..7f82626 100644
++++++--- a/work_dirt/main.py
+++++++++ b/work_dirt/main.py
++++++@@ -21,6 +21,7 @@ import utils
++++++ from seq_scripts import seq_train, seq_eval
++++++ from torch.cuda.amp import autocast as autocast
++++++ from utils.misc import *
+++++++from utils.decode import analyze_frame_lengths
++++++ class Processor():
++++++     def __init__(self, arg):
++++++         self.arg = arg
++++++@@ -105,13 +106,25 @@ class Processor():
++++++                 print('Please appoint --weights.')
++++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++ +
+++++++            train_wer = seq_eval(
+++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++            )
+++++++            dev_wer = seq_eval(
+++++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++            )
+++++++            test_wer = seq_eval(
+++++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++++            )
+++++ +
++++++             self.recoder.print_log('Evaluation Done.\n')
+++++ +
+++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++++            analyze_frame_lengths()
+++++ +
++++++         elif self.arg.phase == "features":
++++++             for mode in ["train", "dev", "test"]:
++++++                 seq_feature_generation(
++++++@@ -119,6 +132,8 @@ class Processor():
++++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++++                 )
++++++ 
+++++ +
+++++ +
++++++     def save_arg(self):
++++++         arg_dict = vars(self.arg)
++++++         if not os.path.exists(self.arg.work_dir):
++++++@@ -239,6 +254,7 @@ class Processor():
++++++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
++++++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
++++++         print("Loading Dataprocessing finished.")
+++++++        time.sleep(10)
++++++     def init_fn(self, worker_id):
++++++         np.random.seed(int(self.arg.random_seed)+worker_id)
+++++  
+++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++++-- 
+++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++++-diff --git a/slr_network.py b/slr_network.py
+++++-index 45295cb..ede70cf 100644
+++++---- a/slr_network.py
+++++-+++ b/slr_network.py
+++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++++index ede70cf..4ed3e3a 100644
++++++--- a/work_dirt/slr_network.py
+++++++++ b/work_dirt/slr_network.py
++++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++++          x = conv1d_outputs['visual_feat']
+++++          lgt = conv1d_outputs['feat_len'].cpu()
+++++          tm_outputs = self.temporal_model(x, lgt)
+++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++-+
+++++-+        print('#######################################################')
++++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++++-
++++++-        print('#######################################################')
+++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++++        # print('#######################################################')
+++++          outputs = self.classifier(tm_outputs['predictions'])
+++++          pred = None if self.training \
+++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
++++++index fc2e926..f4de478 100644
++++++--- a/work_dirt/test.txt
+++++++++ b/work_dirt/test.txt
++++++@@ -8,3 +8,21 @@
++++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
++++++index a2087e5..8677315 100644
++++++--- a/work_dirt/train.txt
+++++++++ b/work_dirt/train.txt
++++++@@ -14,3 +14,21 @@
++++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++++index acadd49..9cb26bd 100644
+++++--- a/work_dirt/log.txt
++++++++ b/work_dirt/log.txt
+++++@@ -116,3 +116,61 @@
+++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
++++ +
++++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
++++ +
++++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
++++ +
++++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
++++ +
++++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
++++ +
++++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
++++ +
++++++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
++++++[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
++++ +
++++++[ Thu Apr  3 11:58:08 2025 ] Model:   slr_network.SLRModel.
++++++[ Thu Apr  3 11:58:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Thu Apr  3 11:59:51 2025 ] Evaluation Done.
++++ +
++++++[ Thu Apr  3 13:19:21 2025 ] Model:   slr_network.SLRModel.
++++++[ Thu Apr  3 13:19:21 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Thu Apr  3 13:21:06 2025 ] Evaluation Done.
++++ +
++++++[ Thu Apr  3 13:45:08 2025 ] Model:   slr_network.SLRModel.
++++++[ Thu Apr  3 13:45:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++++[ Thu Apr  3 13:46:47 2025 ] Evaluation Done.
++++ +
+++++diff --git a/work_dirt/main.py b/work_dirt/main.py
+++++index 18ac59b..7f82626 100644
+++++--- a/work_dirt/main.py
++++++++ b/work_dirt/main.py
+++++@@ -21,6 +21,7 @@ import utils
+++++ from seq_scripts import seq_train, seq_eval
+++++ from torch.cuda.amp import autocast as autocast
+++++ from utils.misc import *
++++++from utils.decode import analyze_frame_lengths
+++++ class Processor():
+++++     def __init__(self, arg):
+++++         self.arg = arg
+++++@@ -105,13 +106,25 @@ class Processor():
+++++                 print('Please appoint --weights.')
+++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++ +
++++++            train_wer = seq_eval(
++++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++            )
++++++            dev_wer = seq_eval(
++++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++            )
++++++            test_wer = seq_eval(
++++++                self.arg, self.data_loader["test"], self.model, self.device,
++++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++++            )
++++ +
+++++             self.recoder.print_log('Evaluation Done.\n')
++++ +
++++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++++            analyze_frame_lengths()
++++ +
+++++         elif self.arg.phase == "features":
+++++             for mode in ["train", "dev", "test"]:
+++++                 seq_feature_generation(
+++++@@ -119,6 +132,8 @@ class Processor():
+++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++++                 )
+++++ 
++++ +
++++ +
+++++     def save_arg(self):
+++++         arg_dict = vars(self.arg)
+++++         if not os.path.exists(self.arg.work_dir):
+++++@@ -239,6 +254,7 @@ class Processor():
+++++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
+++++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
+++++         print("Loading Dataprocessing finished.")
++++++        time.sleep(10)
+++++     def init_fn(self, worker_id):
+++++         np.random.seed(int(self.arg.random_seed)+worker_id)
++++  
++++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++++-- 
++++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++++-diff --git a/slr_network.py b/slr_network.py
++++-index 45295cb..ede70cf 100644
++++---- a/slr_network.py
++++-+++ b/slr_network.py
++++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++++index ede70cf..4ed3e3a 100644
+++++--- a/work_dirt/slr_network.py
++++++++ b/work_dirt/slr_network.py
+++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++++          x = conv1d_outputs['visual_feat']
++++          lgt = conv1d_outputs['feat_len'].cpu()
++++          tm_outputs = self.temporal_model(x, lgt)
++++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++-+
++++-+        print('#######################################################')
+++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++++-
+++++-        print('#######################################################')
++++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++++        # print('#######################################################')
++++          outputs = self.classifier(tm_outputs['predictions'])
++++          pred = None if self.training \
++++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
+++++index fc2e926..3c897a6 100644
+++++--- a/work_dirt/test.txt
++++++++ b/work_dirt/test.txt
+++++@@ -8,3 +8,23 @@
+++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
+++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
+++++index a2087e5..866a45c 100644
+++++--- a/work_dirt/train.txt
++++++++ b/work_dirt/train.txt
+++++@@ -14,3 +14,23 @@
+++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++++index acadd49..df1ffa8 100644
++++--- a/work_dirt/log.txt
+++++++ b/work_dirt/log.txt
++++@@ -116,3 +116,65 @@
++++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+++ +
+++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
+++ +
+++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
+++ +
+++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
+++ +
+++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
+++ +
+++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
+++ +
+++++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
+++++[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
+++ +
+++++[ Thu Apr  3 11:58:08 2025 ] Model:   slr_network.SLRModel.
+++++[ Thu Apr  3 11:58:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Thu Apr  3 11:59:51 2025 ] Evaluation Done.
+++ +
+++++[ Thu Apr  3 13:19:21 2025 ] Model:   slr_network.SLRModel.
+++++[ Thu Apr  3 13:19:21 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Thu Apr  3 13:21:06 2025 ] Evaluation Done.
+++ +
+++++[ Thu Apr  3 13:45:08 2025 ] Model:   slr_network.SLRModel.
+++++[ Thu Apr  3 13:45:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Thu Apr  3 13:46:47 2025 ] Evaluation Done.
+++ +
+++++[ Thu Apr  3 13:48:29 2025 ] Model:   slr_network.SLRModel.
+++++[ Thu Apr  3 13:48:29 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++++[ Thu Apr  3 13:50:07 2025 ] Evaluation Done.
+++ +
++++diff --git a/work_dirt/main.py b/work_dirt/main.py
++++index 18ac59b..7f82626 100644
++++--- a/work_dirt/main.py
+++++++ b/work_dirt/main.py
++++@@ -21,6 +21,7 @@ import utils
++++ from seq_scripts import seq_train, seq_eval
++++ from torch.cuda.amp import autocast as autocast
++++ from utils.misc import *
+++++from utils.decode import analyze_frame_lengths
++++ class Processor():
++++     def __init__(self, arg):
++++         self.arg = arg
++++@@ -105,13 +106,25 @@ class Processor():
++++                 print('Please appoint --weights.')
++++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++ +
+++++            train_wer = seq_eval(
+++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++            )
+++++            dev_wer = seq_eval(
+++++                self.arg, self.data_loader["dev"], self.model, self.device,
+++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++            )
+++++            test_wer = seq_eval(
+++++                self.arg, self.data_loader["test"], self.model, self.device,
+++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++++            )
+++ +
++++             self.recoder.print_log('Evaluation Done.\n')
+++ +
+++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++++            analyze_frame_lengths()
+++ +
++++         elif self.arg.phase == "features":
++++             for mode in ["train", "dev", "test"]:
++++                 seq_feature_generation(
++++@@ -119,6 +132,8 @@ class Processor():
++++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++++                 )
++++ 
+++ +
+++ +
++++     def save_arg(self):
++++         arg_dict = vars(self.arg)
++++         if not os.path.exists(self.arg.work_dir):
++++@@ -239,6 +254,7 @@ class Processor():
++++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
++++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
++++         print("Loading Dataprocessing finished.")
+++++        time.sleep(10)
++++     def init_fn(self, worker_id):
++++         np.random.seed(int(self.arg.random_seed)+worker_id)
+++  
+++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+++-- 
+++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+++-diff --git a/slr_network.py b/slr_network.py
+++-index 45295cb..ede70cf 100644
+++---- a/slr_network.py
+++-+++ b/slr_network.py
+++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++++index ede70cf..4ed3e3a 100644
++++--- a/work_dirt/slr_network.py
+++++++ b/work_dirt/slr_network.py
++++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+++          x = conv1d_outputs['visual_feat']
+++          lgt = conv1d_outputs['feat_len'].cpu()
+++          tm_outputs = self.temporal_model(x, lgt)
+++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++-+
+++-+        print('#######################################################')
++++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++++-
++++-        print('#######################################################')
+++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++++        # print('#######################################################')
+++          outputs = self.classifier(tm_outputs['predictions'])
+++          pred = None if self.training \
+++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
++++index fc2e926..0a96650 100644
++++--- a/work_dirt/test.txt
+++++++ b/work_dirt/test.txt
++++@@ -8,3 +8,25 @@
++++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++++[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++++[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
++++index a2087e5..8ac8882 100644
++++--- a/work_dirt/train.txt
+++++++ b/work_dirt/train.txt
++++@@ -14,3 +14,25 @@
++++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++++[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++++[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+++index acadd49..824d3c6 100644
+++--- a/work_dirt/log.txt
++++++ b/work_dirt/log.txt
+++@@ -116,3 +116,69 @@
+++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
++ +
++++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
++ +
++++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
++ +
++++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
++ +
++++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
++ +
++++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
++ +
++++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
++++[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
++ +
++++[ Thu Apr  3 11:58:08 2025 ] Model:   slr_network.SLRModel.
++++[ Thu Apr  3 11:58:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Thu Apr  3 11:59:51 2025 ] Evaluation Done.
++ +
++++[ Thu Apr  3 13:19:21 2025 ] Model:   slr_network.SLRModel.
++++[ Thu Apr  3 13:19:21 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Thu Apr  3 13:21:06 2025 ] Evaluation Done.
++ +
++++[ Thu Apr  3 13:45:08 2025 ] Model:   slr_network.SLRModel.
++++[ Thu Apr  3 13:45:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Thu Apr  3 13:46:47 2025 ] Evaluation Done.
++ +
++++[ Thu Apr  3 13:48:29 2025 ] Model:   slr_network.SLRModel.
++++[ Thu Apr  3 13:48:29 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Thu Apr  3 13:50:07 2025 ] Evaluation Done.
++ +
++++[ Thu Apr  3 13:52:37 2025 ] Model:   slr_network.SLRModel.
++++[ Thu Apr  3 13:52:37 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++++[ Thu Apr  3 13:54:15 2025 ] Evaluation Done.
++ +
+++diff --git a/work_dirt/main.py b/work_dirt/main.py
+++index 18ac59b..7f82626 100644
+++--- a/work_dirt/main.py
++++++ b/work_dirt/main.py
+++@@ -21,6 +21,7 @@ import utils
+++ from seq_scripts import seq_train, seq_eval
+++ from torch.cuda.amp import autocast as autocast
+++ from utils.misc import *
++++from utils.decode import analyze_frame_lengths
+++ class Processor():
+++     def __init__(self, arg):
+++         self.arg = arg
+++@@ -105,13 +106,25 @@ class Processor():
+++                 print('Please appoint --weights.')
+++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++ +
++++            train_wer = seq_eval(
++++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++            )
++++            dev_wer = seq_eval(
++++                self.arg, self.data_loader["dev"], self.model, self.device,
++++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++            )
++++            test_wer = seq_eval(
++++                self.arg, self.data_loader["test"], self.model, self.device,
++++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++++            )
++ +
+++             self.recoder.print_log('Evaluation Done.\n')
++ +
++++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++++            analyze_frame_lengths()
++ +
+++         elif self.arg.phase == "features":
+++             for mode in ["train", "dev", "test"]:
+++                 seq_feature_generation(
+++@@ -119,6 +132,8 @@ class Processor():
+++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+++                 )
+++ 
++ +
++ +
+++     def save_arg(self):
+++         arg_dict = vars(self.arg)
+++         if not os.path.exists(self.arg.work_dir):
+++@@ -239,6 +254,7 @@ class Processor():
+++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
+++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
+++         print("Loading Dataprocessing finished.")
++++        time.sleep(10)
+++     def init_fn(self, worker_id):
+++         np.random.seed(int(self.arg.random_seed)+worker_id)
++  
++--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
++-- 
++-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
++-diff --git a/slr_network.py b/slr_network.py
++-index 45295cb..ede70cf 100644
++---- a/slr_network.py
++-+++ b/slr_network.py
++-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+++index ede70cf..4ed3e3a 100644
+++--- a/work_dirt/slr_network.py
++++++ b/work_dirt/slr_network.py
+++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
++          x = conv1d_outputs['visual_feat']
++          lgt = conv1d_outputs['feat_len'].cpu()
++          tm_outputs = self.temporal_model(x, lgt)
++-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++-+
++-+        print('#######################################################')
+++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+++-
+++-        print('#######################################################')
++++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++++        # print('#######################################################')
++          outputs = self.classifier(tm_outputs['predictions'])
++          pred = None if self.training \
++              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
+++index fc2e926..98aec99 100644
+++--- a/work_dirt/test.txt
++++++ b/work_dirt/test.txt
+++@@ -8,3 +8,27 @@
+++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
+++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++++[ Thu Apr  3 13:54:15 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++++[ Thu Apr  3 13:54:15 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
+++index a2087e5..3daa496 100644
+++--- a/work_dirt/train.txt
++++++ b/work_dirt/train.txt
+++@@ -14,3 +14,27 @@
+++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++++[ Thu Apr  3 13:53:12 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++++[ Thu Apr  3 13:53:12 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++diff --git a/work_dirt/log.txt b/work_dirt/log.txt
++index acadd49..2b5e6a4 100644
++--- a/work_dirt/log.txt
+++++ b/work_dirt/log.txt
++@@ -116,3 +116,71 @@
++ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
++ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+ +
+++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
+ +
+++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
+ +
+++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
+ +
+++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
+ +
+++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
+ +
+++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
+++[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
+ +
+++[ Thu Apr  3 11:58:08 2025 ] Model:   slr_network.SLRModel.
+++[ Thu Apr  3 11:58:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Thu Apr  3 11:59:51 2025 ] Evaluation Done.
+ +
+++[ Thu Apr  3 13:19:21 2025 ] Model:   slr_network.SLRModel.
+++[ Thu Apr  3 13:19:21 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Thu Apr  3 13:21:06 2025 ] Evaluation Done.
+ +
+++[ Thu Apr  3 13:45:08 2025 ] Model:   slr_network.SLRModel.
+++[ Thu Apr  3 13:45:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Thu Apr  3 13:46:47 2025 ] Evaluation Done.
+ +
+++[ Thu Apr  3 13:48:29 2025 ] Model:   slr_network.SLRModel.
+++[ Thu Apr  3 13:48:29 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Thu Apr  3 13:50:07 2025 ] Evaluation Done.
+ +
+++[ Thu Apr  3 13:52:37 2025 ] Model:   slr_network.SLRModel.
+++[ Thu Apr  3 13:52:37 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+++[ Thu Apr  3 13:54:15 2025 ] Evaluation Done.
+ +
+++[ Thu Apr  3 16:23:52 2025 ] Model:   slr_network.SLRModel.
+++[ Thu Apr  3 16:23:52 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++diff --git a/work_dirt/main.py b/work_dirt/main.py
++index 18ac59b..7f82626 100644
++--- a/work_dirt/main.py
+++++ b/work_dirt/main.py
++@@ -21,6 +21,7 @@ import utils
++ from seq_scripts import seq_train, seq_eval
++ from torch.cuda.amp import autocast as autocast
++ from utils.misc import *
+++from utils.decode import analyze_frame_lengths
++ class Processor():
++     def __init__(self, arg):
++         self.arg = arg
++@@ -105,13 +106,25 @@ class Processor():
++                 print('Please appoint --weights.')
++             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
++             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
++-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
++-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
++-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
++-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
++-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+ +
+++            train_wer = seq_eval(
+++                self.arg, self.data_loader["train_eval"], self.model, self.device,
+++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++            )
+++            dev_wer = seq_eval(
+++                self.arg, self.data_loader["dev"], self.model, self.device,
+++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++            )
+++            test_wer = seq_eval(
+++                self.arg, self.data_loader["test"], self.model, self.device,
+++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+++            )
+ +
++             self.recoder.print_log('Evaluation Done.\n')
+ +
+++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+++            analyze_frame_lengths()
+ +
++         elif self.arg.phase == "features":
++             for mode in ["train", "dev", "test"]:
++                 seq_feature_generation(
++@@ -119,6 +132,8 @@ class Processor():
++                     self.model, self.device, mode, self.arg.work_dir, self.recoder
++                 )
++ 
+ +
+ +
++     def save_arg(self):
++         arg_dict = vars(self.arg)
++         if not os.path.exists(self.arg.work_dir):
++@@ -239,6 +254,7 @@ class Processor():
++             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
++             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
++         print("Loading Dataprocessing finished.")
+++        time.sleep(10)
++     def init_fn(self, worker_id):
++         np.random.seed(int(self.arg.random_seed)+worker_id)
+  
+--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
+-- 
+-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
+-diff --git a/slr_network.py b/slr_network.py
+-index 45295cb..ede70cf 100644
+---- a/slr_network.py
+-+++ b/slr_network.py
+-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
++diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
++index ede70cf..4ed3e3a 100644
++--- a/work_dirt/slr_network.py
+++++ b/work_dirt/slr_network.py
++@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
+          x = conv1d_outputs['visual_feat']
+          lgt = conv1d_outputs['feat_len'].cpu()
+          tm_outputs = self.temporal_model(x, lgt)
+-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+-+
+-+        print('#######################################################')
++-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
++-
++-        print('#######################################################')
+++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+++        # print('#######################################################')
+          outputs = self.classifier(tm_outputs['predictions'])
+          pred = None if self.training \
+              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
++diff --git a/work_dirt/test.txt b/work_dirt/test.txt
++index fc2e926..98aec99 100644
++--- a/work_dirt/test.txt
+++++ b/work_dirt/test.txt
++@@ -8,3 +8,27 @@
++ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+++[ Thu Apr  3 13:54:15 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+++[ Thu Apr  3 13:54:15 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++diff --git a/work_dirt/train.txt b/work_dirt/train.txt
++index a2087e5..3daa496 100644
++--- a/work_dirt/train.txt
+++++ b/work_dirt/train.txt
++@@ -14,3 +14,27 @@
++ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+++[ Thu Apr  3 13:53:12 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+++[ Thu Apr  3 13:53:12 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+diff --git a/work_dirt/log.txt b/work_dirt/log.txt
+index acadd49..eb46f65 100644
+--- a/work_dirt/log.txt
++++ b/work_dirt/log.txt
+@@ -116,3 +116,75 @@
+ [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+ [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
+ [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
++[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
++[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
++[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
++[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
++[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
++[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
++[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
 +
++[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
 +
++[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
 +
++[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
 +
++[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
 +
++[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
 +
++[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
++[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
 +
++[ Thu Apr  3 11:58:08 2025 ] Model:   slr_network.SLRModel.
++[ Thu Apr  3 11:58:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Thu Apr  3 11:59:51 2025 ] Evaluation Done.
 +
++[ Thu Apr  3 13:19:21 2025 ] Model:   slr_network.SLRModel.
++[ Thu Apr  3 13:19:21 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Thu Apr  3 13:21:06 2025 ] Evaluation Done.
 +
++[ Thu Apr  3 13:45:08 2025 ] Model:   slr_network.SLRModel.
++[ Thu Apr  3 13:45:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Thu Apr  3 13:46:47 2025 ] Evaluation Done.
 +
++[ Thu Apr  3 13:48:29 2025 ] Model:   slr_network.SLRModel.
++[ Thu Apr  3 13:48:29 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Thu Apr  3 13:50:07 2025 ] Evaluation Done.
 +
++[ Thu Apr  3 13:52:37 2025 ] Model:   slr_network.SLRModel.
++[ Thu Apr  3 13:52:37 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Thu Apr  3 13:54:15 2025 ] Evaluation Done.
 +
++[ Thu Apr  3 16:23:52 2025 ] Model:   slr_network.SLRModel.
++[ Thu Apr  3 16:23:52 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Thu Apr  3 16:24:51 2025 ] Model:   slr_network.SLRModel.
++[ Thu Apr  3 16:24:51 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
++[ Thu Apr  3 16:26:30 2025 ] Evaluation Done.
 +
+diff --git a/work_dirt/main.py b/work_dirt/main.py
+index 18ac59b..4e8393c 100644
+--- a/work_dirt/main.py
++++ b/work_dirt/main.py
+@@ -21,6 +21,7 @@ import utils
+ from seq_scripts import seq_train, seq_eval
+ from torch.cuda.amp import autocast as autocast
+ from utils.misc import *
++from utils.decode import analyze_frame_lengths
+ class Processor():
+     def __init__(self, arg):
+         self.arg = arg
+@@ -105,13 +106,25 @@ class Processor():
+                 print('Please appoint --weights.')
+             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
+             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
+-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
+-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
+-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
+-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
 +
++            train_wer = seq_eval(
++                self.arg, self.data_loader["train_eval"], self.model, self.device,
++                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++            )
++            dev_wer = seq_eval(
++                self.arg, self.data_loader["dev"], self.model, self.device,
++                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++            )
++            test_wer = seq_eval(
++                self.arg, self.data_loader["test"], self.model, self.device,
++                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
++            )
 +
+             self.recoder.print_log('Evaluation Done.\n')
 +
++            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
++            decode.analyze_frame_lengths()
 +
+         elif self.arg.phase == "features":
+             for mode in ["train", "dev", "test"]:
+                 seq_feature_generation(
+@@ -119,6 +132,8 @@ class Processor():
+                     self.model, self.device, mode, self.arg.work_dir, self.recoder
+                 )
+ 
 +
 +
+     def save_arg(self):
+         arg_dict = vars(self.arg)
+         if not os.path.exists(self.arg.work_dir):
+@@ -225,12 +240,14 @@ class Processor():
+         print("Loading Dataprocessing")
+         self.feeder = import_class(self.arg.feeder)
+         shutil.copy2(inspect.getfile(self.feeder), self.arg.work_dir)
 +
+         if self.arg.dataset == 'CSL':
+             dataset_list = zip(["train", "dev"], [True, False])
+         elif 'phoenix' in self.arg.dataset:
+             dataset_list = zip(["train", "train_eval", "dev", "test"], [True, False, False, False]) 
+         elif self.arg.dataset == 'CSL-Daily':
+             dataset_list = zip(["train", "train_eval", "dev", "test"], [True, False, False, False])
 +
+         for idx, (mode, train_flag) in enumerate(dataset_list):
+             arg = self.arg.feeder_args
+             arg["prefix"] = self.arg.dataset_info['dataset_root']
+@@ -239,6 +256,7 @@ class Processor():
+             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
+             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
+         print("Loading Dataprocessing finished.")
++        # time.sleep(10)
+     def init_fn(self, worker_id):
+         np.random.seed(int(self.arg.random_seed)+worker_id)
  
--    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
-- 
-+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
-diff --git a/slr_network.py b/slr_network.py
-index 45295cb..ede70cf 100644
---- a/slr_network.py
-+++ b/slr_network.py
-@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
+diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
+index ede70cf..4ed3e3a 100644
+--- a/work_dirt/slr_network.py
++++ b/work_dirt/slr_network.py
+@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
          x = conv1d_outputs['visual_feat']
          lgt = conv1d_outputs['feat_len'].cpu()
          tm_outputs = self.temporal_model(x, lgt)
-+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
-+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
-+
-+        print('#######################################################')
+-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+-
+-        print('#######################################################')
++        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
++        # print('#######################################################')
          outputs = self.classifier(tm_outputs['predictions'])
          pred = None if self.training \
              else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
+diff --git a/work_dirt/test.txt b/work_dirt/test.txt
+index fc2e926..4c7c0cd 100644
+--- a/work_dirt/test.txt
++++ b/work_dirt/test.txt
+@@ -8,3 +8,29 @@
+ [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
+ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+ [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Thu Apr  3 13:54:15 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Thu Apr  3 13:54:15 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
++[ Thu Apr  3 16:26:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
++[ Thu Apr  3 16:26:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+diff --git a/work_dirt/train.txt b/work_dirt/train.txt
+index a2087e5..07cbf5e 100644
+--- a/work_dirt/train.txt
++++ b/work_dirt/train.txt
+@@ -14,3 +14,29 @@
+ [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+ [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Thu Apr  3 13:53:12 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Thu Apr  3 13:53:12 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
++[ Thu Apr  3 16:25:26 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
++[ Thu Apr  3 16:25:26 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
diff --git a/work_dirt/log.txt b/work_dirt/log.txt
index acadd49..cf74e4b 100644
--- a/work_dirt/log.txt
+++ b/work_dirt/log.txt
@@ -116,3 +116,79 @@
 [ Tue Mar 25 13:02:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
 [ Tue Mar 25 13:03:47 2025 ] Model:   slr_network.SLRModel.
 [ Tue Mar 25 13:03:47 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Fri Mar 28 11:11:19 2025 ] Model:   slr_network.SLRModel.
+[ Fri Mar 28 11:11:19 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Fri Mar 28 11:12:07 2025 ] Model:   slr_network.SLRModel.
+[ Fri Mar 28 11:12:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Fri Mar 28 11:13:27 2025 ] Model:   slr_network.SLRModel.
+[ Fri Mar 28 11:13:27 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Fri Mar 28 15:07:54 2025 ] Model:   slr_network.SLRModel.
+[ Fri Mar 28 15:07:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Tue Apr  1 15:59:59 2025 ] Model:   slr_network.SLRModel.
+[ Tue Apr  1 15:59:59 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Tue Apr  1 16:02:57 2025 ] Model:   slr_network.SLRModel.
+[ Tue Apr  1 16:02:57 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Tue Apr  1 16:09:18 2025 ] Model:   slr_network.SLRModel.
+[ Tue Apr  1 16:09:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 14:07:30 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 14:07:30 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 14:09:09 2025 ] Evaluation Done.
+
+[ Wed Apr  2 14:43:01 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 14:43:01 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 14:44:41 2025 ] Evaluation Done.
+
+[ Wed Apr  2 15:42:48 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 15:42:48 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 15:44:26 2025 ] Evaluation Done.
+
+[ Wed Apr  2 16:06:33 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 16:06:33 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 16:08:12 2025 ] Evaluation Done.
+
+[ Wed Apr  2 16:48:54 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 16:48:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 16:50:35 2025 ] Evaluation Done.
+
+[ Wed Apr  2 16:51:18 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 16:51:18 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 16:52:57 2025 ] Evaluation Done.
+
+[ Wed Apr  2 16:53:54 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 16:53:54 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 17:45:32 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 17:45:32 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 17:45:53 2025 ] Model:   slr_network.SLRModel.
+[ Wed Apr  2 17:45:53 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Wed Apr  2 17:47:30 2025 ] Evaluation Done.
+
+[ Thu Apr  3 11:58:08 2025 ] Model:   slr_network.SLRModel.
+[ Thu Apr  3 11:58:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Thu Apr  3 11:59:51 2025 ] Evaluation Done.
+
+[ Thu Apr  3 13:19:21 2025 ] Model:   slr_network.SLRModel.
+[ Thu Apr  3 13:19:21 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Thu Apr  3 13:21:06 2025 ] Evaluation Done.
+
+[ Thu Apr  3 13:45:08 2025 ] Model:   slr_network.SLRModel.
+[ Thu Apr  3 13:45:08 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Thu Apr  3 13:46:47 2025 ] Evaluation Done.
+
+[ Thu Apr  3 13:48:29 2025 ] Model:   slr_network.SLRModel.
+[ Thu Apr  3 13:48:29 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Thu Apr  3 13:50:07 2025 ] Evaluation Done.
+
+[ Thu Apr  3 13:52:37 2025 ] Model:   slr_network.SLRModel.
+[ Thu Apr  3 13:52:37 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Thu Apr  3 13:54:15 2025 ] Evaluation Done.
+
+[ Thu Apr  3 16:23:52 2025 ] Model:   slr_network.SLRModel.
+[ Thu Apr  3 16:23:52 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Thu Apr  3 16:24:51 2025 ] Model:   slr_network.SLRModel.
+[ Thu Apr  3 16:24:51 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Thu Apr  3 16:26:30 2025 ] Evaluation Done.
+
+[ Fri Apr  4 14:16:07 2025 ] Model:   slr_network.SLRModel.
+[ Fri Apr  4 14:16:07 2025 ] Weights: /home/jhy/SignGraph/_best_model.pt.
+[ Fri Apr  4 14:17:46 2025 ] Evaluation Done.
+
diff --git a/work_dirt/main.py b/work_dirt/main.py
index 18ac59b..2349480 100644
--- a/work_dirt/main.py
+++ b/work_dirt/main.py
@@ -21,6 +21,7 @@ import utils
 from seq_scripts import seq_train, seq_eval
 from torch.cuda.amp import autocast as autocast
 from utils.misc import *
+from utils.decode import analyze_frame_lengths
 class Processor():
     def __init__(self, arg):
         self.arg = arg
@@ -105,13 +106,25 @@ class Processor():
                 print('Please appoint --weights.')
             self.recoder.print_log('Model:   {}.'.format(self.arg.model))
             self.recoder.print_log('Weights: {}.'.format(self.arg.load_weights))
-            train_wer = seq_eval(self.arg, self.data_loader["train_eval"], self.model, self.device,
-                                 "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
-            dev_wer = seq_eval(self.arg, self.data_loader["dev"], self.model, self.device,
-                               "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
-            test_wer = seq_eval(self.arg, self.data_loader["test"], self.model, self.device,
-                                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool)
+
+            train_wer = seq_eval(
+                self.arg, self.data_loader["train_eval"], self.model, self.device,
+                "train", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+            )
+            dev_wer = seq_eval(
+                self.arg, self.data_loader["dev"], self.model, self.device,
+                "dev", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+            )
+            test_wer = seq_eval(
+                self.arg, self.data_loader["test"], self.model, self.device,
+                "test", 6667, self.arg.work_dir, self.recoder, self.arg.evaluate_tool
+            )
+
             self.recoder.print_log('Evaluation Done.\n')
+
+            # ✅ 프레임 길이 분석 추가 (기존 코드 변경 없이 추가만!)
+            analyze_frame_lengths()
+
         elif self.arg.phase == "features":
             for mode in ["train", "dev", "test"]:
                 seq_feature_generation(
@@ -119,6 +132,8 @@ class Processor():
                     self.model, self.device, mode, self.arg.work_dir, self.recoder
                 )
 
+
+
     def save_arg(self):
         arg_dict = vars(self.arg)
         if not os.path.exists(self.arg.work_dir):
@@ -225,12 +240,14 @@ class Processor():
         print("Loading Dataprocessing")
         self.feeder = import_class(self.arg.feeder)
         shutil.copy2(inspect.getfile(self.feeder), self.arg.work_dir)
+
         if self.arg.dataset == 'CSL':
             dataset_list = zip(["train", "dev"], [True, False])
         elif 'phoenix' in self.arg.dataset:
             dataset_list = zip(["train", "train_eval", "dev", "test"], [True, False, False, False]) 
         elif self.arg.dataset == 'CSL-Daily':
             dataset_list = zip(["train", "train_eval", "dev", "test"], [True, False, False, False])
+
         for idx, (mode, train_flag) in enumerate(dataset_list):
             arg = self.arg.feeder_args
             arg["prefix"] = self.arg.dataset_info['dataset_root']
@@ -239,6 +256,7 @@ class Processor():
             self.dataset[mode] = self.feeder(gloss_dict=self.gloss_dict, kernel_size= self.kernel_sizes, dataset=self.arg.dataset, **arg)
             self.data_loader[mode] = self.build_dataloader(self.dataset[mode], mode, train_flag)
         print("Loading Dataprocessing finished.")
+        # time.sleep(10)
     def init_fn(self, worker_id):
         np.random.seed(int(self.arg.random_seed)+worker_id)
 
@@ -247,7 +265,7 @@ class Processor():
         if len(self.device.gpu_list) > 1:
             if train_flag:
                 sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=train_flag)
-            else:
+            else: # test flag
                 sampler = torch.utils.data.SequentialSampler(dataset)
             batch_size = self.arg.batch_size if mode == "train" else self.arg.test_batch_size
             loader = torch.utils.data.DataLoader(
diff --git a/work_dirt/slr_network.py b/work_dirt/slr_network.py
index ede70cf..4ed3e3a 100644
--- a/work_dirt/slr_network.py
+++ b/work_dirt/slr_network.py
@@ -89,10 +89,8 @@ class SLRModel(nn.Module):
         x = conv1d_outputs['visual_feat']
         lgt = conv1d_outputs['feat_len'].cpu()
         tm_outputs = self.temporal_model(x, lgt)
-        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
-        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
-
-        print('#######################################################')
+        # print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+        # print('#######################################################')
         outputs = self.classifier(tm_outputs['predictions'])
         pred = None if self.training \
             else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
diff --git a/work_dirt/test.txt b/work_dirt/test.txt
index fc2e926..f50e2f6 100644
--- a/work_dirt/test.txt
+++ b/work_dirt/test.txt
@@ -8,3 +8,31 @@
 [ Fri Mar 21 16:19:11 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1734  ins:2.2332, del:5.6522
 [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
 [ Fri Mar 21 17:53:45 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Wed Apr  2 14:09:09 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Wed Apr  2 14:44:41 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Wed Apr  2 15:44:26 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Wed Apr  2 16:08:12 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Wed Apr  2 16:50:35 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Wed Apr  2 16:52:56 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Wed Apr  2 16:52:57 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Wed Apr  2 17:47:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Thu Apr  3 11:59:51 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Thu Apr  3 13:21:06 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Thu Apr  3 13:46:47 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Thu Apr  3 13:50:07 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Thu Apr  3 13:54:15 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Thu Apr  3 13:54:15 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Thu Apr  3 16:26:30 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Thu Apr  3 16:26:30 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
+[ Fri Apr  4 14:17:46 2025 ] 	Epoch: 6667 test done. Conv wer: 18.4632  ins:1.4941, del:6.2967
+[ Fri Apr  4 14:17:46 2025 ] 	Epoch: 6667 test done. LSTM wer: 18.1430  ins:1.7076, del:5.0160
diff --git a/work_dirt/train.txt b/work_dirt/train.txt
index a2087e5..912025f 100644
--- a/work_dirt/train.txt
+++ b/work_dirt/train.txt
@@ -14,3 +14,31 @@
 [ Tue Mar 25 13:03:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
 [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
 [ Tue Mar 25 13:04:29 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Wed Apr  2 14:08:06 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Wed Apr  2 14:43:37 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Wed Apr  2 15:43:24 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Wed Apr  2 16:07:09 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Wed Apr  2 16:49:30 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Wed Apr  2 16:51:54 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Wed Apr  2 17:46:27 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Thu Apr  3 11:58:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Thu Apr  3 13:20:02 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Thu Apr  3 13:45:45 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Thu Apr  3 13:49:04 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Thu Apr  3 13:53:12 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Thu Apr  3 13:53:12 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Thu Apr  3 16:25:26 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Thu Apr  3 16:25:26 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
+[ Fri Apr  4 14:16:43 2025 ] 	Epoch: 6667 train done. Conv wer: 0.6843  ins:0.0978, del:0.3910
+[ Fri Apr  4 14:16:43 2025 ] 	Epoch: 6667 train done. LSTM wer: 0.4888  ins:0.0000, del:0.2933
