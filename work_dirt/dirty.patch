diff --git a/README.md b/README.md
index bdbc17f..8cb240b 100644
--- a/README.md
+++ b/README.md
@@ -43,7 +43,7 @@ We make some imporvments of our code, and provide newest checkpoionts and better
 
 
 ​To evaluate the pretrained model, choose the dataset from phoenix2014/phoenix2014-T/CSL/CSL-Daily in line 3 in ./config/baseline.yaml first, and run the command below：   
-`python main.py --device your_device --load-weights path_to_weight.pt --phase test`
+`python main.py --device your_device --load-weights /home/jhy/SignGraph/_best_model.pt --phase test`
 
 ### Training
 
diff --git a/configs/baseline.yaml b/configs/baseline.yaml
index bfc1da8..25ffa61 100644
--- a/configs/baseline.yaml
+++ b/configs/baseline.yaml
@@ -1,14 +1,14 @@
 feeder: dataset.dataloader_video.BaseFeeder
 phase: train
-dataset: phoenix2014-T
+dataset: phoenix2014
 #CSL-Daily
 # dataset: phoenix14-si5
 
 work_dir: ./work_dirt/
-batch_size: 4
+batch_size: 1
 random_seed: 0 
-test_batch_size: 4
-num_worker: 20
+test_batch_size: 1
+num_worker: 3
 device: 0
 log_interval: 10000
 eval_interval: 1
diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
index 555f4b8..c126e7a 100644
--- a/dataset/dataloader_video.py
+++ b/dataset/dataloader_video.py
@@ -40,7 +40,10 @@ class BaseFeeder(data.Dataset):
         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
         #/data1/gsw/CSL-Daily/sentence/frames_512x512
         self.transform_mode = "train" if transform_mode else "test"
-        self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+        #self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+        full_inputs_dict = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
+        self.inputs_list = dict(list(full_inputs_dict.items())[:100]) #select length
+
         print(mode, len(self))
         self.data_aug = self.transform()
         print("")
@@ -60,27 +63,52 @@ class BaseFeeder(data.Dataset):
             return input_data, label, self.inputs_list[idx]['original_info']
 
     def read_video(self, index):
-        # load file info
         fi = self.inputs_list[index]
+    
         if 'phoenix' in self.dataset:
-            img_folder = os.path.join(self.prefix, "features/fullFrame-210x260px/" + fi['folder'])
+#            frame_pattern = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/train/01June_2011_Wednesday_heute_default-5/1/*.png")
+#            img_list = sorted(glob.glob(frame_pattern))
+#            print(img_list)
+
+#            print("[LOG] Using phoenix")
+
+            self.prefix = os.path.expanduser("~/SignGraph/phoenix2014-release/phoenix-2014-multisigner")
+            img_folder = os.path.join(self.prefix, "features", "fullFrame-210x260px", fi['folder'])
 
+#            print(f"len img_folder:{len(img_folder)}, type: {type(img_folder)}")
+#            print(img_folder)
+#            img_list = sorted(glob.glob(img_folder))
+#            print(f"[DEBUG] Found {len(img_list)} frames")
+#            print(len(img_list))
         elif self.dataset == 'CSL-Daily':
-            img_folder = os.path.join(self.prefix, "sentence/frames_512x512/" + fi['folder'])
+            img_folder = os.path.join(self.prefix, "sentence", "frames_512x512", fi['folder'])
+    
         img_list = sorted(glob.glob(img_folder))
+    
+        if len(img_list) == 0:
+            print(f"[WARNING] No frames found in: {img_list}")
+    
         img_list = img_list[int(torch.randint(0, self.frame_interval, [1]))::self.frame_interval]
+    
         label_list = []
-        if self.dataset=='phoenix2014':
+        if self.dataset == 'phoenix2014':
             fi['label'] = clean_phoenix_2014(fi['label'])
-        if self.dataset=='phoenix2014-T':
-            fi['label']=clean_phoenix_2014_trans(fi['label'])
+        elif self.dataset == 'phoenix2014-T':
+            fi['label'] = clean_phoenix_2014_trans(fi['label'])
+    
         for phase in fi['label'].split(" "):
-            if phase == '':
-                continue
-            if phase in self.dict.keys():
+            if phase and phase in self.dict:
                 label_list.append(self.dict[phase][0])
-        return [cv2.cvtColor(cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
-                             cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
+    
+        video = [
+            cv2.cvtColor(
+                cv2.resize(cv2.imread(img_path), (256, 256), interpolation=cv2.INTER_LANCZOS4),
+                cv2.COLOR_BGR2RGB
+            )   
+            for img_path in img_list
+        ]
+    
+        return video, label_list, fi
 
     def read_features(self, index):
         # load file info
diff --git a/main.py b/main.py
index 9e68cee..18ac59b 100644
--- a/main.py
+++ b/main.py
@@ -256,7 +256,7 @@ class Processor():
                 batch_size=batch_size,
                 collate_fn=self.feeder.collate_fn,
                 num_workers=self.arg.num_worker,
-                pin_memory=True,
+                pin_memory=False,
                 worker_init_fn=self.init_fn,
             )
             return loader
@@ -268,7 +268,7 @@ class Processor():
                 drop_last=train_flag,
                 num_workers=self.arg.num_worker,  # if train_flag else 0
                 collate_fn=self.feeder.collate_fn,
-                pin_memory=True,
+                pin_memory=False,
                 worker_init_fn=self.init_fn,
             )
 
diff --git a/seq_scripts.py b/seq_scripts.py
index 528856d..d8fcaf9 100644
--- a/seq_scripts.py
+++ b/seq_scripts.py
@@ -61,9 +61,11 @@ def seq_train(loader, model, optimizer, device, epoch_idx, recoder):
     return
 
 
+import csv 
+from jiwer import wer as jiwer_wer
 def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluate_tool="python"):
     model.eval()
-    results=defaultdict(dict)
+    results = defaultdict(dict)
 
     for batch_idx, data in enumerate(tqdm(loader)):
         recoder.record_timer("device")
@@ -79,20 +81,106 @@ def seq_eval(cfg, loader, model, device, mode, epoch, work_dir, recoder, evaluat
                 results[inf]['conv_sents'] = conv_sents
                 results[inf]['recognized_sents'] = recognized_sents
                 results[inf]['gloss'] = gl
+
     gls_hyp = [' '.join(results[n]['conv_sents']) for n in results]
     gls_ref = [results[n]['gloss'] for n in results]
     wer_results_con = wer_list(hypotheses=gls_hyp, references=gls_ref)
+
     gls_hyp = [' '.join(results[n]['recognized_sents']) for n in results]
     wer_results = wer_list(hypotheses=gls_hyp, references=gls_ref)
-    if wer_results['wer'] < wer_results_con['wer']:
-        reg_per = wer_results
-    else:
-        reg_per = wer_results_con
+
+    reg_per = wer_results if wer_results['wer'] < wer_results_con['wer'] else wer_results_con
+
     recoder.print_log('\tEpoch: {} {} done. Conv wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
         epoch, mode, wer_results_con['wer'], wer_results_con['ins'], wer_results_con['del']),
         f"{work_dir}/{mode}.txt")
+
     recoder.print_log('\tEpoch: {} {} done. LSTM wer: {:.4f}  ins:{:.4f}, del:{:.4f}'.format(
-        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']), f"{work_dir}/{mode}.txt")
+        epoch, mode, wer_results['wer'], wer_results['ins'], wer_results['del']),
+        f"{work_dir}/{mode}.txt")
+
+    # ✅ 전체 결과 CSV로 저장
+    save_folder = os.path.join(work_dir, f"{mode}_detailed_results")
+    os.makedirs(save_folder, exist_ok=True)
+    csv_path = os.path.join(save_folder, "wer_results.csv")
+
+    rows = []
+    for file_id in results:
+        gt = results[file_id]['gloss']
+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+        conv_wer = jiwer_wer(gt, conv_pred)
+        lstm_wer = jiwer_wer(gt, lstm_pred)
+
+        rows.append([
+            file_id,
+            gt,
+            conv_pred,
+            f"{conv_wer:.4f}",
+            lstm_pred,
+            f"{lstm_wer:.4f}"
+        ])
+
+    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
+        writer = csv.writer(f)
+        writer.writerow(['file_id', 'gt', 'conv_pred', 'conv_wer', 'lstm_pred', 'lstm_wer'])
+        writer.writerows(rows)
+
+    print(f"\n✅ 전체 결과가 다음 경로에 저장되었습니다:\n{csv_path}\n")
+
+
+
+    # WER 기준 상위 5개 샘플 출력
+    sample_wers = []
+    for file_id in results:
+        gt = results[file_id]['gloss']
+        conv_pred = ' '.join(results[file_id]['conv_sents'])
+        lstm_pred = ' '.join(results[file_id]['recognized_sents'])
+    
+        conv_wer = jiwer_wer(gt, conv_pred)
+        lstm_wer = jiwer_wer(gt, lstm_pred)
+    
+        sample_wers.append({
+            'file_id': file_id,
+            'gt': gt,
+            'conv_pred': conv_pred,
+            'conv_wer': conv_wer,
+            'lstm_pred': lstm_pred,
+            'lstm_wer': lstm_wer,
+            'max_wer': max(conv_wer, lstm_wer)
+        })
+
+    top5 = sorted(sample_wers, key=lambda x: x['max_wer'], reverse=True)[:5]
+    
+    print("\n📢 WER 상위 5개 샘플 (Conv vs LSTM):\n")
+    for sample in top5:
+        print(f"[{sample['file_id']}] WER (Conv: {sample['conv_wer']:.4f}, LSTM: {sample['lstm_wer']:.4f})")
+        print(f"GT   : {sample['gt']}")
+        print(f"Conv : {sample['conv_pred']}")
+        print(f"LSTM : {sample['lstm_pred']}")
+        print("-" * 60)
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 
-    return {"wer":reg_per['wer'], "ins":reg_per['ins'], 'del':reg_per['del']}
- 
+    return {"wer": reg_per['wer'], "ins": reg_per['ins'], 'del': reg_per['del']}
diff --git a/slr_network.py b/slr_network.py
index 45295cb..ede70cf 100644
--- a/slr_network.py
+++ b/slr_network.py
@@ -89,6 +89,10 @@ class SLRModel(nn.Module):
         x = conv1d_outputs['visual_feat']
         lgt = conv1d_outputs['feat_len'].cpu()
         tm_outputs = self.temporal_model(x, lgt)
+        print(tm_outputs['predictions'].shape) #'predictions', 'hidden'
+        print(tm_outputs['hidden'].shape) #'predictions', 'hidden'
+
+        print('#######################################################')
         outputs = self.classifier(tm_outputs['predictions'])
         pred = None if self.training \
             else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)
