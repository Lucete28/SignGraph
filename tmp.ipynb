{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # <== 여기가 중요\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhy/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import modules.resnet as resnet\n",
    "from modules import BiLSTMLayer, TemporalConv\n",
    "from modules.criterions import SeqKD\n",
    "import utils\n",
    "import modules.resnet as resnet\n",
    "# Identity Layer (ResNet의 Fully Connected 제거용)\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# L2 정규화 선형 레이어\n",
    "class NormLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(NormLinear, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.matmul(x, F.normalize(self.weight, dim=0))\n",
    "        return outputs\n",
    "\n",
    "# SLRModel (수어 인식 모델)\n",
    "class SLRModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, num_classes, c2d_type, conv_type, use_bn=False,\n",
    "            hidden_size=1024, gloss_dict=None, loss_weights=None,\n",
    "            weight_norm=True, share_classifier=True\n",
    "    ):\n",
    "        super(SLRModel, self).__init__()\n",
    "        self.decoder = None\n",
    "        self.loss = dict()\n",
    "        self.criterion_init()\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_weights = loss_weights\n",
    "        self.conv2d = getattr(resnet, c2d_type)()  # ResNet 기반 2D CNN\n",
    "        self.conv2d.fc = Identity()  # Fully Connected 제거\n",
    "\n",
    "        # 1D CNN을 활용한 Temporal Encoding\n",
    "        self.conv1d = TemporalConv(input_size=512,\n",
    "                                   hidden_size=hidden_size,\n",
    "                                   conv_type=conv_type,\n",
    "                                   use_bn=use_bn,\n",
    "                                   num_classes=num_classes)\n",
    "\n",
    "        self.decoder = utils.Decode(gloss_dict, num_classes, 'beam')\n",
    "\n",
    "        # BiLSTM 기반 Temporal Model\n",
    "        self.temporal_model = BiLSTMLayer(rnn_type='LSTM', input_size=hidden_size, hidden_size=hidden_size,\n",
    "                                          num_layers=2, bidirectional=True)\n",
    "\n",
    "        # Classifier (NormLinear 사용 여부 결정)\n",
    "        if weight_norm:\n",
    "            self.classifier = NormLinear(hidden_size, self.num_classes)\n",
    "            self.conv1d.fc = NormLinear(hidden_size, self.num_classes)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_size, self.num_classes)\n",
    "            self.conv1d.fc = nn.Linear(hidden_size, self.num_classes)\n",
    "\n",
    "        # Classifier 공유 여부\n",
    "        if share_classifier:\n",
    "            self.conv1d.fc = self.classifier\n",
    "\n",
    "    def forward(self, x, len_x, label=None, label_lgt=None):\n",
    "        # CNN으로 Frame-wise Feature 추출\n",
    "        if len(x.shape) == 5:\n",
    "            batch, temp, channel, height, width = x.shape\n",
    "            framewise = self.conv2d(x.permute(0,2,1,3,4)).view(batch, temp, -1).permute(0,2,1)  # btc -> bct\n",
    "        else:\n",
    "            framewise = x\n",
    "\n",
    "        conv1d_outputs = self.conv1d(framewise, len_x)\n",
    "        x = conv1d_outputs['visual_feat']\n",
    "        lgt = conv1d_outputs['feat_len'].cpu()\n",
    "\n",
    "        # BiLSTM을 활용한 Temporal Modeling\n",
    "        tm_outputs = self.temporal_model(x, lgt)\n",
    "        features_before_classifier = tm_outputs['predictions']  # ✨ 분류기 전 특징값 저장\n",
    "\n",
    "        # 최종 Classifier 적용\n",
    "        outputs = self.classifier(features_before_classifier)\n",
    "\n",
    "        # Inference 모드에서 Decoding\n",
    "        pred = None if self.training else self.decoder.decode(outputs, lgt, batch_first=False, probs=False)\n",
    "        conv_pred = None if self.training else self.decoder.decode(conv1d_outputs['conv_logits'], lgt, batch_first=False, probs=False)\n",
    "\n",
    "        return {\n",
    "            \"framewise_features\": framewise,\n",
    "            \"visual_features\": x,\n",
    "            \"temproal_features\": tm_outputs['predictions'],\n",
    "            \"feat_len\": lgt,\n",
    "            \"conv_logits\": conv1d_outputs['conv_logits'],\n",
    "            \"sequence_logits\": outputs,\n",
    "            \"features_before_classifier\": features_before_classifier,  # ✨ 추가된 부분\n",
    "            \"conv_sents\": conv_pred,\n",
    "            \"recognized_sents\": pred,\n",
    "        }\n",
    "\n",
    "    def criterion_init(self):\n",
    "        self.loss['CTCLoss'] = torch.nn.CTCLoss(reduction='none', zero_infinity=False)\n",
    "        self.loss['distillation'] = SeqKD(T=8)\n",
    "        return self.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dataset_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ✅ gloss_dict 로드\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m gloss_dict_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# `dict_path`는 YAML 파일에 정의됨\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gloss_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(gloss_dict_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Gloss Dictionary Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dataset_info'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "# 환경 변수 설정\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# ✅ config 파일 로드 (dataset 정보 포함)\n",
    "config_path = \"./configs/baseline.yaml\"  # 혹은 원하는 설정 파일\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# ✅ gloss_dict 로드\n",
    "gloss_dict_path = config[\"dataset_info\"][\"dict_path\"]  # `dict_path`는 YAML 파일에 정의됨\n",
    "gloss_dict = np.load(gloss_dict_path, allow_pickle=True).item()\n",
    "\n",
    "print(\"📌 Gloss Dictionary Loaded!\")\n",
    "print(f\"Total Classes (Including Blank): {len(gloss_dict) + 1}\")\n",
    "print(f\"Sample Gloss Mapping: {list(gloss_dict.items())[:5]}\")  # 일부만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSLRModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m226\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2d_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# conv_type은 tconv.py에서 정의된 값 중 하나로 설정\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSLRModel.__init__\u001b[0;34m(self, num_classes, c2d_type, conv_type, use_bn, hidden_size, gloss_dict, loss_weights, weight_norm, share_classifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 1D CNN을 활용한 Temporal Encoding\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m TemporalConv(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     48\u001b[0m                            hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     49\u001b[0m                            conv_type\u001b[38;5;241m=\u001b[39mconv_type,\n\u001b[1;32m     50\u001b[0m                            use_bn\u001b[38;5;241m=\u001b[39muse_bn,\n\u001b[1;32m     51\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# BiLSTM 기반 Temporal Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_model \u001b[38;5;241m=\u001b[39m BiLSTMLayer(rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     57\u001b[0m                                   num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/SignGraph/utils/decode.py:13\u001b[0m, in \u001b[0;36mDecode.__init__\u001b[0;34m(self, gloss_dict, num_classes, search_mode, blank_id, beam_width)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gloss_dict, num_classes, search_mode, blank_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((v[\u001b[38;5;241m0\u001b[39m], k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2i_dict \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2g_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 모델 불러오기\n",
    "model = SLRModel(\n",
    "    num_classes=226, c2d_type=\"resnet18\", conv_type=2,  # conv_type은 tconv.py에서 정의된 값 중 하나로 설정\n",
    "    use_bn=True, hidden_size=1024, gloss_dict=None, loss_weights=None\n",
    ")\n",
    "\n",
    "# 저장된 가중치 로드\n",
    "state_dict = torch.load(\"model.pt\", map_location=\"cpu\")\n",
    "\n",
    "# state_dict가 딕셔너리인지 확인 후 모델에 로드\n",
    "if isinstance(state_dict, dict):\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
